{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](9444-project.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "student.py\n",
    "\n",
    "UNSW COMP9444 Neural Networks and Deep Learning\n",
    "\n",
    "You may modify this file however you wish, including creating additional\n",
    "variables, functions, classes, etc., so long as your code runs with the\n",
    "hw2main.py file unmodified, and you are only using the approved packages.\n",
    "\n",
    "You have been given some default values for the variables train_val_split,\n",
    "batch_size as well as the transform function.\n",
    "You are encouraged to modify these to improve the performance of your model.\n",
    "\n",
    "The variable device may be used to refer to the CPU/GPU being used by PyTorch.\n",
    "You may change this variable in the config.py file.\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from math import ceil\n",
    "import math\n",
    "\n",
    "'''\n",
    "Below is a slightly modified version of the implementation of Wide ResNet:  https://github.com/meliketoy/wide-resnet.pytorch\n",
    "BELOW COMMENT on the network architecture is based on the original paper - Wide Residual Network\n",
    "the reference e.g. [1] is also refering to the paper's reference\n",
    "'''\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "   Answer to Question:\n",
    "\n",
    "Briefly describe how your program works, and explain any design and training\n",
    "decisions you made along the way.\n",
    "\n",
    "Task is to classify 14 different Simpsons Characters .\n",
    "Firstly, EDA:\n",
    "image is in grayscale, and 64*64\n",
    "the distribution of classes is ploted via bar plot and is found to be a unbalanced dataset (523~2146).\n",
    "\n",
    "Pre-processing/transformation\n",
    "\t\n",
    "\tInitial data augmentation includes random perspective, random crop, Random Horizontal Flip and grayscale.\n",
    "\tIt is realised the model would overfit and so more transforms ()\n",
    "\t are added, however,  the original model is then not able to learnt, and it is found the model needs to be a lot complex in order to learn this extra transform dataset.\n",
    "\tSince both models give similar result (mention below), so the simpler model is chosen and so only 3 transforms are added.\n",
    "\tThe importance of certain transformation is determined by a model that is trained with 95% accuracy, if the new transformation decreases the accuracy within 10%, it is then accepted, however, it is found that most transformation is not needed for this dataset.\n",
    "\t\n",
    "Model selection\n",
    "\twide resnet 16 ,w = 5 - 95% - 20MB\n",
    "\tefficientnetv2 - 95% - 42MB\n",
    "\tresnet50 - 93% - 40 MB\n",
    "\tvision transformer or the hybrid variant is not considered since the dataset is small and comparison in Cifar-10 shows that it is not much improvement compared to ResNet.\n",
    "\t\n",
    "\twide resnet is chosen.\n",
    "\t\n",
    "\tFirstly, wide resnet converge faster and can achieve 99% test accuracy(not submission) \n",
    "\tSecondly, the paper - and the Cifar-10 benchmark shows great result and it is No.1 in ranking for SVHN dataset and since Cifar-10 dataset is quite similar to this project dataset in term of size of the data and the dimension of each image, this architecture is selected and adapted.\n",
    "\tThirdly, dropout works well with this model and as the dataset is relatively small, dropout is considered a great approach to avoid overfitting\n",
    "\tBesides, the wide resnet model size can be modified easily with the width factor be the dominating  factor (e.g. 6 -> 4 reduce model size by factor of 2)\n",
    "\tLastly, this model allow me to explore the effect between depth and width of the network as well as the standard ResNet architecture, the idea of residual blocks are used widely e.g. in Efficientnet and more.\n",
    "\t\n",
    "\tWide ResNet \n",
    "\t\twidth vs depth\n",
    "\t\tvery deep residual network has problem of diminishing feature reuse which makes it slow to train\n",
    "\t\tThis architecture decrease depth and increase the width of residual blocks\n",
    "\t\tdropout \n",
    "\t\tcould regularize the model and prevent overfitting\n",
    "\t\tdropout in residual networks was studied previously to have a negative effect when used in the identity part of the block. \n",
    "\t\tIn the wide ResNet the autoher argue here that dropout should be inserted between convolutional layers.\n",
    "\t\t\n",
    "Parameter tuning\n",
    "\tLearning rate \n",
    "\t\tIt is adjusted during the training based on the convergence in 10 epoch, if the improvement in 10 epoch is significantly less, then the learning rate is decreased by 10 times manually as what the exponential learning rate decay does.\n",
    "\t\tIt is sometimes increase/decrease 80% to check whether it stuck at local minimum and step size too big\n",
    "\tweight decay\n",
    "\t\tit is initially set as 5e-4 compared with the standard 1.5e-4 to avoid overfitting\n",
    "\t\tonce closer to 95%, it is adjusted slightly to ensure the model can continue to learn\n",
    "\tdrop out\n",
    "\t\tdrop out = 0.5 is considered to a bit high and the model stops improving at 91%.\n",
    "\t\tdrop out = 0.3 still seems to stop the model from improving from 96%\n",
    "\t\tdrop out = 0.1 \n",
    "\t\tThe reason could be some less important but still crucial feature with less weight is constantly dropped and cause the model to stop learning\n",
    "\ttrain_val_split\n",
    "\t\t0.8 - is used initially to testify the model selection \n",
    "\t\t0.95 is used once the model converged and stop improving\n",
    "\tearly stopping\n",
    "\t\tIt is found that it gives up to 2% improvement if the model is stopped early when the testing accuracy does not improve much anymore.\n",
    "\twidth factor\n",
    "\t\twidth of 6 can give 99.48% in the test set but around 95% in submission and so it is reduced as it is though to overfit since the model is too complex\n",
    "\t\twidth of 2 is found not learning at the end\n",
    "\tdepth\n",
    "\t\tdepth of 12 is found to learn really slowly at the end\n",
    "\tkernel size of first convolution layer and image size\n",
    "\t\tthe bottleneck of the time complexity is the ratio between the size of the image vs the kernel size of the first convolution layer\n",
    "\t\tSince the size needs to match with the input size, so only certain set of kernel + padding+ stride combo can be used based on the formula\n",
    "\tbatch size\n",
    "\t\tsmaller batch size could have effect of regularising and based on experiment 96 is chosen.\n",
    "Notes from the Paper:\n",
    "\tncreasing both depth and width helps until the number of parameters becomes too high and stronger regularization is needed\n",
    "\twidening consistently improves performance across residual networks of different depth;\n",
    "Implementation\n",
    "\t\n",
    "\tFollow from the original paper \"use SGD with Nesterov momentum and cross-entropy loss, initial learning rate is set to 0.1, weight decay to 0.0005, momentum to 0.9\"\n",
    "\tMinibatch size = 96\n",
    "\tthe test set from train_test_split is used as an indicator of improvement and generalisation of the network, however, this does not guarantee the \n",
    "Observation\n",
    "\tAdam could be better than SGD for resnet50, more than 5% faster convergence\n",
    "Future Improvement\n",
    "\tcutMix, learning rate scheduler are some examples of data augmentation that may improve the result \n",
    "Result\n",
    "\n",
    "appendix\n",
    "\t\n",
    "\tlinks that inferenced\n",
    "\t\t\n",
    "\t\twide Resnet paper: 1605.07146v2.pdf (arxiv.org)\n",
    "\t\t\n",
    "\t\tbatch size: How to Control the Stability of Training Neural Networks With the Batch Size (machinelearningmastery.com)\n",
    "\t\t\n",
    "\t\ttransform: Illustration of transforms — Torchvision 0.10.0 documentation (pytorch.org)\n",
    "\t\t\n",
    "\t\thow to improve efficientnet: Image classification via fine-tuning with EfficientNet (keras.io)\n",
    "\t\t\t\n",
    "\t\tEnetv2 implementation link: https://github.com/d-li14/efficientnetv2.pytorch/blob/775326e6c16bfc863e9b8400eca7d723dbfeb06e/effnetv2.py#L16\n",
    "\t\t\n",
    "\t\tparameter tuning for resNet https://arxiv.org/pdf/2103.07579v1.pdf\n",
    "\t\t\n",
    "\t\tdeep vs width: https://stats.stackexchange.com/questions/222883/why-are-neural-networks-becoming-deeper-but-not-wider\n",
    "\t\t\n",
    "\"\"\"\n",
    "\n",
    "data_path = \"/data\"\n",
    "\n",
    "############################################################################\n",
    "######     Specify transform(s) to be applied to the input images     ######\n",
    "############################################################################\n",
    "def transform(mode):\n",
    "    \"\"\"\n",
    "    Called when loading the data. Visit this URL for more information:\n",
    "    https://pytorch.org/vision/stable/transforms.html\n",
    "    You may specify different transforms for training and testing\n",
    "\n",
    "    visulsation of transformation - https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
    "    since the dataset is relatively small, more transforms are used to avoid overfitting\n",
    "    \"\"\"\n",
    "    if mode == 'train':\n",
    "        return transforms.Compose(\n",
    "        [\n",
    "        transforms.Grayscale(num_output_channels=1), \n",
    "        transforms.Resize(32),\n",
    "        transforms.RandomPerspective(distortion_scale=0.3),\n",
    "        transforms.RandomCrop(32, padding=4),# Since cropping is done after padding, https://pytorch.org/vision/stable/transforms.html\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ]) # #1 channel so len = 1, keep range to [-1,1] more explanation - https://discuss.pytorch.org/t/understanding-transform-normalize/21730\n",
    "    elif mode == 'test':\n",
    "        return transforms.Compose(\n",
    "        [\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize(32), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "\n",
    "    \n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1): # (n +2p -k) / s + 1, here: (32 + 2*1 - 3)/1 + 1 = 32\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "\n",
    "class wide_basic(nn.Module):\n",
    "    '''\n",
    "    Page 4\n",
    "    two consecutive 3 × 3 convolutions with batch normalization and ReLU\n",
    "    preceding convolution: conv3×3-conv3×3\n",
    "    Compared to the original architecture, the order of layers is changed from:\n",
    "    conv-BN-ReLU to BN-ReLU-conv\n",
    "    There is another type of blocks in standard ResNet called BOTTLENECK, however, since it thinner the network,\n",
    "    and the purpose of the paper is depth vs width, so it is not considered here.\n",
    "    '''\n",
    "    def __init__(self, in_planes, planes, dropout_rate, stride=1):\n",
    "        super(wide_basic, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, bias=True)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dropout(self.conv1(F.relu(self.bn1(x))))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += self.shortcut(x) # shortcut concept in residual net\n",
    "\n",
    "        return out\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    '''\n",
    "    Top of page 4 shows the overall network architure in a table\n",
    "    Small 3×3 filters are shown to be more effective [26, 32]\n",
    "    it consists of an initial convolutional layer conv1 that is followed by \n",
    "    3 groups (each of size N) of residual blocks conv2, conv3 and conv4, followed by \n",
    "    average pooling and final classification layer\n",
    "    * Size of conv1 is fixed \n",
    "    * introduced widening factor k scales the width of the residual blocks in the three groups conv2-4 \n",
    "    * a dropout layer into each residual block between convolutions to avoid heavy augmentation\n",
    "\n",
    "    Experiment result:\n",
    "    * blocks (consist of convolution layers with different kernel size) with comparable number of parameters turned out to give more or less the same results\n",
    "    * B(3,3) - original «basic» block - 2 3*3 convolution layers gives the best result compared to kernel size\n",
    "    '''\n",
    "    def __init__(self, depth, widen_factor, dropout_rate):\n",
    "        '''\n",
    "        @depth: deepening factor (refers to l in the paper) is the number of convolutions in a block\n",
    "        @widen_factor: k multiplies the number of features in convolutional layers\n",
    "        baseline «basic» block corresponds to l = 2, k = 1.\n",
    "        '''\n",
    "        \n",
    "        super(WideResNet, self).__init__()\n",
    "        n_classes=14\n",
    "        self.in_planes = 16\n",
    "\n",
    "        assert ((depth-4)%6 ==0), 'Wide-resnet depth should be 6n+4'\n",
    "        n = (depth-4)/6\n",
    "        k = widen_factor\n",
    "\n",
    "        nStages = [16, 16*k, 32*k, 64*k] # Original architecture(ResNet) is equivalent to k = 1.\n",
    "        in_channel = 1\n",
    "        self.conv1 = conv3x3(in_channel,nStages[0])\n",
    "        self.layer1 = self._wide_layer(wide_basic, nStages[1], n, dropout_rate, stride=1)\n",
    "        self.layer2 = self._wide_layer(wide_basic, nStages[2], n, dropout_rate, stride=2)\n",
    "        self.layer3 = self._wide_layer(wide_basic, nStages[3], n, dropout_rate, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(nStages[3], momentum=0.9)\n",
    "        self.linear = nn.Linear(nStages[3], n_classes)\n",
    "\n",
    "    def _wide_layer(self, block, planes, num_blocks, dropout_rate, stride):\n",
    "        strides = [stride] + [1]*(int(num_blocks)-1)\n",
    "        layers = []\n",
    "\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, dropout_rate, stride))\n",
    "            self.in_planes = planes\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "def get_class_weight():\n",
    "    import os, os.path\n",
    "\n",
    "    # path joining version for other paths\n",
    "    import os\n",
    "\n",
    "    data_dir = \"./data\"\n",
    "    dir_list = [os.path.join(data_dir, o) for o in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,o))]\n",
    "\n",
    "    n_files = []\n",
    "\n",
    "    for d in dir_list:\n",
    "        nf = len(os.listdir(d))\n",
    "        n_files.append(nf)\n",
    "        \n",
    "    max_class = max(n_files)\n",
    "    weights = [round(max_class/i, 2) for i in n_files]\n",
    "    class_weights = torch.FloatTensor(weights).cuda()\n",
    "    return class_weights  # weighted - use the min(n_class) / all n_classes, min_class = 2\n",
    "\n",
    "_net = WideResNet(16, 6, 0.5)\n",
    "#_net.load_state_dict(torch.load(\"./checkModel10.pth\"))\n",
    "class_weights = [2.65,4.1,1.73,1.97,2.42,1.0,1.94,1.71,1.8,2.19,1.59,1.59,1.96,2.77] #get_class_weight()\n",
    "lossFunc = nn.CrossEntropyLoss(weight=class_weights) # loss()\n",
    "\n",
    "############################################################################\n",
    "#######              Metaparameters and training options              ######\n",
    "############################################################################\n",
    "dataset = \"./data\"\n",
    "train_val_split = 0.8\n",
    "batch_size = 96 # https://github.com/facebookresearch/LaMCTS/blob/master/LaNAS/LaNet/CIFAR10/train.py\n",
    "epochs = 200 # since resnet50 140 ep takes 90mb, 50 also 90mb\n",
    "_optimiser = optim.SGD(_net.parameters(), lr=1e-1, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing/transformation\n",
    " \n",
    "Initial data augmentation includes resize(32) random perspective, random crop, Random Horizontal Flip, mean/std normalisation and grayscale.\n",
    "It is realised the model would overfit and so more transforms are added (most transform except for those applies to RGB image). However,  the original model is then failed to be learnt, and more complexity is needed to be added to the model for it to learn this extra transformed dataset.\n",
    "\n",
    "Since Efficientnet and resnet and wide resnet models all give a similar result (mention below), so the simpler and more flexible model is chosen with only 3 transforms are added.\n",
    "The importance of certain transformations is determined by a model that is trained with 95% accuracy, if the new transformation decreases the accuracy within 10%, it is then accepted, however, it is found that most transformation is redundant for this dataset.\n",
    " \n",
    "Moreover, the dataset is found to be unbalanced (500+ ~ 2000+), and a class weighted (ratio is as maximum class data among all classes/each individual class data number) is added to the loss function to solve this issue. For example, a dataset with 1 positive class data and 100 negatives. The update weighted for the 1 will be 100/1 higher. This is a sampling weight technique.\n",
    "# Model selection\n",
    "Note:\n",
    "since the Cifar-10 dataset is quite similar to this project dataset in term of size of the data and the dimension of each image, the benchmark of it is used as a guide on what model to try.\n",
    "\n",
    "wide resnet 16 ,w = 6 - 95% - 20MB\n",
    "wide 16 w = 4 – 94%, 10mb\n",
    "efficientnetv2 - 95% - 42MB\n",
    "resnet50 - 93% - 40 MB\n",
    "vision transformer or the hybrid variant is not considered since the dataset is small and benchmark in Cifar-10 shows that it is not much different compared to ResNet.\n",
    "  \n",
    "Firstly, wide resnet 16 (w = 6) converge faster and can achieve 99% test accuracy(but not submission) \n",
    "\n",
    "Secondly, the paper - and the Cifar-10 benchmark shows a great result and it is No.1 in ranking for SVHN dataset and high in the Cifar-10 dataset.\n",
    "Therefore, this architecture is selected and adapted.\n",
    "\n",
    "Thirdly, dropout works well with this model and since the dataset is relatively small, dropout is considered as a great approach to solve overfitting problem.\n",
    "Besides, the wide resnet model size can be modified easily with the width factor be the dominating  factor (e.g. 6 -> 4 reduce the model size approximately by a factor of 2)\n",
    "\n",
    "Lastly, this model allows me to explore the effect between depth and width of the network as well as the standard ResNet architecture, the idea of residual blocks is founded used widely e.g. in Efficientnet and other models.\n",
    " \n",
    "# Wide ResNet \n",
    "This model is primarily used to compare the effect of width vs depth.\n",
    "Below is quote from the author of the wide resnet paper:\n",
    "very deep residual network has a problem of diminishing feature reuse which makes it slow to train. This architecture decreases the depth and increases the width of residual blocks\n",
    "dropout could regularize the model and prevent overfitting, dropout in residual networks was studied previously to have a negative effect when used in the identity part of the block. \n",
    "In the wide ResNet, the author argues that dropout should be inserted between convolutional layers instead of within the identity part of the blocks.\n",
    "\n",
    "# Parameter tuning\n",
    "\n",
    "## Learning rate \n",
    "It is adjusted during the training based on the convergence in each 10 epoch, if the improvement after one 10 epoch is significantly less than the previous, then the learning rate is decreased by 10 times manually as what the typical learning rate decay does.\n",
    "Increases/decreases 80% is also tried to check whether the model is stucked at a local minimum and step size too big\n",
    "## weight decay\n",
    "it is initially set as 5e-4 compared with the standard 1.5e-4 to avoid overfitting\n",
    "once closer to 95%, it is adjusted slightly to ensure the model can continue to learn\n",
    "## drop out\n",
    "drop out = 0.5 is considered to be a bit high and the model stops improving at 91%.\n",
    "drop out = 0.3 still seems to stop the model from improving from 96%\n",
    "drop out = 0.1 \n",
    "The reason could be some less important but still crucial feature with less weight is constantly dropped and cause the model to stop learning\n",
    "## train_val_split\n",
    "0.8 - is used initially to testify the model selection \n",
    "0.95 is used once the model converged and stop improving\n",
    "## Early stopping\n",
    "It is found that it gives up to 2% improvement if the model is stopped early when the testing accuracy does not improve much anymore.\n",
    "kernel size of first convolution layer and image size\n",
    "the bottleneck of the time complexity is the ratio between the size of the image vs the kernel size of the first convolution layer\n",
    "Since the size needs to match with the input size, so only a certain set of kernel + padding+ stride combo can be used based on the formula\n",
    "## batch size\n",
    "smaller batch size could have the effect of regularising and based on experiment 96 is chosen.\n",
    "\n",
    "## width factor\n",
    "width of 6 can give 99.48% in the test set but around 95% in submission and so it is reduced as it is thought to overfit since the model is too complex\n",
    "width of 2 is found not learning at the end\n",
    "## depth\n",
    "depth of 12 is found to learn exceptionally slow at the end\n",
    "# Notes from the Paper:\n",
    "increasing both depth and width helps until the number of parameters becomes too high and stronger regularization is needed\n",
    "widening consistently improves performance across residual networks of different depth;\n",
    "# Implementation\n",
    " \n",
    "Follow from the original paper \"use SGD with Nesterov momentum and cross-entropy loss, the initial learning rate is set to 0.1, weight decay to 0.0005, momentum to 0.9\"\n",
    "Minibatch size = 96\n",
    "the test set from train_test_split is used as an indicator of improvement and generalisation of the network, however, this does not guarantee the \n",
    "Observation\n",
    "Adam could be better than SGD for resnet50, with more than 5% faster convergence\n",
    "Future Improvement\n",
    "cutMix, learning rate scheduler are some examples of data augmentation that may improve the result, ZCA whitening as mentioned in the paper.\n",
    "Result\n",
    " \n",
    "# Appendix\n",
    " \n",
    "links used as an inference\n",
    " \n",
    "wide Resnet paper: 1605.07146v2.pdf (arxiv.org)\n",
    " \n",
    "batch size: How to Control the Stability of Training Neural Networks With the Batch Size (machinelearningmastery.com)\n",
    " \n",
    "transform: Illustration of transforms — Torchvision 0.10.0 documentation (pytorch.org)\n",
    " \n",
    "how to improve efficientnet: Image classification via fine-tuning with EfficientNet (keras.io)\n",
    " \n",
    "Enetv2 implementation link: https://github.com/d-li14/efficientnetv2.pytorch/blob/775326e6c16bfc863e9b8400eca7d723dbfeb06e/effnetv2.py#L16\n",
    " \n",
    "parameter tuning for resNet https://arxiv.org/pdf/2103.07579v1.pdf\n",
    " \n",
    "deep vs width: https://stats.stackexchange.com/questions/222883/why-are-neural-networks-becoming-deeper-but-not-wider\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97643606afa0bdeaaecd0f24aa18bb8435e8e86e869d6d34c9b6af0ede22ba0d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
