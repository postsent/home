{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki2nlxHc4a99"
      },
      "source": [
        "# In Game Toxicity seq2seq classification\n",
        "A2 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdx1MiUE97vn"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZToQi1TAB3dI"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"/content/train.csv\")\n",
        "val_data = pd.read_csv(\"/content/validation.csv\")\n",
        "test_data = pd.read_csv(\"/content/test_without_labels.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDjNbie6BfZl"
      },
      "outputs": [],
      "source": [
        "# convert to list\n",
        "X_train_raw = train_data['sents'].tolist()\n",
        "y_train_raw = train_data['labels'].tolist()\n",
        "X_val_raw = val_data['sents'].tolist()\n",
        "y_val_raw = val_data['labels'].tolist()\n",
        "X_test_raw = test_data['sents'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8R1gFHtDc8P",
        "outputId": "462cada6-2caf-4859-b837-8ab5e6e8af96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "Size of training dataset: 26078\n",
            "Size of testing dataset: 500\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Sample Data\n",
            "LABEL: O O SEPA P O O O / SENTENCE: just end [SEPA] i wan nex game\n",
            "------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"------------------------------------\")\n",
        "print(\"Size of training dataset: {0}\".format(len(train_data)))\n",
        "print(\"Size of testing dataset: {0}\".format(len(test_data)))\n",
        "print(\"------------------------------------\")\n",
        "sample_ix = 17\n",
        "print(\"------------------------------------\")\n",
        "print(\"Sample Data\")\n",
        "print(\"LABEL: {0} / SENTENCE: {1}\".format(y_train_raw[sample_ix], X_train_raw[sample_ix]))\n",
        "print(\"------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "YHlrdgICER7A",
        "outputId": "a4f7bfb7-1ab3-4d72-af75-c58c25f716c1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e6d1a047-a232-41c4-9ea5-e4cab9f70617\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sents</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wow</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WTF</td>\n",
              "      <td>T</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wpe wpe</td>\n",
              "      <td>O O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6d1a047-a232-41c4-9ea5-e4cab9f70617')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e6d1a047-a232-41c4-9ea5-e4cab9f70617 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e6d1a047-a232-41c4-9ea5-e4cab9f70617');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     sents labels\n",
              "0      wow      O\n",
              "1      WTF      T\n",
              "2  wpe wpe    O O"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.head(3)\n",
        "# 5\ti cant [SEPA] play [SEPA] with 4 trash   |\tP O SEPA O SEPA O O O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zg6pSe6vRI8z"
      },
      "outputs": [],
      "source": [
        "# Tokenise words\n",
        "X_train = Prep.tokenise_words(X_train_raw)\n",
        "X_val = Prep.tokenise_words(X_val_raw)\n",
        "X_test = Prep.tokenise_words(X_test_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBNIe75wRihm"
      },
      "outputs": [],
      "source": [
        "# tokenise tags\n",
        "y_train = Prep.tokenise_tags(y_train_raw)\n",
        "y_val = Prep.tokenise_tags(y_val_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqMu7zbcReve",
        "outputId": "b9aa42ac-6629-4477-afa5-caf972856f07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['just', 'end', '[sepa]', 'i', 'wan', 'nex', 'game']\n",
            "['O', 'O', 'SEPA', 'P', 'O', 'O', 'O']\n"
          ]
        }
      ],
      "source": [
        "print(X_train[17])\n",
        "print(y_train[17])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPnpCS7xhi7k"
      },
      "source": [
        "word_to_ix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLpEfu5fRsN3"
      },
      "outputs": [],
      "source": [
        "word_to_ix, word_list = Prep.get_word_to_ix(X_train+X_val+X_test)\n",
        "tag_to_ix = Prep.get_tag_to_ix(y_train + y_val)\n",
        "ix_to_tag = {y: x for x, y in tag_to_ix.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMwZtsmiwDEp"
      },
      "outputs": [],
      "source": [
        "X_train_len = [len(x) for x in X_train]\n",
        "X_val_len = [len(x) for x in X_val]\n",
        "X_test_len = [len(x) for x in X_test]\n",
        "\n",
        "max_seq_length = max(X_train_len + X_val_len + X_test_len)\n",
        "X_train_attn_pad = [[True]*len(x)+[False]*(max_seq_length - len(x)) for x in X_train]\n",
        "X_val_attn_pad = [[True]*len(x)+[False]*(max_seq_length - len(x)) for x in X_val]\n",
        "X_test_attn_pad = [[True]*len(x)+[False]*(max_seq_length - len(x)) for x in X_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTMizDRuhmVM"
      },
      "source": [
        "encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ejk3gWZQSPxt"
      },
      "outputs": [],
      "source": [
        "# Convert input to idx, vectoriser\n",
        "X_train_index =  Prep.encode_to_index_and_pad(X_train,word_to_ix) # (26078, 57)\n",
        "y_train_index = Prep.encode_to_index_and_pad(y_train,tag_to_ix)\n",
        "X_val_index = Prep.encode_to_index_and_pad(X_val, word_to_ix)\n",
        "y_val_index = Prep.encode_to_index_and_pad(y_val,tag_to_ix)\n",
        "X_test_index = Prep.encode_to_index_and_pad(X_test,word_to_ix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZgeSEeaha-S"
      },
      "source": [
        "embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbwaH9n2m8mu"
      },
      "outputs": [],
      "source": [
        "dota_data, toxic_data = Emb.get_domain_dataset()\n",
        "dota_embed = Emb.make_self_trained_gensim_model(dota_data, dimension_=50, window_=3) \n",
        "toxic_embed = Emb.make_self_trained_gensim_model(toxic_data, dimension_=50, window_=3) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTBmqTbgjszH",
        "outputId": "c4c9c312-b3e1-43e2-859c-e9795162f692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "fasttext_embed = Emb.make_self_trained_gensim_model(X_train+X_val+X_test, dimension_=50, window_=3) \n",
        "glove_twitter_gensim = api.load(\"glove-twitter-25\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zcQVIHfWEae"
      },
      "outputs": [],
      "source": [
        "# get additional embedding\n",
        "# load features from file to save time\n",
        "pos_features=np.load('/content/features/pos_features.npy', allow_pickle=True).tolist()\n",
        "dep_features=np.load('/content/features/dep_features.npy', allow_pickle=True).tolist()\n",
        "\n",
        "# ðŸ’Ž uncomment below to run the code\n",
        "# 3mins\n",
        "# features = Emb.get_word_features(X_train_raw+X_val_raw+X_test_raw)\n",
        "# pos_features, dep_features = features['pos'], features['dep']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-kCrtImJv7D"
      },
      "outputs": [],
      "source": [
        "f = Emb.get_word_features(X_train_raw+X_val_raw+X_test_raw) # \n",
        "len_features, cap_features = f['len'], f['cap']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyM4kLzNnFDv"
      },
      "outputs": [],
      "source": [
        "pretrained_index = 25 # first n cols\n",
        "all_embeds = [\n",
        "    glove_twitter_gensim,\n",
        "    fasttext_embed, \n",
        "    len_features,\n",
        "    cap_features,\n",
        "    pos_features,\n",
        "    dep_features,\n",
        "    dota_embed,\n",
        "    toxic_embed\n",
        "]\n",
        "all_embeds_name = [\n",
        "    'glove_twitter_gensim',\n",
        "    'fasttext_embed', \n",
        "    'len_features',\n",
        "    'cap_features',\n",
        "    'pos_features',\n",
        "    'dep_features',\n",
        "    'dota_embed',\n",
        "    'toxic_embed'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRb_aKbki9wI"
      },
      "outputs": [],
      "source": [
        "embed_matrix, embed_matrix_dim = Emb.build_concat_embed_table(word_list, all_embeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0tYT2aZdB04"
      },
      "outputs": [],
      "source": [
        "# # sanity check\n",
        "# X_train_index = X_train_index[:100]\n",
        "# y_train_index = y_train_index[:100]\n",
        "# X_val_index = X_val_index[:100]\n",
        "# y_val_index = y_val_index[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYZcVTgdNdrh"
      },
      "source": [
        "# batch train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kik6eU42tpyD"
      },
      "outputs": [],
      "source": [
        "def to_torch(x):\n",
        "    return torch.from_numpy(np.array(x)).to(device)\n",
        "    \n",
        "\n",
        "N_EPOCH = 2\n",
        "BATCH_SIZE = 20\n",
        "HIDDEN_DIM = 50\n",
        "VOCAB_SIZE = len(word_to_ix)\n",
        "N_DIRECTION = 2\n",
        "################################## dataset ##################################\n",
        "# train set below\n",
        "X_train_len_torch = to_torch(X_train_len).cpu() \n",
        "X_train_attn_pad_torch = to_torch(X_train_attn_pad).to(device)\n",
        "X_train_torch = to_torch(X_train_index) \n",
        "y_train_torch = to_torch(y_train_index) \n",
        "training_set = CustomDataSet(X_train_torch, y_train_torch, X_train_len_torch, X_train_attn_pad_torch)\n",
        "params_dataset = {'batch_size': BATCH_SIZE,\n",
        "        'shuffle': True,\n",
        "        'drop_last':True # if the sample size cannot be batched properly\n",
        "        }\n",
        "train_generator = torch.utils.data.DataLoader(training_set, **params_dataset)\n",
        "\n",
        "# validation set below\n",
        "X_val_torch = to_torch(X_val_index) \n",
        "y_val_torch = to_torch(y_val_index) \n",
        "X_val_attn_pad_torch = to_torch(X_val_attn_pad).to(device)\n",
        "X_val_len_torch = to_torch(X_val_len).cpu() \n",
        "\n",
        "val_set = CustomDataSet(X_val_torch, y_val_torch, X_val_len_torch, X_val_attn_pad_torch)\n",
        "params_dataset = {'batch_size': BATCH_SIZE,\n",
        "        'shuffle': True,\n",
        "        'drop_last':True # if the sample size cannot be batched properly\n",
        "        }\n",
        "val_generator = torch.utils.data.DataLoader(val_set, **params_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFGGyCdrU-tu"
      },
      "source": [
        "# Test Program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT_1wHiN5H__"
      },
      "source": [
        "## funcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ba4fr9dRualt"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "def get_model_args():\n",
        "    return {\n",
        "        'vocab_size': VOCAB_SIZE, \n",
        "        'n_direction':N_DIRECTION, # bi direction or 1\n",
        "        'tag_to_ix': tag_to_ix, \n",
        "        'embedding_dim': embed_matrix_dim, \n",
        "        'embed_matrix':embed_matrix,\n",
        "        'hidden_dim': HIDDEN_DIM, \n",
        "        'batch_size': BATCH_SIZE, \n",
        "        'nhead':1,\n",
        "        'stack_layers':1, # 3, 5 does not learn\n",
        "        'attn_name': ['location_base', 'scaled_dot_product', 'dot_product', 'additive'], \n",
        "        'is_lstm2': False,\n",
        "        'is_lstm3': False,\n",
        "        'is_attn1':False,\n",
        "        'is_attn2':False,\n",
        "        'is_attn3':False,\n",
        "        'is_pretrained':False\n",
        "    }\n",
        "\n",
        "def train(model_args_, is_report=False, is_print=False):\n",
        "    model = BiLSTM_attn(model_args_).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(),  lr=1e-2, weight_decay=1e-4) # 2e-3\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    ################################## Epoch loss history ##################################\n",
        "    report = ()\n",
        "    for epoch in range(N_EPOCH):  \n",
        "        time1 = datetime.datetime.now()\n",
        "\n",
        "        train_loss = []\n",
        "\n",
        "        for local_batch, local_labels, local_lens, local_pad in train_generator:\n",
        "            model.train()\n",
        "            model.zero_grad()\n",
        "\n",
        "            y_pred = model(local_batch, local_lens, local_pad)#, local_pad)\n",
        "            \n",
        "            batch_size, seq_len = local_batch.size()\n",
        "            # torch.Size([1440, 10]) the loss function will make it same as RHS,  torch.Size([1440]), \n",
        "            loss = criterion(y_pred.view(batch_size * seq_len, -1), local_labels.view(batch_size * seq_len))\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1) # https://www.reddit.com/r/MachineLearning/comments/kqgne3/choosing_gradient_norm_clip_value_d/\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss.append(loss.item())\n",
        "        # since batch, so avg\n",
        "        train_loss = np.mean(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        # evaluation\n",
        "        with torch.no_grad():\n",
        "            train_f1, _ = eval_f1(model, train_generator)\n",
        "            val_f1, val_report = eval_f1(model, val_generator, is_report)\n",
        "\n",
        "            time2 = datetime.datetime.now()\n",
        "\n",
        "            print(\"Epoch:%d, Training loss: %.2f, time: %.2fs\" %(epoch+1, train_loss, (time2-time1).total_seconds()))\n",
        "            print('train_f1', train_f1, 'val_f1', val_f1)\n",
        "            if is_report and epoch+1 == N_EPOCH:\n",
        "                report = (val_f1, val_report)\n",
        "            if is_print:\n",
        "                print(val_report)\n",
        "    if is_print:\n",
        "        return\n",
        "    if is_report:\n",
        "        return report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrB-f3yKYnRJ"
      },
      "outputs": [],
      "source": [
        "def test_performance():\n",
        "    print(\"ðŸ’Ž 1 Performace Comparison ðŸ’Ž\")\n",
        "    model_args = get_model_args()\n",
        "    f1, report = train(model_args, True)\n",
        "    p('')  \n",
        "    print('T-F1: ', f1)\n",
        "    \n",
        "    for k, v in report.items():\n",
        "        try:\n",
        "            i = int(k)\n",
        "        except:\n",
        "            continue\n",
        "        if i not in [0,1]: # ignore pad and SEPA\n",
        "            print(f'T-F1({ix_to_tag[i]}): ', v['f1-score'])\n",
        "\n",
        "    p('')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud812Qf-d1yO"
      },
      "outputs": [],
      "source": [
        "def test_embedding():\n",
        "    base_ix = 1 # self train on given dataset\n",
        "    print(\"ðŸ’Ž 2 Embedding ðŸ’Ž\")\n",
        "    print(\"Name: \", all_embeds_name[base_ix])\n",
        "    model_args = get_model_args()\n",
        "    embed_matrix_, embed_matrix_dim_ = Emb.build_concat_embed_table(word_list, [all_embeds[base_ix]])\n",
        "    model_args['embed_matrix'] = embed_matrix_\n",
        "    model_args['embedding_dim'] = embed_matrix_dim_\n",
        "    \n",
        "    train(model_args)\n",
        "    p('')    \n",
        "    print('ðŸ’Ž Below is feature addition to the existing self-train embedding')\n",
        "    p('')\n",
        "    for i in range(0, len(all_embeds_name)):\n",
        "        if i == base_ix: # ignore baseline embedding\n",
        "            continue\n",
        "        print(\"Name: \", all_embeds_name[i])\n",
        "        model_args = get_model_args()\n",
        "        embed_matrix_, embed_matrix_dim_ = Emb.build_concat_embed_table(word_list, [all_embeds[base_ix], all_embeds[i]])\n",
        "        model_args['embed_matrix'] = embed_matrix_\n",
        "        model_args['embedding_dim'] = embed_matrix_dim_\n",
        "        if i == 0:\n",
        "            model_args['is_pretrained'] = True\n",
        "        train(model_args)\n",
        "        p('')    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuA7n7SOcSBd"
      },
      "outputs": [],
      "source": [
        "def test_attn_func():\n",
        "    print(\"ðŸ’Ž 3 Attention ðŸ’Ž\")\n",
        "    for i in range(4):\n",
        "        model_args = get_model_args()\n",
        "        model_args['attn_name'] = model_args['attn_name'][i]\n",
        "        model_args['is_attn1'] = True\n",
        "        print(\"Types: \", model_args['attn_name'])\n",
        "        train(model_args)\n",
        "        p('')    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLWOZN6mQD2G"
      },
      "outputs": [],
      "source": [
        "def test_attn_position():\n",
        "    print(\"ðŸ’Ž 3 Attention - Position ðŸ’Ž\")\n",
        "    p('')\n",
        "    model_args = get_model_args()\n",
        "    attn_name_ = model_args['attn_name'][1]\n",
        "    model_args = get_model_args()\n",
        "    model_args['attn_name'] = attn_name_\n",
        "    print('After the first layer')\n",
        "    model_args['is_attn1'] = True\n",
        "    train(model_args)\n",
        "    p('')    \n",
        "    print('After the second layer')\n",
        "    model_args = get_model_args()\n",
        "    model_args['attn_name'] = attn_name_\n",
        "    model_args['is_attn2'] = True\n",
        "    model_args['is_lstm2'] = True\n",
        "    train(model_args)\n",
        "    p('')    \n",
        "    print('After the third layer')\n",
        "    model_args = get_model_args()\n",
        "    model_args['attn_name'] = attn_name_\n",
        "    model_args['is_attn3'] = True\n",
        "    model_args['is_lstm2'] = True\n",
        "    model_args['is_lstm3'] = True\n",
        "    train(model_args)\n",
        "    p('')  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36rQ9ZigWaVf"
      },
      "outputs": [],
      "source": [
        "def test_stack_seq2seq():\n",
        "    print(\"ðŸ’Ž 4 Stacked Seq2Seq model ðŸ’Ž\")\n",
        "    for i in range(1, 5):\n",
        "        print(\"Number of Stack layers: \", i)\n",
        "        model_args = get_model_args()\n",
        "        model_args['stack_layers'] = i\n",
        "        train(model_args)\n",
        "        p('')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJTHWDAzj3eI"
      },
      "outputs": [],
      "source": [
        "def test_crf():\n",
        "    print(\"ðŸ’Ž 5 with CRF ðŸ’Ž\")\n",
        "    pipeline(X_train, y_train, word_to_ix, tag_to_ix)\n",
        "    print(\"ðŸ’Ž 5 without CRF ðŸ’Ž\")\n",
        "    model_args = get_model_args()\n",
        "    train(model_args, is_print=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcdTp9n85DGy"
      },
      "source": [
        "## run all "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16pY7Dxf5Md5",
        "outputId": "609bc50a-359e-47e5-f135-2bc2b6c01032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’Ž 1 Performace Comparison ðŸ’Ž\n",
            "Epoch:1, Training loss: 0.18, time: 16.68s\n",
            "train_f1 0.937883411942415 val_f1 0.935654106101287\n",
            "Epoch:2, Training loss: 0.01, time: 16.28s\n",
            "train_f1 0.9435033478354778 val_f1 0.9372970967442723\n",
            "\n",
            "T-F1:  0.9372970967442723\n",
            "T-F1(O):  0.9850870355936607\n",
            "T-F1(T):  0.9267767408470926\n",
            "T-F1(P):  0.9915579432079816\n",
            "T-F1(S):  0.9818070631401926\n",
            "T-F1(D):  0.7336523125996811\n",
            "T-F1(C):  0.9471686175260993\n",
            "\n",
            "ðŸ’Ž 2 Embedding ðŸ’Ž\n",
            "Name:  fasttext_embed\n",
            "Epoch:1, Training loss: 0.20, time: 16.48s\n",
            "train_f1 0.9203373077160685 val_f1 0.9177420852288553\n",
            "Epoch:2, Training loss: 0.01, time: 16.38s\n",
            "train_f1 0.9242503621066905 val_f1 0.922165664044424\n",
            "\n",
            "ðŸ’Ž Below is feature addition to the existing self-train embedding\n",
            "\n",
            "Name:  glove_twitter_gensim\n",
            "Epoch:1, Training loss: 0.18, time: 16.57s\n",
            "train_f1 0.940317852183305 val_f1 0.9386429035551842\n",
            "Epoch:2, Training loss: 0.01, time: 16.70s\n",
            "train_f1 0.9398419655534292 val_f1 0.9364843532361408\n",
            "\n",
            "Name:  len_features\n",
            "Epoch:1, Training loss: 0.18, time: 16.43s\n",
            "train_f1 0.917194109111118 val_f1 0.9124304267161409\n",
            "Epoch:2, Training loss: 0.01, time: 16.46s\n",
            "train_f1 0.9254233106128366 val_f1 0.9216577044316707\n",
            "\n",
            "Name:  cap_features\n",
            "Epoch:1, Training loss: 0.18, time: 16.92s\n",
            "train_f1 0.908100265481262 val_f1 0.9077836534001299\n",
            "Epoch:2, Training loss: 0.01, time: 16.38s\n",
            "train_f1 0.9545846950730714 val_f1 0.9487155708058982\n",
            "\n",
            "Name:  pos_features\n",
            "Epoch:1, Training loss: 0.19, time: 16.54s\n",
            "train_f1 0.9167026726745262 val_f1 0.9144022999165353\n",
            "Epoch:2, Training loss: 0.01, time: 16.42s\n",
            "train_f1 0.9493334979017527 val_f1 0.9483062645011601\n",
            "\n",
            "Name:  dep_features\n",
            "Epoch:1, Training loss: 0.18, time: 16.39s\n",
            "train_f1 0.9338062924120913 val_f1 0.9319992588475079\n",
            "Epoch:2, Training loss: 0.01, time: 16.47s\n",
            "train_f1 0.9395950367306624 val_f1 0.9345525436316375\n",
            "\n",
            "Name:  dota_embed\n",
            "Epoch:1, Training loss: 0.18, time: 17.03s\n",
            "train_f1 0.9216932522908889 val_f1 0.9164580435790449\n",
            "Epoch:2, Training loss: 0.01, time: 16.40s\n",
            "train_f1 0.9525087118759059 val_f1 0.9474269819193324\n",
            "\n",
            "Name:  toxic_embed\n",
            "Epoch:1, Training loss: 0.20, time: 16.72s\n",
            "train_f1 0.9465672377665475 val_f1 0.9435274480712166\n",
            "Epoch:2, Training loss: 0.01, time: 16.46s\n",
            "train_f1 0.9467589018251443 val_f1 0.9420612813370474\n",
            "\n",
            "ðŸ’Ž 3 Attention ðŸ’Ž\n",
            "Types:  location_base\n",
            "Epoch:1, Training loss: 0.03, time: 22.21s\n",
            "train_f1 0.8266773408873972 val_f1 0.8199407517126458\n",
            "Epoch:2, Training loss: 0.02, time: 22.12s\n",
            "train_f1 0.8806112409297749 val_f1 0.8712051517939282\n",
            "\n",
            "Types:  scaled_dot_product\n",
            "Epoch:1, Training loss: 0.03, time: 21.92s\n",
            "train_f1 0.8629460201280879 val_f1 0.8618005311841743\n",
            "Epoch:2, Training loss: 0.01, time: 21.91s\n",
            "train_f1 0.8927282735000918 val_f1 0.8913623401710974\n",
            "\n",
            "Types:  dot_product\n",
            "Epoch:1, Training loss: 0.03, time: 21.81s\n",
            "train_f1 0.8536357047952652 val_f1 0.8493311641359363\n",
            "Epoch:2, Training loss: 0.01, time: 21.91s\n",
            "train_f1 0.9270352127757211 val_f1 0.921590281612369\n",
            "\n",
            "Types:  additive\n",
            "Epoch:1, Training loss: 0.03, time: 22.00s\n",
            "train_f1 0.8831678947688936 val_f1 0.8807734604105572\n",
            "Epoch:2, Training loss: 0.01, time: 22.15s\n",
            "train_f1 0.8997070848843596 val_f1 0.898879500367377\n",
            "\n",
            "ðŸ’Ž 3 Attention - Position ðŸ’Ž\n",
            "\n",
            "After the first layer\n",
            "Epoch:1, Training loss: 0.03, time: 22.29s\n",
            "train_f1 0.8775201307743536 val_f1 0.8743622448979592\n",
            "Epoch:2, Training loss: 0.01, time: 21.98s\n",
            "train_f1 0.9003717019072572 val_f1 0.8972709730772764\n",
            "\n",
            "After the second layer\n",
            "Epoch:1, Training loss: 0.04, time: 31.58s\n",
            "train_f1 0.7375964382642637 val_f1 0.7322921451538815\n",
            "Epoch:2, Training loss: 0.01, time: 31.81s\n",
            "train_f1 0.8699174564353409 val_f1 0.8691571586308429\n",
            "\n",
            "After the third layer\n",
            "Epoch:1, Training loss: 0.06, time: 41.21s\n",
            "train_f1 0.6946755909310178 val_f1 0.6911898274296094\n",
            "Epoch:2, Training loss: 0.01, time: 41.47s\n",
            "train_f1 0.7747695323267281 val_f1 0.7696349557522123\n",
            "\n",
            "ðŸ’Ž 4 Stacked Seq2Seq model ðŸ’Ž\n",
            "Number of Stack layers:  1\n",
            "Epoch:1, Training loss: 0.20, time: 16.47s\n",
            "train_f1 0.9408322296485663 val_f1 0.934611466148004\n",
            "Epoch:2, Training loss: 0.01, time: 16.71s\n",
            "train_f1 0.9390462088962922 val_f1 0.9338835311572701\n",
            "\n",
            "Number of Stack layers:  2\n",
            "Epoch:1, Training loss: 0.19, time: 21.13s\n",
            "train_f1 0.9034301275970748 val_f1 0.902088892978743\n",
            "Epoch:2, Training loss: 0.01, time: 21.16s\n",
            "train_f1 0.9500261562605778 val_f1 0.9433456561922366\n",
            "\n",
            "Number of Stack layers:  3\n",
            "Epoch:1, Training loss: 0.20, time: 25.73s\n",
            "train_f1 0.8476126271360319 val_f1 0.8503714184648038\n",
            "Epoch:2, Training loss: 0.01, time: 25.87s\n",
            "train_f1 0.9081666616236498 val_f1 0.9029823604291688\n",
            "\n",
            "Number of Stack layers:  4\n",
            "Epoch:1, Training loss: 0.22, time: 30.38s\n",
            "train_f1 0.6121686968231621 val_f1 0.6075506445672192\n",
            "Epoch:2, Training loss: 0.03, time: 30.25s\n",
            "train_f1 0.6516932848810578 val_f1 0.645821854912764\n",
            "\n",
            "ðŸ’Ž 5 with CRF ðŸ’Ž\n",
            "Epoch:1, Training loss: 34299.72, time: 821.50s\n",
            "train_f1 0.8264946526522232 val_f1 0.829259293423116\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00      3603\n",
            "           2       0.92      1.00      0.96     18985\n",
            "           3       0.97      0.88      0.92      1469\n",
            "           4       1.00      0.97      0.98      3936\n",
            "           5       0.99      0.82      0.90      3322\n",
            "           6       0.98      0.12      0.21       398\n",
            "           7       0.92      0.67      0.78      1641\n",
            "\n",
            "    accuracy                           0.94     33354\n",
            "   macro avg       0.97      0.78      0.82     33354\n",
            "weighted avg       0.95      0.94      0.94     33354\n",
            "\n",
            "Epoch:2, Training loss: 12964.36, time: 822.50s\n",
            "train_f1 0.9281420597436054 val_f1 0.9281829077164193\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00      3603\n",
            "           2       0.97      1.00      0.98     18985\n",
            "           3       0.97      0.95      0.96      1469\n",
            "           4       1.00      0.99      0.99      3936\n",
            "           5       0.96      0.95      0.96      3322\n",
            "           6       0.91      0.37      0.53       398\n",
            "           7       0.95      0.90      0.92      1641\n",
            "\n",
            "    accuracy                           0.98     33354\n",
            "   macro avg       0.97      0.88      0.91     33354\n",
            "weighted avg       0.98      0.98      0.97     33354\n",
            "\n",
            "ðŸ’Ž 5 without CRF ðŸ’Ž\n",
            "Epoch:1, Training loss: 0.18, time: 17.34s\n",
            "train_f1 0.9015954170259948 val_f1 0.8988431281813976\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      1.00      0.00         0\n",
            "           2       0.96      1.00      0.98     18972\n",
            "           3       0.88      0.88      0.88      1469\n",
            "           4       1.00      0.98      0.99      3935\n",
            "           5       0.98      0.95      0.97      3321\n",
            "           6       1.00      0.02      0.04       398\n",
            "           7       0.96      0.85      0.90      1641\n",
            "\n",
            "    accuracy                           0.96     29736\n",
            "   macro avg       0.83      0.81      0.68     29736\n",
            "weighted avg       0.97      0.96      0.96     29736\n",
            "\n",
            "Epoch:2, Training loss: 0.01, time: 16.17s\n",
            "train_f1 0.9337611452195107 val_f1 0.9296505700250255\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      1.00      0.00         0\n",
            "           1       0.00      1.00      0.00         0\n",
            "           2       0.97      1.00      0.99     18976\n",
            "           3       0.99      0.88      0.93      1469\n",
            "           4       1.00      0.99      0.99      3934\n",
            "           5       0.96      0.97      0.97      3321\n",
            "           6       0.99      0.39      0.56       398\n",
            "           7       0.99      0.90      0.94      1640\n",
            "\n",
            "    accuracy                           0.97     29738\n",
            "   macro avg       0.74      0.89      0.67     29738\n",
            "weighted avg       0.98      0.97      0.97     29738\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_performance()\n",
        "\n",
        "test_embedding()\n",
        "\n",
        "test_attn_func()\n",
        "test_attn_position()\n",
        "\n",
        "test_stack_seq2seq()\n",
        "\n",
        "test_crf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbmYc8aBFYQH"
      },
      "source": [
        "## additional tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lUIrj37S1bW"
      },
      "source": [
        "- Note: Typing mistake. The log text should print 3 embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RcT2z3pD313",
        "outputId": "6efda799-7b78-41f8-c5ce-6a853ef39ae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’Ž 2 Embedding ðŸ’Ž\n",
            "Name:  fasttext_embed + cap_features + pos_features\n",
            "Epoch:1, Training loss: 0.20, time: 15.56s\n",
            "train_f1 0.9150228085316238 val_f1 0.9110781771026306\n",
            "Epoch:2, Training loss: 0.01, time: 15.52s\n",
            "train_f1 0.9468259111810635 val_f1 0.9392562366688305\n",
            "\n",
            "ðŸ’Ž 2 Embedding ðŸ’Ž\n",
            "Name:  fasttext_embed + cap_features + dota_embed\n",
            "Epoch:1, Training loss: 0.20, time: 15.53s\n",
            "train_f1 0.9397226502311248 val_f1 0.9368567454798331\n",
            "Epoch:2, Training loss: 0.01, time: 15.65s\n",
            "train_f1 0.9527894347074797 val_f1 0.9474953617810761\n",
            "\n",
            "ðŸ’Ž 2 Embedding ðŸ’Ž\n",
            "Name:  fasttext_embed + pos_features + dota_embed\n",
            "Epoch:1, Training loss: 0.18, time: 15.93s\n",
            "train_f1 0.9163533255254885 val_f1 0.9136111111111112\n",
            "Epoch:2, Training loss: 0.01, time: 15.65s\n",
            "train_f1 0.9387144531490963 val_f1 0.9341995359628771\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# additional embed tests\n",
        "\n",
        "def test_embedding_additional(idx_1, idx_2):\n",
        "    base_ix = 1 # self train on given dataset\n",
        "    print(\"ðŸ’Ž 2 Embedding ðŸ’Ž\")\n",
        "    print(\"Name: \", all_embeds_name[base_ix], '+', all_embeds_name[idx_1], '+', all_embeds_name[idx_2])\n",
        "    model_args = get_model_args()\n",
        "    embed_matrix_, embed_matrix_dim_ = Emb.build_concat_embed_table(word_list, [all_embeds[base_ix], all_embeds[idx_1], all_embeds[idx_2]])\n",
        "    model_args['embed_matrix'] = embed_matrix_\n",
        "    model_args['embedding_dim'] = embed_matrix_dim_\n",
        "    \n",
        "    train(model_args)\n",
        "    p('')    \n",
        "\n",
        "test_embedding_additional(3, 4)\n",
        "test_embedding_additional(3, 6)\n",
        "test_embedding_additional(4, 6)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9Iox69EMXp7"
      },
      "source": [
        "Note that the CRF result is incorrect on the first big code cell since the environments are different for the model, below is the correct version:\n",
        "CRF attachment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHmkHGEYH6-m",
        "outputId": "09f3b043-d479-420b-fc62-f4b5c745ba71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’Ž 5 with CRF ðŸ’Ž\n",
            "Epoch:1, Training loss: 26921.06, time: 520.24s\n",
            "train_f1 0.9726499938400887 val_f1 0.9655140532544378\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00      3603\n",
            "           2       0.99      1.00      0.99     18985\n",
            "           3       0.98      0.94      0.96      1469\n",
            "           4       1.00      1.00      1.00      3936\n",
            "           5       1.00      0.99      0.99      3322\n",
            "           6       0.96      0.70      0.81       398\n",
            "           7       0.95      0.96      0.96      1641\n",
            "\n",
            "    accuracy                           0.99     33354\n",
            "   macro avg       0.98      0.94      0.96     33354\n",
            "weighted avg       0.99      0.99      0.99     33354\n",
            "\n",
            "Epoch:2, Training loss: 2782.26, time: 514.06s\n",
            "train_f1 0.9944754791518781 val_f1 0.9800461893764434\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00      3603\n",
            "           2       0.99      1.00      1.00     18985\n",
            "           3       0.98      0.97      0.97      1469\n",
            "           4       1.00      1.00      1.00      3936\n",
            "           5       0.99      0.99      0.99      3322\n",
            "           6       0.98      0.91      0.95       398\n",
            "           7       0.99      0.98      0.98      1641\n",
            "\n",
            "    accuracy                           0.99     33354\n",
            "   macro avg       0.99      0.98      0.98     33354\n",
            "weighted avg       0.99      0.99      0.99     33354\n",
            "\n",
            "ðŸ’Ž 5 without CRF ðŸ’Ž\n",
            "Epoch:1, Training loss: 0.17, time: 11.47s\n",
            "train_f1 0.9373035923347095 val_f1 0.9321656936335836\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      1.00      0.00         0\n",
            "           2       0.97      1.00      0.98     18974\n",
            "           3       0.97      0.88      0.92      1467\n",
            "           4       1.00      0.99      0.99      3933\n",
            "           5       0.98      0.97      0.98      3320\n",
            "           6       1.00      0.49      0.66       398\n",
            "           7       0.99      0.90      0.94      1638\n",
            "\n",
            "    accuracy                           0.98     29730\n",
            "   macro avg       0.84      0.89      0.78     29730\n",
            "weighted avg       0.98      0.98      0.98     29730\n",
            "\n",
            "Epoch:2, Training loss: 0.01, time: 11.46s\n",
            "train_f1 0.946791189210777 val_f1 0.9409754295215224\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      1.00      0.00         0\n",
            "           2       0.98      1.00      0.99     18965\n",
            "           3       0.98      0.90      0.94      1467\n",
            "           4       0.99      0.99      0.99      3931\n",
            "           5       0.99      0.98      0.98      3318\n",
            "           6       1.00      0.69      0.81       397\n",
            "           7       1.00      0.90      0.95      1639\n",
            "\n",
            "    accuracy                           0.98     29717\n",
            "   macro avg       0.85      0.92      0.81     29717\n",
            "weighted avg       0.98      0.98      0.98     29717\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def test_crf_v2():\n",
        "    print(\"ðŸ’Ž 5 with CRF ðŸ’Ž\")\n",
        "    pipeline(X_train, y_train, word_to_ix, tag_to_ix)\n",
        "    print(\"ðŸ’Ž 5 without CRF ðŸ’Ž\")\n",
        "    model_args = get_model_args()\n",
        "    train(model_args, is_print=True)\n",
        "test_crf_v2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aThNLm_dQJuH"
      },
      "source": [
        "# OOP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXQzWXbf8zlw"
      },
      "source": [
        "## Set up file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlXxK7TQ57pn"
      },
      "source": [
        "### Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-xJ18Zw5_4Y",
        "outputId": "e64a70dd-680d-4faf-fc29-e2924425d213"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.2 MB 24.0 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.1 MB 61.8 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181 kB 73.8 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42 kB 1.5 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 457 kB 62.2 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 660 kB 70.3 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58 kB 7.8 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.3.0\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "############################################ general ###########################################\n",
        "import shutil # move files\n",
        "import os\n",
        "from typing import *\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "import string\n",
        "\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "############################################ data manipulation ############################################\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "############################################ pretrain  ############################################\n",
        "# TODO: Glove \n",
        "from gensim.models import Word2Vec \n",
        "from gensim.models import FastText\n",
        "import gensim.downloader as api\n",
        "\n",
        "############################################ preprocessing ############################################\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import re\n",
        "############################################ Spacy ############################################\n",
        "# !pip install -U -q pip setuptools wheel\n",
        "!pip -q install -U  spacy\n",
        "!python -q -m spacy download en_core_web_sm  | grep -v 'already satisfied' # en_core_web_trf\n",
        "\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "############################################ pytorch ############################################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "############################################ sklearn ############################################\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "############################################ clean up pwd ############################################\n",
        "!rm -r /content/sample_data # remove default data folder\n",
        "\n",
        "############################################ set seed ############################################\n",
        "\n",
        "CONFIG = {\n",
        "    'seed': 23\n",
        "}\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    '''Sets seed so result unchanged - reproducibility'''\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "set_seed(CONFIG['seed'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuT2wVkd6G85"
      },
      "source": [
        "### Google drive connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryPD8got6IBA"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import numpy as np\n",
        "# Authenticate\n",
        "gdrive = None\n",
        "def authenticate():\n",
        "    global gdrive\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    gdrive = GoogleDrive(gauth)\n",
        "\n",
        "#Download files\n",
        "def downloadFiles(fileIds):\n",
        "    authenticate()\n",
        "    for fileId in fileIds:    \n",
        "        downloaded = gdrive.CreateFile({\"id\": fileId[1]})\n",
        "        downloaded.GetContentFile(fileId[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbNpVPu16J95"
      },
      "source": [
        "## Helper file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtGMwtgJ6K_p"
      },
      "outputs": [],
      "source": [
        "class Helper:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.pwd = '/content'\n",
        "        self.jigsaw_path = f'{self.pwd}/jigsaw'\n",
        "        self.featuers_path = f'{self.pwd}/features'\n",
        "        self.dota_path = f'{self.pwd}/dota'\n",
        "\n",
        "        # Supply dataset\n",
        "        self.gdrive_supply_files = {\n",
        "            'train_csv': ('train.csv', '1WBFZINdPeGN_86Mm_yyAEqDnZMCGdBP0'),\n",
        "            'val_csv': ('validation.csv', '1jCazOeqrgw2kghMoRdT6SPEQc7ZEkryj'),\n",
        "            'test_csv': ('test_without_labels.csv', '1mAhSjNNBALEY9c5yk70eA8SEo6lkeHrF'),\n",
        "            'sample_csv': ('sample.csv', '1WPCOgJDfggM58SXbLRJV9v3EMunzRBDX')\n",
        "        }\n",
        "        \n",
        "        # Jigsaw comment - https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/overview\n",
        "        self.gdrive_jigsaw = {\n",
        "            'jigsaw_train': ('jigsaw_train.csv', '1EM3s2ptPtJ6c_N5yT_NphzR8qXUy3ep0'),\n",
        "            'jigsaw_train_zip': ('train.csv.zip', '1Y9bSMSAx2Tvi64rLoKnecVWd4zOhzSQW')\n",
        "        }\n",
        "        # Dota 2 Wiki dataset\n",
        "        self.gdrive_dota = {\n",
        "            'dota_wiki': ('dota_wiki.txt', '18Md9xdJLVhIpSvm1Ws9J6Tb605ka24Rr')\n",
        "        }\n",
        "        self.gdrive_features = {\n",
        "            'pos_feaures': ('pos_features.npy', '187oMCdy7uOW9-orsk20MWw3pskYp1XXm'),\n",
        "            'dep_feaures': ('dep_features.npy', '1oG-XlC_N7lbkvHM6EvsMN57iffwoPRvg')\n",
        "        }\n",
        "        self.data = [self.gdrive_supply_files, self.gdrive_jigsaw, self.gdrive_features, self.gdrive_dota]\n",
        "\n",
        "\n",
        "        # Load all gdrive files\n",
        "        self.load_all_gdrive_files()\n",
        "\n",
        "        # move and organises folder\n",
        "        self.move_files_in_folder(self.gdrive_jigsaw, self.jigsaw_path)\n",
        "        self.move_files_in_folder(self.gdrive_features, self.featuers_path)\n",
        "        self.move_files_in_folder(self.gdrive_dota, self.dota_path)\n",
        "\n",
        "    def load_all_gdrive_files(self):\n",
        "        # 1: Collect \n",
        "        file_dict = {}\n",
        "        for d in self.data:\n",
        "            file_dict.update(d)\n",
        "        \n",
        "        # 2: Load every needed csv files into current pwd\n",
        "        for _, file in file_dict.items():\n",
        "            fname, f_link = file\n",
        "            self.load_one_gdrive_file(fname, f_link)\n",
        "                                   \n",
        "    def load_one_gdrive_file(self, fname:str, gdrive_link:str):\n",
        "        \"\"\"Download file if not existing\"\"\"\n",
        "        try:\n",
        "            _ = open(fname, \"r\")\n",
        "        except:\n",
        "            downloadFiles([[fname, gdrive_link]])\n",
        "            _ = open(fname, \"r\")\n",
        "\n",
        "    def move_files_in_folder(self, folder_dict:dict, folder_name:str):\n",
        "        \"\"\"Move files from pwd to a dedicated folder\"\"\"\n",
        "        \n",
        "        if not os.path.exists(folder_name):\n",
        "                os.makedirs(folder_name)\n",
        "\n",
        "        for _ , f in folder_dict.items():\n",
        "            fname, _ = f\n",
        "            shutil.move(f\"{self.pwd}/{fname}\", f\"{folder_name}/{fname}\")\n",
        "    # Note: Due to time contraints, this is the third worst code design I ever coded.\n",
        "    \"\"\"\n",
        "    from google.colab import files\n",
        "    files.download('pos_features.npy') \n",
        "    \"\"\"\n",
        "Myhelper = Helper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF6HMTOGhnSe"
      },
      "outputs": [],
      "source": [
        "def p(x):\n",
        "    print(x)\n",
        "def dbg(x):\n",
        "    print(\"#################################### \", x, \" ####################################\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F3zWZpkivO3"
      },
      "source": [
        "## Preprocessing file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLXJe-8diweG"
      },
      "outputs": [],
      "source": [
        "class Preprocessing:\n",
        "    def __init__(self) -> None:\n",
        "        # self.BOS = \"<BOS>\"\n",
        "        # self.EOS = \"<EOS>\"\n",
        "        self.PAD = \"<PAD>\"\n",
        "        self.SEPA = \"SEPA\"\n",
        "        self.ix_to_tag = None\n",
        "\n",
        "    ########################### 1 word tokenisation ###########################\n",
        "    def tokenise_words(self, sent_list):\n",
        "        output = []\n",
        "        for sent in sent_list:\n",
        "            # lowercase\n",
        "            sent = sent.lower()\n",
        "            output.append(sent.split())\n",
        "        return output\n",
        "\n",
        "    def tokenise_only(self, sent_list):\n",
        "        output = []\n",
        "        for sent in sent_list:\n",
        "            output.append(sent.split())\n",
        "        return output\n",
        "\n",
        "    def tokenise_tags(self, tag_list):\n",
        "        output = []\n",
        "        for tag in tag_list:\n",
        "            output.append(tag.split())\n",
        "        return output\n",
        "    \n",
        "    ########################### encode str to index ###########################\n",
        "\n",
        "    def get_word_to_ix(self, Xs):\n",
        "        \"\"\"set up a vocab to index dictionary\"\"\"\n",
        "\n",
        "        word_to_ix = {self.PAD:0}\n",
        "        assert type(Xs) == list, 'X_train + X_val + X_test'\n",
        "\n",
        "        for sentence in Xs:\n",
        "            for word in sentence:\n",
        "                word = word.lower()\n",
        "                if word not in word_to_ix:\n",
        "                    word_to_ix[word] = len(word_to_ix)\n",
        "        word_list = list(word_to_ix.keys())\n",
        "        return word_to_ix, word_list\n",
        "        \n",
        "    def get_tag_to_ix(self, ys):\n",
        "        assert type(ys) == list, 'y_train + y_val'\n",
        "\n",
        "        tag_to_ix = {self.PAD:0, self.SEPA:1} # all unique\n",
        "        for tags in ys:\n",
        "            for tag in tags:\n",
        "                if tag not in tag_to_ix:\n",
        "                    tag_to_ix[tag] = len(tag_to_ix)\n",
        "        self.ix_to_tag = {y: x for x, y in tag_to_ix.items()}\n",
        "        return tag_to_ix\n",
        "\n",
        "    def get_ix_to_tag(self):\n",
        "        return self.ix_to_tag\n",
        "\n",
        "    def encode_to_index_and_pad(self, sent_tokenised_list, dictionary, max_len=72):\n",
        "        \"\"\" construct token index lists for input, output and target \"\"\"\n",
        "        res = []\n",
        "        # TODO: decide whether to use tge largest or manully set one\n",
        "        \n",
        "        for sent in sent_tokenised_list:\n",
        "            encoded = [dictionary[t] for t in sent]\n",
        "            if len(encoded) < max_len:\n",
        "                encoded += [dictionary[self.PAD]] * (max_len - len(encoded)) \n",
        "            if len(encoded) > max_len:\n",
        "                encoded = encoded[:max_len]\n",
        "            res.append(encoded)\n",
        "        return res\n",
        "    def encode_to_index(self, token_list, dictionary):\n",
        "        \"\"\" construct token index lists for input, output and target \"\"\"\n",
        "        res = []\n",
        "        for tok in token_list:\n",
        "            res.append([dictionary[t] for t in tok])\n",
        "        return res\n",
        "    # def v2(self, sent_list):\n",
        "    #     output = []\n",
        "    #     for sent in sent_list:\n",
        "    #         sent = sent.lower()\n",
        "    #         for word, new_word in contraction_dict.items():\n",
        "    #             sent = sent.replace(word, new_word)\n",
        "    #         sent = re.sub(r'[^\\w\\s]','',sent)        \n",
        "    #         output.append(word_tokenize(sent))\n",
        "    #     return output\n",
        "\n",
        "Prep = Preprocessing()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONRab7y_Ct4l"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PSmVNOqntVE"
      },
      "source": [
        "### tokenisation demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZNSSnErlASl",
        "outputId": "f99cc207-58f4-4e86-9e9c-293387966f48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['the', 'comeback', 'is', 'real', ':', ')']\n",
            "['the', 'comeback', 'is', 'real', ':)']\n"
          ]
        }
      ],
      "source": [
        "# Based on below, we should not use word_token()\n",
        "tmp = 'i cant [SEPA] play [SEPA] with 4 trash'\n",
        "tmp = '; P ; BD'\n",
        "tmp = \"the comeback    is real :)\"\n",
        "p( word_tokenize(tmp) ) \n",
        "p ( tmp.split() ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xCqaSWVmnsz7",
        "outputId": "af942ccf-73ff-477d-e495-229956aeda2e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'just end [SEPA] i wan nex game'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: convert abbreviation to normal form\n",
        "\"\"\"just end [SEPA] i wan nex game\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmfr__YiSun5"
      },
      "source": [
        "## Embedding "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cA-k-ZXtWh-F"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=[\"lemmatizer\"])  # en_core_web_trf\n",
        "# tokenise based on whitespaces only\n",
        "nlp.tokenizer = Tokenizer(nlp.vocab, token_match=re.compile(r'\\S+').match)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIGkFbg9UxKW"
      },
      "outputs": [],
      "source": [
        "class Embedding():\n",
        "    def __init__(self):\n",
        "        # the accuracy one not the efficiency: https://spacy.io/usage\n",
        "        self.spacy_model = nlp\n",
        "        self.pos_list = [\"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CONJ\", \"CCONJ\", \"DET\", \"INTJ\", \"NOUN\", \"NUM\", \"PART\", \"PRON\", \"PROPN\", \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\", \"SPACE\"]\n",
        "        self.dep_list = self.spacy_model.get_pipe(\"parser\").labels\n",
        "\n",
        "    ########################### Feature embedding ###########################\n",
        "    def _np(self, f):\n",
        "        \"\"\"convert to numpy features\"\"\"\n",
        "        return np.array([f])\n",
        "\n",
        "    def get_dep_and_pos(self, text):\n",
        "        \"\"\"use spacy API to get POS tag and dependency path\n",
        "        return: list of predicted tags for each token\n",
        "        \"\"\"\n",
        "        # split by spaces only\n",
        "        \n",
        "        dep = []\n",
        "        pos = []\n",
        "        tokenised = self.spacy_model(text)\n",
        "        for tok in tokenised:\n",
        "            dep.append(tok.dep_) # dep_ gives the actual name while this gives the hash value\n",
        "            pos.append(tok.pos_)\n",
        "        return dep, pos\n",
        "\n",
        "    # def one_hot_encoding(self, label_list, actual_list):\n",
        "    #     \"\"\"one hot encoding for tags and dependency path\n",
        "    #     For example:\n",
        "    #     actual_list: ['Tom'] has actual_list=['NN']\n",
        "    #     label_list: ['NN', and other taggers]\n",
        "    #     \"\"\"\n",
        "    #     res = [0] * len(label_list)\n",
        "    #     for el in actual_list:\n",
        "    #         ix = label_list.index(el)\n",
        "    #         res[ix] = 1\n",
        "    #     return np.array(res)\n",
        "\n",
        "    def get_predicted_word_features(self, raw_data):\n",
        "        \"\"\"return the pos and depdency path feature for each seq\"\"\"\n",
        "        res = {\n",
        "            'pos':{}, # detail pos\n",
        "            'dep':{}\n",
        "        }\n",
        " \n",
        "        for seq in raw_data:\n",
        "            \n",
        "            dep, pos = self.get_dep_and_pos(seq)\n",
        "            \n",
        "            # below are operations for each word\n",
        "            s = seq.split()\n",
        "            \n",
        "            for i in range(len(s)):\n",
        "                word = s[i]\n",
        "                w = word.lower()\n",
        "                \n",
        "                if w not in res['pos'].keys():\n",
        "                    res['pos'][w] = [] \n",
        "                    res['dep'][w] = [] \n",
        "                    \n",
        "                res['pos'][w].append(self.pos_list.index(pos[i])) \n",
        "                res['dep'][w].append(self.dep_list.index(dep[i])) \n",
        " \n",
        "        # choose the most votes tag\n",
        "        for k, v in res['pos'].items():\n",
        "            res['pos'][k] = self._np(max(set(v), key=v.count))\n",
        "        for k, v in res['dep'].items():\n",
        "            res['dep'][k] = self._np(max(set(v), key=v.count))\n",
        "        return res\n",
        "\n",
        "    def get_word_features(self, raw_data):\n",
        "        \"\"\"return the len feature for each seq\"\"\"\n",
        "        data_ = Prep.tokenise_only(raw_data)\n",
        "        res = {\n",
        "            'len': {},\n",
        "            'cap':{}, # frequency of being cap\n",
        "        }\n",
        "\n",
        "        for seq in data_:\n",
        "            \n",
        "            for i in range(len(seq)):\n",
        "                word = seq[i]\n",
        "                w = word.lower()\n",
        "                \n",
        "                # len\n",
        "                if w not in res['len'].keys():\n",
        "                    res['len'][w] = self._np(len(word))\n",
        "                    res['cap'][w] = [] \n",
        "                else:\n",
        "                    if word.isupper():\n",
        "                        res['cap'][w].append(1) # old + new freq\n",
        "                    else:\n",
        "                        res['cap'][w].append(0)\n",
        "        # convert cap list to cap prob\n",
        "        for k, v in res['cap'].items():\n",
        "            if len(v) != 0:\n",
        "                res['cap'][k] = self._np(round(sum(v) / len(v), 2))\n",
        "            else:\n",
        "                res['cap'][k] = self._np(0)\n",
        "        return res\n",
        "\n",
        "    ########################################################################################################################################\n",
        "    \"\"\"\n",
        "    Build training model for word embeddings: You are to build a training model for word embeddings. \n",
        "    You are required to articulate the hyperparameters [Lab2] you choose (dimension of embeddings and window size) \n",
        "    in Section 4.1. Note that any word embeddings model [Lab2] (e.g. word2vec-CBOW, word2vec-Skip gram, fasttext, glove) \n",
        "    can be applied. (Section 4.1. and Section 4.3., Justify your decision)\n",
        "    \"\"\"\n",
        "    def make_self_trained_gensim_model(self, posts_, dimension_=25, window_=3, which_=1) -> Type[Word2Vec]:\n",
        "        \"\"\"contruct gensim word2vec model\n",
        "        Args:\n",
        "            posts_(List[str]): a list of string: either F - Feeler or T - Thinker\n",
        "        Returns:\n",
        "            (gensim.model.Word2Vec): gensim API to create word2vec model\n",
        "        \"\"\"\n",
        "        #TODO: params\n",
        "        models = {\n",
        "            1: FastText, # OOV\n",
        "            2: Word2Vec\n",
        "        }\n",
        "        return models[which_](sentences=posts_, size=dimension_, window=window_, min_count=5, workers=4, sg=1, seed=42)\n",
        "\n",
        "    def build_concat_embed_table(self, word_list_: list, embeddings:List[object]) -> np.array:\n",
        "        \"\"\" Build the embedding table from the pre-train model.\n",
        "        Extract those word embedding that exists in the vocab of the current dataset\n",
        "        Args:\n",
        "            word_list_(list): a list of unique word in the corpus\n",
        "        Returns:\n",
        "            embeddings(List[gensim.model]): a list of gensim models\n",
        "        The inner element should be in np array\n",
        "\n",
        "        a = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "        b = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "        np.concatenate((a, b), 0)\n",
        "        output: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "        \"\"\"\n",
        "        emb_dim = 0\n",
        "        emb_dim_list = []\n",
        "        for eb in embeddings:\n",
        "            # TODO\n",
        "            if type(eb) == dict:\n",
        "                emb_dim_list.append(next(iter(eb.values())).shape[0]) # the shape of one element in the dict\n",
        "            else:\n",
        "                emb_dim_list.append(eb.vector_size)\n",
        "        emb_dim = sum(emb_dim_list)\n",
        "\n",
        "        emb_table = []\n",
        "\n",
        "        # TODO: handle missing value in embedding\n",
        "        for word in word_list_:\n",
        "            concat = []\n",
        "            for i, eb in enumerate(embeddings):\n",
        "                \n",
        "                if word in eb:\n",
        "                    if concat == []: # not initalised\n",
        "                        concat = eb[word]\n",
        "                    else:\n",
        "                        concat = np.concatenate((concat, eb[word]), 0)\n",
        "                else:\n",
        "                    # use 0 to represent missing value instead e.g. [unk]\n",
        "                    unk = np.array([0]*emb_dim_list[i])\n",
        "                    if concat == []:\n",
        "                        concat = unk\n",
        "                    else:\n",
        "                        concat = np.concatenate((concat, unk), 0)\n",
        "            emb_table.append(concat)    \n",
        "\n",
        "        return np.array(emb_table), emb_dim\n",
        "\n",
        "    def get_domain_dataset(self):\n",
        "        # dota wiki dataset and jigsaw toxicity dataset preprocessing\n",
        "        # dota wiki dataset\n",
        "\n",
        "        with open('/content/dota/dota_wiki.txt') as f:\n",
        "            corpus = f.readlines()\n",
        "\n",
        "        # lower casing\n",
        "        corpus = [c.lower() for c in corpus]\n",
        "\n",
        "        # only alphabets\n",
        "        corpus = [re.sub(r'[^A-Za-z]+', ' ', c) for c in corpus]\n",
        "\n",
        "        # split\n",
        "        corpus = [c.split() for c in corpus]\n",
        "\n",
        "        # remove tokens that has len 1 (s from 's)\n",
        "        corpus = [[t for t in c if len(t) > 1] for c in corpus]\n",
        "\n",
        "        # remove short sentences with len < 5 as many of them are headings/titles of the wiki\n",
        "        corpus = [c for c in corpus if len(c) >= 5]\n",
        "\n",
        "        dota_corpus = corpus\n",
        "\n",
        "        # jigsaw toxicity dataset\n",
        "        toxic_df = pd.read_csv(\"/content/jigsaw/jigsaw_train.csv\")\n",
        "\n",
        "        only_toxic_df = toxic_df.query(\n",
        "            \"toxic == 1 or severe_toxic == 1 or obscene == 1 or threat == 1 or insult == 1 or identity_hate == 1\")\n",
        "\n",
        "        toxic_comments = only_toxic_df['comment_text'].tolist()\n",
        "\n",
        "        toxic_comments\n",
        "        # lower casing\n",
        "        corpus = [c.lower() for c in toxic_comments]\n",
        "\n",
        "        # only alphabets\n",
        "        corpus = [re.sub(r'[^A-Za-z]+', ' ', c) for c in corpus]\n",
        "\n",
        "        # split\n",
        "        corpus = [c.split() for c in corpus]\n",
        "\n",
        "        toxic_corpus = corpus\n",
        "        return dota_corpus, toxic_corpus\n",
        "\n",
        "Emb = Embedding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkNGPLrVvs0H"
      },
      "outputs": [],
      "source": [
        "# np.array([0]*10)\n",
        "# putoutput: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV7gbH2DuWI7"
      },
      "source": [
        "## Model file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZGaAv1bZEO1"
      },
      "source": [
        "### attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSO4yljKqF8z"
      },
      "outputs": [],
      "source": [
        "class Attention_type:\n",
        "    dot_product = 'dot_product'\n",
        "    scaled_dot_product = 'scaled_dot_product'\n",
        "    location_base = 'location_base'\n",
        "    additive  = 'additive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHdUFCcbiZtE"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\"\"\"\n",
        "    def __init__(self, input_dim, embed_dim, num_heads, attn_score=\"\"):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "        d_model = 256\n",
        "        self.attn_score = attn_score\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "       \n",
        "        # Stack all weight matrices 1...h together for efficiency\n",
        "        # Note that in many implementations you see \"bias=False\" which is optional\n",
        "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self._reset_parameters()\n",
        "        \n",
        "        \n",
        "    #################################################################################################################################\n",
        "\n",
        "    def attn_score_fn(self, q, k, v, mask=None):\n",
        "\n",
        "        d_k = q.size()[-1]\n",
        "        attn_logits = None\n",
        "\n",
        "        # torch.Size([20, 1, 72, 72]), [batch, nhead, dim, dim]\n",
        "        if self.attn_score == Attention_type.dot_product:\n",
        "            attn_logits = torch.matmul(q, k.transpose(-2, -1)) # [a, b] * [b, a] -> [a, a]\n",
        "        \n",
        "        elif self.attn_score == Attention_type.scaled_dot_product:\n",
        "            attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "            attn_logits = attn_logits / math.sqrt(d_k) # all non zero values\n",
        "\n",
        "        elif self.attn_score == Attention_type.additive:\n",
        "\n",
        "            attn_logits = q + k\n",
        "            attn_logits = torch.matmul( torch.tanh(attn_logits), k.transpose(-2, -1))\n",
        "            #attn_logits = torch.tanh(attn_logits)\n",
        "\n",
        "        elif self.attn_score == Attention_type.location_base:\n",
        "            # k torch.Size([20, 1, 72, 50])\n",
        "            # res \n",
        "            # cos = nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
        "            #attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "\n",
        "            q_ = F.softmax(q, dim=-1)\n",
        "            attn_logits = torch.matmul(q_, k.transpose(-2, -1))\n",
        "            #attn_logits = torch.matmul(torch.tanh(q_), k.transpose(-2, -1))\n",
        "            \n",
        "        if mask is not None:\n",
        "            \n",
        "            # print(mask.unsqueeze(1).unsqueeze(2).size()) # torch.Size([20, 1, 1, 72])\n",
        "            # those place with pad token whose value is 0 will receive the very negative value \n",
        "            # which make those position unlearnable in the embedding\n",
        "            attn_logits = attn_logits.masked_fill(\n",
        "                mask.unsqueeze(1).unsqueeze(2)==False,\n",
        "                float('-inf'),\n",
        "            )\n",
        "            #attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "      \n",
        "        attention = F.softmax(attn_logits, dim=-1)\n",
        "        values = torch.matmul(attention, v)\n",
        "        # print(values.size()) # torch.Size([20, 1, 72, 50])\n",
        "        return values, attention\n",
        "    #################################################################################################################################\n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        batch_size, seq_length, embed_dim = x.size()\n",
        "        qkv = self.qkv_proj(x)\n",
        "\n",
        "        # Separate Q, K, V from linear output\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # Determine value outputs\n",
        "        values, attention = self.attn_score_fn(q, k, v, mask)\n",
        "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
        "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
        "        o = self.o_proj(values)\n",
        "\n",
        "        if return_attention:\n",
        "            return o, attention\n",
        "        else:\n",
        "            return o\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
        "        self.qkv_proj.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.o_proj.bias.data.fill_(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhowyqwZZJ6N"
      },
      "source": [
        "### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGngepdPEF5c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class BiLSTM_attn(nn.Module):\n",
        "    \"\"\"minibatch\"\"\"\n",
        "    def __init__(self, args):\n",
        "        super(BiLSTM_attn, self).__init__()\n",
        "        \n",
        "        a = args\n",
        "        \n",
        "        self.embedding_dim = a['embedding_dim']\n",
        "        self.unit_hidden_dim = a['hidden_dim'] // 2\n",
        "        self.n_direction = a['n_direction']\n",
        "        self.hidden_dim = self.unit_hidden_dim * self.n_direction\n",
        "        self.vocab_size = a['vocab_size']\n",
        "        self.tag_to_ix = a['tag_to_ix']\n",
        "        self.tagset_size = len(a['tag_to_ix'])\n",
        "        self.word_embeddings = nn.Embedding(a['vocab_size'], a['embedding_dim'])\n",
        "        self.stack_layers = a['stack_layers']\n",
        "\n",
        "        self.batch_size = a['batch_size']\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(self.hidden_dim, self.tagset_size)\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "        \n",
        "        ##############################################################################################################################\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        embed_tmp = torch.from_numpy(a['embed_matrix'])\n",
        "        # freeze pretrained weights\n",
        "        if a['is_pretrained']:\n",
        "            embed_tmp[:,:pretrained_index].requires_grad = False\n",
        "\n",
        "        self.word_embeddings.weight.data.copy_(embed_tmp)\n",
        "        ##############################################################################################################################\n",
        "        # different setup\n",
        "        self.is_lstm2 = a['is_lstm2']\n",
        "        self.is_lstm3 = a['is_lstm3']\n",
        "        self.attn_name = a['attn_name'] \n",
        "        self.is_attn1 = a['is_attn1']\n",
        "        self.is_attn2 = a['is_attn2']\n",
        "        self.is_attn3 = a['is_attn3']\n",
        "        self.attn = MultiheadAttention(\n",
        "                input_dim=self.hidden_dim,\n",
        "                embed_dim=self.hidden_dim,\n",
        "                num_heads=a['nhead'],\n",
        "                attn_score=self.attn_name\n",
        "            )\n",
        "        ###################################################################################################################################################\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, self.unit_hidden_dim,\n",
        "                            num_layers=self.stack_layers, bidirectional=True if self.n_direction==2 else False, \n",
        "                            batch_first=True)\n",
        "        ###################################################################################################################################################\n",
        "        if self.is_lstm2:\n",
        "            self.lstm2 = nn.LSTM(self.hidden_dim, self.unit_hidden_dim,\n",
        "                            num_layers=self.stack_layers, bidirectional=True if self.n_direction==2 else False, \n",
        "                            batch_first=True)\n",
        "        if self.is_lstm3:\n",
        "            self.lstm3 = nn.LSTM(self.hidden_dim, self.unit_hidden_dim,\n",
        "                            num_layers=self.stack_layers, bidirectional=True if self.n_direction==2 else False, \n",
        "                            batch_first=True)\n",
        "        ###################################################################################################################################################\n",
        "        \n",
        "\n",
        "        self.print_config()\n",
        "        \n",
        "\n",
        "    def init_hidden(self):\n",
        "        h_0_size = self.n_direction * self.stack_layers\n",
        "        return (torch.randn(h_0_size, self.batch_size, self.unit_hidden_dim).to(device),\n",
        "                torch.randn(h_0_size, self.batch_size, self.unit_hidden_dim).to(device))\n",
        "\n",
        "    def forward(self, sentences, X_lengths, attn_pad_mask=None):\n",
        "        \"\"\"\n",
        "        X_lengths: longest sequence length - size(20)\n",
        "        \"\"\"\n",
        "        \n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "        batch_size, seq_len = sentences.size()\n",
        "\n",
        "        embeds = self.word_embeddings(sentences)\n",
        "    \n",
        "        if not self.is_attn1:\n",
        "            embeds = torch.nn.utils.rnn.pack_padded_sequence(embeds, X_lengths, batch_first=True, enforce_sorted=False)\n",
        "            lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "            # undo padding\n",
        "            lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True, total_length=seq_len)\n",
        "            \n",
        "        else:\n",
        "            lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "            lstm_out = self.attn(lstm_out)\n",
        "\n",
        "        ###################################################################################################################################################\n",
        "        if self.is_lstm2:\n",
        "            \n",
        "            if not self.is_attn2:\n",
        "                lstm_out = torch.nn.utils.rnn.pack_padded_sequence(lstm_out, X_lengths, batch_first=True, enforce_sorted=False)\n",
        "                lstm_out, self.hidden = self.lstm2(lstm_out)\n",
        "                lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True, total_length=seq_len)\n",
        "            else:\n",
        "                lstm_out, self.hidden = self.lstm2(lstm_out)\n",
        "                lstm_out = self.attn(lstm_out)\n",
        "        if self.is_lstm3:\n",
        "            \n",
        "            if not self.is_attn3:\n",
        "                lstm_out = torch.nn.utils.rnn.pack_padded_sequence(lstm_out, X_lengths, batch_first=True, enforce_sorted=False)\n",
        "                lstm_out, self.hidden = self.lstm3(lstm_out)\n",
        "                lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True, total_length=seq_len)\n",
        "            else:\n",
        "                lstm_out, self.hidden = self.lstm3(lstm_out)\n",
        "                lstm_out = self.attn(lstm_out)\n",
        "\n",
        "        res = self.hidden2tag(lstm_out.view(batch_size* seq_len, -1))\n",
        "        \n",
        "        return res\n",
        "        \n",
        "    def print_config(self):\n",
        "        pass\n",
        "        # if self.is_attn1 or self.is_attn2:\n",
        "        #     dbg(f\"Attention used: {self.attn_name}\")\n",
        "        # else:\n",
        "        #     dbg(\"No Attention used\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pAulWzXb0Om"
      },
      "outputs": [],
      "source": [
        " # input size: torch.Size([20, 72])\n",
        "# self.word_embed(sentence): torch.Size([20, 72, 50])\n",
        "# embeds.view(len(sentence), 1, -1).shape: torch.Size([20, 1, 3600])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9zosZ6pu7y5"
      },
      "source": [
        "## eval f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edGrLIEdG2Su"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def cal_f1(model_, input_batch, output_batch, len_batch, pad_batch):\n",
        "    \"\"\"ignore O, pad and sepa tag  accuracy\"\"\"\n",
        "    predicted = []\n",
        "\n",
        "    batch_size, seq_len = input_batch.size()\n",
        "    pred = model_(input_batch, len_batch).view(batch_size * seq_len, -1) # torch.Size([72, 10])\n",
        "    # row: token col: prob of such token on each tag\n",
        "    _, pred = torch.max(pred, 1)\n",
        "    predicted += list(pred.cpu().numpy()) # a list of all individual tag perdiction for all seq\n",
        "    \n",
        "    ground_truth = list(output_batch.view(batch_size * seq_len).cpu().numpy())\n",
        "    ###################################################################################################################################################\n",
        "    # ignore O and SEPA tag\n",
        "    ground_truth_no_O = []\n",
        "    predicted_no_O = []\n",
        "    ground_truth_O = []\n",
        "    predicted_O = []\n",
        "    for gold, pred in zip(ground_truth, predicted):\n",
        "\n",
        "        if pred == gold and ix_to_tag[gold] in [Prep.SEPA, Prep.PAD]:\n",
        "            continue\n",
        "        ground_truth_O.append(gold)\n",
        "        predicted_O.append(pred)  \n",
        "\n",
        "        if pred == gold and ix_to_tag[gold] == 'O':\n",
        "            continue\n",
        "        ground_truth_no_O.append(gold)\n",
        "        predicted_no_O.append(pred)\n",
        "\n",
        "    return (ground_truth_no_O, predicted_no_O), (ground_truth_O, predicted_O)\n",
        "\n",
        "def eval_f1(model_, generator, is_report=False) -> tuple:\n",
        "    \"\"\"Generates the model predictions using the input data and calculates the f1 by \n",
        "    comparing the model predictions with the ground truth labels.\n",
        "    \"\"\"\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    y_pred_report = []\n",
        "    y_true_report = []\n",
        "    for local_batch_, local_labels_, local_lens_, local_pad_ in generator:\n",
        "        \n",
        "        x, y = cal_f1(model_, local_batch_, local_labels_, local_lens_, local_pad_)\n",
        "        \n",
        "        y_true_report += y[0]\n",
        "        y_pred_report += y[1]\n",
        "\n",
        "        true, pred = x\n",
        "        y_pred += pred\n",
        "        y_true += true\n",
        "\n",
        "    f1 = f1_score(y_true, y_pred, average='micro', zero_division=1)\n",
        "    report = classification_report(y_true_report, y_pred_report, zero_division=1, output_dict=is_report)\n",
        "\n",
        "    return f1, report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3kicSytvMfI"
      },
      "outputs": [],
      "source": [
        "# create custom dataset class\n",
        "class CustomDataSet(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    References:\n",
        "    standford: https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
        "    \"\"\"\n",
        "    def __init__(self, text:list, labels:list, lens:list, pad:list):\n",
        "        \"\"\"Initialization\"\"\"\n",
        "        self.X = text\n",
        "        self.y = labels\n",
        "        self.ls = lens\n",
        "        self.pad = pad\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the total number of samples\"\"\"\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Generates one sample of data\"\"\"\n",
        "        X = self.X[idx]\n",
        "        y = self.y[idx]\n",
        "        l = self.ls[idx]\n",
        "        p = self.pad[idx]\n",
        "        return X, y, l, p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nrMpGXdn8aq"
      },
      "source": [
        "## pipeline - not batched"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeQGztGen_Dt"
      },
      "outputs": [],
      "source": [
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "\n",
        "        embed_matrix_, embed_matrix_dim_ = Emb.build_concat_embed_table(word_list, [fasttext_embed])\n",
        "        embedding_dim = embed_matrix_dim_\n",
        "        embed_matrix = embed_matrix_\n",
        "        \n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        \n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.n_direction = 2\n",
        "        self.stack_layers = 1\n",
        "            \n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embed_matrix))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=self.stack_layers, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix['<BOS>'], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix['<EOS>']] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        h_0_size = self.n_direction * self.stack_layers\n",
        "        return (torch.randn(h_0_size, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(h_0_size, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        \n",
        "        \"\"\"\n",
        "        https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention\n",
        "        Examples::\n",
        "\n",
        "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
        "        \"\"\"\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        return self.hidden2tag(lstm_out)\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # '<BOS>' has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix['<BOS>']] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix['<EOS>']]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix['<BOS>']], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix['<EOS>'], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix['<BOS>']] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to '<EOS>'\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix['<EOS>']]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix['<BOS>']  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "def cal_f1_pipline(model:object, input_index:list, output_index:list) -> tuple:\n",
        "    \"\"\"Generates the model predictions using the input data and calculates the f1 by \n",
        "    comparing the model predictions with the ground truth labels.\n",
        "    \"\"\"\n",
        "    predicted = []\n",
        "    for x in input_index:\n",
        "        pred = model(torch.from_numpy(np.array(x)).to(device)) # [(tensor(2.1390, device='cuda:0', grad_fn=<SelectBackward0>), [2]), \n",
        "        predicted += pred[1] # a list of all individual tag perdiction for all seq\n",
        "    # eval\n",
        "    # add dtype=object since the sequence is of different length\n",
        "    ground_truth = list(np.concatenate(np.array(output_index, dtype=object), axis=0 ))\n",
        "\n",
        "    # ignore O and SEPA tag\n",
        "    ground_truth_no_O = []\n",
        "    predicted_no_O = []\n",
        "    for gold, pred in zip(ground_truth, predicted):\n",
        "        if pred == gold and (ix_to_tag[gold] == 'O' or ix_to_tag[gold] == 'SEPA'):\n",
        "            continue\n",
        "        \n",
        "        ground_truth_no_O.append(gold)\n",
        "        predicted_no_O.append(pred)\n",
        "        \n",
        "    f1 = f1_score(ground_truth_no_O, predicted_no_O, average='micro', zero_division=1)\n",
        "    report = classification_report(ground_truth, predicted, zero_division=1)\n",
        "\n",
        "    return f1, report\n",
        "\n",
        "\n",
        "\n",
        "# Convert input to idx, vectoriser\n",
        "def pipeline(X_train, y_train, word_to_ix_, tag_to_ix):\n",
        "    tag_to_ix_ = {'<BOS>':0, '<EOS>':8}\n",
        "    tag_to_ix_.update(tag_to_ix)\n",
        "    \n",
        "    del tag_to_ix_['<PAD>']\n",
        "    \n",
        "    X_train_index_ =  Prep.encode_to_index(X_train,word_to_ix_)\n",
        "    y_train_index_ = Prep.encode_to_index(y_train,tag_to_ix_)\n",
        "    X_val_index_ = Prep.encode_to_index(X_val, word_to_ix_)\n",
        "    y_val_index_ = Prep.encode_to_index(y_val,tag_to_ix_)\n",
        "    X_test_index_ = Prep.encode_to_index(X_test,word_to_ix_)\n",
        "    fasttext_embed = Emb.make_self_trained_gensim_model(X_train+X_val+X_test, dimension_=50, window_=3) \n",
        "\n",
        "    HIDDEN_DIM = 50\n",
        "    N_EPOCH = 2\n",
        "    model_ = BiLSTM_CRF(len(word_to_ix_), tag_to_ix_, embed_matrix_dim, HIDDEN_DIM).to(device)\n",
        "    optimizer = optim.SGD(model_.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "    for epoch in range(N_EPOCH):  \n",
        "        time1 = datetime.datetime.now()\n",
        "        train_loss = 0\n",
        "\n",
        "        model_.train()\n",
        "        for i, idxs in enumerate(X_train_index_):\n",
        "            #print(i, idxs)\n",
        "            tags_index = y_train_index_[i]\n",
        "            \n",
        "            # Step 1. Remember that Pytorch accumulates gradients.\n",
        "            # We need to clear them out before each instance\n",
        "            model_.zero_grad()\n",
        "\n",
        "            # Step 2. Get our inputs ready for the network, that is,\n",
        "            # turn them into Tensors of word indices.\n",
        "            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "            # Step 3. Run our forward pass.\n",
        "            loss = model_.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "            # calling optimizer.step()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss+=loss.item()\n",
        "\n",
        "        model_.eval()\n",
        "        # evaluation\n",
        "\n",
        "        train_f1, _ = cal_f1_pipline(model_, X_train_index_, y_train_index_)\n",
        "        val_f1, val_report = cal_f1_pipline(model_, X_val_index_,y_val_index_)\n",
        "        time2 = datetime.datetime.now()\n",
        "\n",
        "        print(\"Epoch:%d, Training loss: %.2f, time: %.2fs\" %(epoch+1, train_loss, (time2-time1).total_seconds()))\n",
        "        print('train_f1', train_f1, 'val_f1', val_f1)\n",
        "        print(val_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFfHPq-DXGRN"
      },
      "source": [
        "# Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwKyCcFVXHEy"
      },
      "source": [
        "The multihead attetion is modified based on the course material from the University of Amsterdam.   \n",
        "Note that only one head is used for the experiment. Using multihead becaues it gives more flexibilty when need to upscale the model which is not the case in this relatively simple dataset.   \n",
        "Link here: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Group257A2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
