# Report - In-game Toxicity Detection
COMP5046 Assignment 2 

Link: https://www.kaggle.com/c/2022-comp5046-a2

# Data preprocessing

No content.

# Input Embedding

Input embeddings are created to represent words in a manner readable by machine learning models. We focus on three aspects, syntactic, semantic and domain-specific feature embeddings. These embeddings are concatenated to form different combinations of combined embeddings. If an embedding does not have weights for a given word, then such embedding is treated as zero while other embeddings in the combination will still be used.

## Syntactic

We consider four syntactic embedding methods. 

POS tagging categories words in a sentence into parts of speeches, such as nouns, verbs and adjectives. This may help identify target tags, for example, assume Dota-specific words such as hero or item names are always nouns, thus being able to identify a word as a noun may be a good indicator that it is also a Dota-specific word. Though in reality, there would also be non-nouns such as ‘feed’ and ‘gank’, so it is just an example. The tag embedding can be constructed by first passing the sentences from the CONDA dataset into the POS tagging model pretrained by the Spacy library, then recording the tag for each unique word. For each unique word, the most frequent tag will be its word embedding.

Dependency parsing assigns relationships between words within a sentence, such as in the sentence “She gave me a raise”, raise is the child of gave, with a dependency relationship of the direct object (dobj). Like POS tagging, these relations may be hints to the actual tag. The dependency embedding can be constructed by passing the sentences from the CONDA dataset into the dependency parsing model pretrained by the Spacy library, then recording the dependency for each unique word. For each unique word, the most frequent dependency will be its dependency embedding.

For POS tagging and dependency parsing, we used the en\_core\_web\_sm model from Spacy. We chose it because of its efficiency. It is at least 16 times faster than the en\_core\_web\_trf model when making inference, while its accuracy is not compromised by a lot when compared with the largest trf model, i.e. from 98% to 97% on POS tagging tasks, and 94% to 90% on dependency parsing tasks [1].

Capital form feature is considered because users can leverage the existing expressed meaning with it, such as stressing extreme happiness or angriness. Words in the dataset are lower-cased during the tokenization process, as we intend to reduce vocabulary redundancy in the corpus; thus, we retain the information of whether the word is originally capitalised with the help of the capital form embedding. The value includes the probability distribution of that word being capitalised and not being capitalised across the corpus.

The Word length feature is plainly the word length itself. It is chosen since some words are repeatedly long which the feature may give insight into the model.

## Semantic

We train prediction-based word models to generate features that represent the semantic relationship of words. Prediction-based models include Word2Vec, FastText, GloVe etc. These models in general can represent words in vectors that show semantic relationships such as in the form of “France - Paris equals to German - Berlin" [2]. 

Here we construct a FastText model trained on the given CONDA dataset. We select FastText because of its capability to deal with out-of-vocabulary issues. Intuitively, word spelling in-game chats are very flexible in the sense that players use a lot of nicknames/abbreviations, and also, they do not care much about typos, such as “pudg” as in the hero “pudge”. FastText constructs the word vector of each word with character n-grams, so the vector of “pudg”, if it is not in the dictionary, can be constructed by using part of the n-gram vectors of “pudge”.  

## Domain

We construct two domain-specific input embeddings to provide more task-related information to the machine learning model i.e. for the toxicity and Dota label tags. We train two FastText-based embeddings with different domain-specific datasets.

The Dota Embedding is trained on texts that we extracted from the subtitles of Dota 2 documentaries and game guides in YouTube, using the online scraping tool “https://downsub.com/”. We chose these materials in order to increase input’s relatedness to Dota specific words, such as hero, ability, and item names, and also words used frequently in-game such as “jungler”, “gank”, “carry”. Due to the different structures of the texts between the Dota dataset and the original CONDA dataset, we have different preprocessing methods. The texts are to be pre-processed with lower-casing, retaining only alphabetical words, removing single-charactered tokens (such as left out ‘s’ when removing the apostrophe), and removing over short sentences as we observed that they are usually not useful such as “(crowd cheers)”. After pre-processing, the data is trained by the FastText model.

`  	`The Jigsaw Embedding is trained on toxic comments from the Jigsaw Comment Classification Challenge from Kaggle (https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). In this case, we try to increase input’s relatedness to toxic language. To acquire the embedding, the texts are to be first pre-processed with lower-casing, and retaining only alphabetical words. Also, only non-toxic comments are filtered out because we only want to know more about toxic language. After pre-processing, the data is trained by FastText model.

# Slot Filling/Tagging model

The model we propose consists of three main components - the stacked seq2seq model, attention layer and CRF attachment.

# Stacked Seq2Seq model

We are using an RNN-based model. Specifically, we are using a multi-stack bi-directional LSTM model. Figure 1 shows the architecture of a sample of a 2-stack bi-direction LSTM model. At the bottom, we will have inputs of word embeddings of BOS/EOS tokens, SEPA tokens and words of the sentence. Then we have multiple layers of bi-LSTM, each represented by a forward LSTM and backward LSTM component. At the end of each layer, the output of the forward and backward components is concatenated (two directions are not treated separately) and passed as the input for the next layer. For our proposed model we have multiple stacks of layers. At the final layer, each output of the final layer will be passed to the attention layer.

![](Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.001.png)

Figure 1: Stacked LSTM architecture, adapted from [3]

RNN-based model is selected as the foundation of the model because of its capability of handling sequential data (sequences to sequences). They are also a few of the most common deep learning algorithms for tackling POS tagging tasks [4], which bear some similarities to our tagging task. Compared with RNN, LSTM models have less tendency to gradient explosion or vanishing through gating; meanwhile having bi-directional model trains the sequence data from both directions in order to achieve better results [5]. Finally, stacking multiple layers is also found to be the method to improve the performance of an LSTM model [6]. 

The optimal number of stacked layers is 1 based on the experiment section.

## Attention
### Attention score functions

Four attention scoring functions are investigated in the experiment section – scaled dot product, dot product, location-based and additive. The general format of the attention calculation is shown in Figure 2 where the function contains three main vectors – query (Q), key (K) and value (V). The idea (similar to how query works in the database system) is to map the query with a set of key-value pairs to an output, whose value is calculated with a suitable function such as softmax in Figure2. Note that the attention used is at sentence level instead of document level i.e. the sentences are fed into the model since the given dataset is in the form of sentences.

![Text Description automatically generated](Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.002.png)

Figure 2: attention calculation [7, Sec. 3.2.1]

Figure 3 shows the four attention functions that will be discussed in the report. 

The dot product attention calculation, namely, is the dot product of the query with all keys and similarly, the scaled dot product divides each result by a scaling factor. The output of these is then applied with a normalisation function e.g. softmax to calculate the weight which would be then multiplied by the values [7, Sec. 3.2.1]. The additive attention can be achieved via concatenation or addition between the st and hi matrixes with a tanh function applied while the location-based involves a trainable weight matrix and softmax function.

![A picture containing text, watch

Description automatically generated](Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.003.png)    ![Text Description automatically generated with medium confidence](Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.004.png)

` `![Text Description automatically generated with low confidence](Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.005.png)     ![](Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.006.png)

Figure 3: four attention score functions
### Self-Attention and positioning

There are three types of attention mechanisms - global, local and self-attention. In this report, self-attention is considered along with the Bi-LSTM model. This is because the other two mechanisms rely more on encoder and decoder architecture where the connection between the decoder hidden states and encoder states are explored. For example, they are often used in the machine translation field where the knowledge of the contextual understanding of the long sequences is important. 

The self-attention mechanism, on the other hand, considers the relationship between different positions of a single sequence [7, Sec. 2]. In the given dataset, the longest sequence length is 72 indicating that a comprehensive contextual understanding may not be required and thus self-attention is chosen. The short sequence length also means that the issue of self-attention operation scaling quadratically with the sequence length is not impactful [7, Sec. 3.5]**.** The flexibility of the self-attention implementation where any attention score functions can be used after replacing the target sequence with the input sequence is also a key factor why such a mechanism is chosen [8].

Three attention positions – after the first, second and third LSTM stack layers are examined as a result of the chosen self-attention. Note that the implementation involves multi-head attention, which is thought to give more flexibility when scaling the model complexity to improve performance. However, by the experiment below, it is shown that the dataset is relatively simple for the model to learn and thus only one head is chosen for the 
## CRF Attachment

The input of the concatenated last hidden states from the forward and backward direction is passed into conditional random field (CRF) layer as its emission probabilities.

The CRF is a probabilistic model for labelling sequence data. Similar probabilistic models include hidden Markov Models (HMM) and maximum entropy Markov models (MEMM). HMM and MEMM predicts a distribution of tags at each time step, and chains them to the next; on the other hand, CRF focuses on a global level as shown in Figure 4. It was shown that CRF performs better than the other two models in POS tagging tasks [9, Sec. 5.3].

![](Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.007.png)Figure 4 - chain structure between time-steps of HMM, MEMM and CRF [9, Fig. 2]

Combining the CRF model with the bi-LSTM layer helps us to solve the problem of bi-LSTM models being unable to handle strongly dependent tags well [10]. In a plain RNN-type model, the decision for each timestep is independent of the decision output tag of the previous timestamps, i.e. the output at time t will not care if the output at time t-1 is a C-tag or a D-tag. However, in practice there may be tags that are highly dependent on previous tags, e.g. in the case of NER tasks, I-PER is always followed after B-PER. CRF through its ability to learn in a sentence-wide manner can help to apply the implicit logic of the tags.

![](Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.008.png)

Figure 5: CRF layer on top of bi-LSTM [11]

# Evaluation
## General

Below is the default configuration applying to the experiment result for the ablation studies.  All the experiments are run on Tesla P100-PCIE-16GB GPU and CUDA 11.1. To maximize the chance of reproducibility, a seed is set wherever is possible (e.g. on the torch random generated tensor). T-F1 evaluation ignores “O”, “SEPA” tags and [PAD] tags if applicable. 


|**Configuration**|**Value**|
| :- | :- |
|Number of epochs|2|
|Evaluation metric|F1 score with micro weight. |
|Model|Bi-LSTM|
|Learning rate|0.01|
|Batch size|20|
|Model hidden dimension|50|
|Optimiser|Adam|
|Weight decay|1e-4|
|Input embedding dimension|50|
|Input embedding window|3|
|Gradient clipping|Yes|
Table 1: Configurations for ablation studies

## Performance Comparison

Our best model is a Bi-LSTM model. The configurations are described in Table 2. Note that gradient clipping, ReLU, freezing all embedding layers and adding training data from the validation set are also the keys to making such best model.


|**Configuration**|**Value**|
| :- | :- |
|Learning rate|5e-3|
|Batch size|20|
|Model hidden dimension|6|
|Optimiser|Adam|
|Weight decay|0|
|Embedding type|FastText on the given dataset, Dota, Jigsaw Toxicity, word being capital probability, POS tag, Dependency path, |
|Input embedding dimension|20|
|Input embedding window|4|
|Number of stack layer |1|
|CRF attachment|No|
|Attention |No|
Table 2: Configurations for the best model

There are several settings different from the baseline model configuration that we would specifically like to elaborate on.

We set the embedding window size to 4 in order to capture a more accurate context. From the Dota Embedding, we can see that there are many heroes or items with compound worded names, such as “Black King Bar” and “Eye of Skadi”. Having a window size that is too small may turn out capturing the proper noun but not the proper noun and its surrounding contexts. Secondly, the hidden dimension for the LSTM layer and input embedding sizes have been tuned down to reduce complexity. Thirdly, we combined the input embeddings proposed in Section 2 except word length, as the input of our best model. The word length embedding is left out because we are dubious of its effects.

## Ablation Studies

In the ablation studies, we will investigate how each part of the component of the system affects the overall performance. We focus on four major components, the embedding model, the attention strategy, the number of stacked layers and the usage of CRF. 

For the embedding model, we will first test one additional embedding together with the baseline embedding (FastText trained on given CONDA dataset). The top three additional embedding methods are then chosen as a combination of 2 additional embeddings together with the baseline embedding. Note that the pretrained word embedding is frozen during the training.

For the attention strategy, we will test different attention scoring functions and also different attention positions separately.

We will also study the effect of stacking different numbers of bi-LSTM layers and using with/without the CRF layer.

The general configurations already described in 4.1.1 and specific configurations will be different in sections that examine the impact of such configurations.  

## Evaluation result
# Experiment
   ## Performance Comparison
Table 3 shows that the Bi-LSTM model performed significantly well compared to the baseline model (Bi-LSTM CRF) which scored 0.85726 on the T-F1 according to the Kaggle benchmark. To view the details of the baseline score, we implemented the baseline model offline, which has better performance than the Kaggle benchmark but our best model still outperformed it. 

For the baseline model, the trend in Table 3 shows that the model generally predicted better on the tags O, P, and S while the scores on T, C, and D tags are relatively lower, especially for tag D which is around 20% less accurate. This trend, in general, follows the pattern of the size of each tag distribution which is O > P > S > C > T > D. It is because machine learning models would tend to choose majority classes in order to seek lower losses [12].

Our best model bettered the baseline model in all sub-scores. The improvement in detecting toxic tags and Dota-related tags is especially significant. It could be due to our inclusion of the Dota Embedding and the Jigsaw Embedding where they are specifically used to improve performance on the Toxic and Dota tags. The class with lowest score for the best model is the Character tag and is also the least performant class in the baseline model result. This may indicate that the characters may have a lot of nicknames or antonomasia (Gordon ➜ the great chef), which could not be captured by the Dota Embedding as the dataset was extracted from Dota game commentaries and guides, in which their choice of words tends to be more formal than actual in-game.


|**Metric**|
| :-: |
|**Model / F1 score**|T-F1|T-F1(P)|T-F1(O)|T-F1(S)|T-F1(C)|T-F1(T)|T-F1(D)|
|<p>Bi-LSTM + CRF </p><p>(Baseline)</p>|0.97168|1.0|0.99|0.99|0.94|0.98|0.98|
|<p>Bi-LSTM</p><p>(Best model)</p>|0.99085|1.0|0.99746|0.99464|0.98638|0.99367|1.0|
Table 3: Performance Evaluation on the Base model. T –F1 evaluates all tags; T (Toxicity), S (game Slang), C (Character), D (Dota-specific), P (Pronoun), and O (Other).

![Text Description automatically generated](Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.009.png) 
![Table Description automatically generated](Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.010.png)

Figure 6: Running logs of best model (left) and baseline model (right) with same setting.
# Ablation Study
##  different input embedding model
Table 4 shows that using the combination of the FastText embedding trained on the given CONDA dataset and the capital letter probability embedding gives us the highest score. Initially, the frequency of the word being full capital was thought to be a helpful feature, but it turns out to be a poor decision which makes the model less accurate. Then, the probability of such a word being full capital is used instead. The probabilistic information of it may have given it the edge in providing insightful information to the machine learning model, as compared to embeddings values that are constructed by majority counts like in POS tag feature and dependency path.

Some other combinations also give us comparable scores (less than 0.2% difference from the highest score), notably C+P, C+CP+DW and C+DW, which are combinations including the capital letter probability, the POS embedding and/or the Dota Embedding. We believe that the reason for the good performance with POS embedding is exactly the reason why we selected it as the embedding, that POS tags may have relations with our target tags. However, as we have explained that the POS embedding for each word is only the majority vote of it across the corpus, the word might not be having a correct POS tag, which may limit its performance. The Dota embedding, also as we have explained, aimed to provide Dota-specific contexts to the word. The embedding did not produce the best combination results, possibly because of its lack of informal Dota knowledge as mentioned in Section 4.2.1.

The remaining embedding methods seem to fall off in performance. Part of them are embeddings that were trained with the task-unrelated data, such as the pretrained Gensim model pretrained on tweets, which the performance was somehow expected as tweets have a fairly different nature to in-game chats. Nonetheless, we are pleased to see that all our proposed embeddings do improve the performance except the length embedding, which was also not surprising as words belonging to the same tag would have different lengths, such as a Dota character can be short as ‘Axe’ and long as ‘Bristleback’.

|**Combination / dataset**|**F1**|
| :-: | :-: |
|C+CP|0.94872|
|C+P|0.94831|
|C+CP+DW|0.94750|
|C+DW|0.94743|
|C+J|0.94206|
|C+CP+P|0.93926|
|C+G|0.93648|
|C+DP|0.93455|
|<p>C+DW+P </p><p>(semantic + domain + syntactic)</p>|0.93420|
|C|0.92217|
|C+L|0.92166|
Table 4: C: FastText on given CONDA dataset, CP: Capital letter probability, DP: dependency path, DW: FastText on Dota dataset, G: Pretrained Gensim Glove Twitter, J: FastText on Jigsaw Comment Classification Challenge, L: Length of each token, P: POS tag feature.
##  different attention strategy
Table 5 shows that the dot product yields the best score while the location-based is the least accurate. However, the performance range between the four attention strategies only differs by 0.05. 

The reason why the dot product performance is better than the scaled one could be due to the existing configuration of the model on minimizing the overfitting issues e.g. gradient clipping. These configurations reduce the impact of the scaling in the attention calculation and meanwhile the unscaled dot product can speed up the learning speed of the model given the limitation of 2 epochs. 

Therefore, the dot product can be seen as the optimal choice as it gives the best performance.


|**Strategy**|**F1 Score**|
| :-: | :-: |
|Dot Product|0.92159|
|Additive|0.89888|
|Scaled Dot Product|0.89136|
|Location-based|0.87120|
Table 5: different attention scoring functions

Table 6 shows that attention at the position of first layer is the best in terms of performance. This may be due to the higher importance of the early layers which gather more detailed information compares to the later ones that focus on the abstraction (bigger picture) [13] i.e. the earlier the attention involves, the more information extracted from the later layers. However, such trend could be caused by the impact of the number of stack layers since section 4.2.4 demonstrates that the longer the stack layers, the worse the performance. Therefore, the optimal attention position is after the first layer.


|**Attention Position (which layer)**|**F1 Score**|
| :-: | :-: |
|1|0.89727|
|2|` `0.86915|
|3|0.76964|
Table 6: Model performance on three attention position
##  different Stacked layer*
Table 7 shows that, in general, the fewer stacked layers the LSTM model has, the better the performance. The reason could be that the dataset is simple relative to the complexity of the model. Although the higher stack layers give the model more capability to project data on higher dimension space to draw the decision boundary between different classes, the complexity of such hidden dynamic is unhealthy for the model to learn the simple dataset, causing issues such as overfitting [14]. Thus, optimal number is 1.


|**Number of stack layers**|**F1 Score**|
| :-: | :-: |
|1|0.93388|
|2|0.94335|
|3|0.90298|
|4|0.64582 (failed to learn in 2 epochs)|
Table 7: Performance comparison on models with different numbers of stack layers.
##  with/without CRF
We have already mentioned that CRF can solve the dependent tag issue of an LSTM model and the study result seems to agree with the statement. Table 8 shows that the CRF attachment improved the performance of the Bi-LSTM model significantly. However, this comparison was not conducted under the optimal configuration from our best model. With hyperparameters fine-tuning, our best model achieved 99.085% f1 in validation score without CRF attachment, thus we decided to keep the model from further increasing complexity which may lead to overfitting issue.


|**CRF Attachment**|**F1 Score**|
| :-: | :-: |
|Yes|0.98005|
|No|0.94098|
Table 8: Performance comparison between Bi-LSTM with and without CRF layer.

**References**

1. “English · spaCy Models Documentation”, spacy.io, https://spacy.io/models/en (accessed: May 31, 2022).

1. T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, “Distributed Representations of Words and Phrases and their Compositionality” arXiv:1310.4546 [cs], Oct. 2013.

1. Z. Wang and J.-C. Liu, “Translating math formula images to latex sequences using deep neural networks with sequence-level training,” International Journal on Document Analysis and Recognition (IJDAR), 2020.

1. A. Chiche and B. Yitagesu, “Part of speech tagging: A systematic review of deep learning and machine learning approaches,” Journal of Big Data, vol. 9, no. 1, 2022.

1. S. Siami-Namini, N. Tavakoli, and A. S. Namin, “The performance of LSTM and BiLSTM in forecasting time series,” 2019 IEEE International Conference on Big Data (Big Data), 2019.

1. H. Liu, M. Liu, Y. Zhang, J. Xu, and Y. Chen, “Improved character-based Chinese dependency parsing by using stack-tree LSTM,” Natural Language Processing and Chinese Computing, pp. 203–212, 2018.

1. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is All You Need” arXiv:1706.03762 [cs], Jun. 2017.

1. J. Cheng, L. Dong, and M. Lapata, “Long Short-Term Memory-Networks for Machine Reading” arXiv: 1601.06733 [cs], Jun. 2017.

1. J. D. Lafferty, A. McCallum, and F. C. N. Pereira, “Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data”, Proceedings of the Eighteenth International Conference on Machine Learning, pp. 282–289, Jul. 2001.

1. H. Wei, M. Gao, A. Zhou, F. Chen, W. Qu, C. Wang, and M. Lu, “Named entity recognition from biomedical texts using a fusion attention-based BILSTM-CRF,” IEEE Access, vol. 7, pp. 73627–73636, 2019.

1. Z. Huang, W. Xu, and K. Yu, “Bidirectional LSTM-CRF Models for Sequence Tagging” arXiv: 1508.01991 [cs], Sep. 2016.

1. S. Kotsiantis, D. Kanellopoulos, and P. E. Pintelas, “Handling imbalanced datasets: A review”. GESTS International Transactions on Computer Science and Engineering, vol. 30, pp. 25-36, 2006.

1. Santiago A. Cadena, Marissa A. Weis, Leon A. Gatys, Matthias Bethge, Alexander S. Ecker; Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 217-232

1. J. Lever, M. Krzywinski, and N. Altman, “Model selection and overfitting,” Nature Methods, vol. 13, no. 9, pp. 703–704, 2016.


