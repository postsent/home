{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUhSNlPl7dhM"
      },
      "source": [
        "# Data Preprocessing and Performance Tuning with Spark\n",
        "A2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKwJ8P3dJ402",
        "outputId": "a8da8bf6-5305-4316-a253-e86c830fc2b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 281.3 MB 48 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199 kB 65.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=8cdf71980cfd86c40db93f3f91dba659e7a5be15e959cdc25008ab2b6fee7737\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzu_iwkO7dhP"
      },
      "source": [
        "```\n",
        "# chmod 700 submit_client.sh\n",
        "# ./submit_client.sh test.json output.json\n",
        "spark-submit \\\n",
        "    --master yarn \\\n",
        "    --deploy-mode client \\\n",
        "    --executor-cores 4 \\\n",
        "    --conf spark.checkpoint.compress=True \\\n",
        "    --conf spark.rdd.compress=True \\\n",
        "    --driver-memory 2g \\\n",
        "    --conf spark.shuffle.file.buffer=96k \\\n",
        "    preprocess.py \\\n",
        "    --input $1 \\\n",
        "    --output $\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy_rDHldJm7B",
        "outputId": "45c45853-c7ea-410f-f025-57c8e8b62345"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ’Ž Read in filename is: test.json\n",
            "ðŸ’Ž Output filename is: output.json\n",
            "\n",
            "\n",
            " ðŸ’Ž The initial input dataframe ðŸ’Ž len:  102 \n",
            "\n",
            "root\n",
            " |-- paragraph: struct (nullable = true)\n",
            " |    |-- context: string (nullable = true)\n",
            " |    |-- qas: array (nullable = true)\n",
            " |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |-- answers: array (nullable = true)\n",
            " |    |    |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |    |    |-- answer_start: long (nullable = true)\n",
            " |    |    |    |    |    |-- text: string (nullable = true)\n",
            " |    |    |    |-- id: string (nullable = true)\n",
            " |    |    |    |-- is_impossible: boolean (nullable = true)\n",
            " |    |    |    |-- question: string (nullable = true)\n",
            "\n",
            "\n",
            " ðŸ’Ž Segments for all contract dataframe (Context to sequences) ðŸ’Ž len:  2380 \n",
            "\n",
            "+--------------------+\n",
            "|                 seq|\n",
            "+--------------------+\n",
            "|{LohaCompanyltd_2...|\n",
            "|{LohaCompanyltd_2...|\n",
            "+--------------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "\n",
            " ðŸ’Ž all types of samples dataframe ðŸ’Ž len:  99392 \n",
            "\n",
            "+--------+---+-----+---+----------+--------------------+--------------------+------------+----------+\n",
            "|sampleID|qID|seqID|cID|negativity|              source|            question|answer_start|answer_end|\n",
            "+--------+---+-----+---+----------+--------------------+--------------------+------------+----------+\n",
            "|   10532|  0|    0|  0|         0|EXHIBIT 10.2   EN...|Highlight the par...|          15|        47|\n",
            "|   10533|  1|    0|  0|         0|EXHIBIT 10.2   EN...|Highlight the par...|         221|       232|\n",
            "|   10534|  1|    0|  0|         0|EXHIBIT 10.2   EN...|Highlight the par...|         221|       232|\n",
            "+--------+---+-----+---+----------+--------------------+--------------------+------------+----------+\n",
            "only showing top 3 rows\n",
            "\n",
            "\n",
            " ðŸ’Ž positive samples dataframe ðŸ’Ž len:  4889 \n",
            "\n",
            "+--------+---+-----+---+----------+--------------------+--------------------+------------+----------+\n",
            "|sampleID|qID|seqID|cID|negativity|              source|            question|answer_start|answer_end|\n",
            "+--------+---+-----+---+----------+--------------------+--------------------+------------+----------+\n",
            "|   10532|  0|    0|  0|         0|EXHIBIT 10.2   EN...|Highlight the par...|          15|        47|\n",
            "|   10533|  1|    0|  0|         0|EXHIBIT 10.2   EN...|Highlight the par...|         221|       232|\n",
            "|   10534|  1|    0|  0|         0|EXHIBIT 10.2   EN...|Highlight the par...|         221|       232|\n",
            "+--------+---+-----+---+----------+--------------------+--------------------+------------+----------+\n",
            "only showing top 3 rows\n",
            "\n",
            "\n",
            " ðŸ’Ž possible negative samples dataframe ðŸ’Ž len:  37839 \n",
            "\n",
            "+--------+---+-----+---+----------+--------------------+--------------------+------------+----------+\n",
            "|sampleID|qID|seqID|cID|negativity|              source|            question|answer_start|answer_end|\n",
            "+--------+---+-----+---+----------+--------------------+--------------------+------------+----------+\n",
            "|   10580|  0|    1|  0|         1|e to the **NFL Al...|Highlight the par...|           0|         0|\n",
            "|   10581|  1|    1|  0|         1|e to the **NFL Al...|Highlight the par...|           0|         0|\n",
            "|   10582|  2|    1|  0|         1|e to the **NFL Al...|Highlight the par...|           0|         0|\n",
            "+--------+---+-----+---+----------+--------------------+--------------------+------------+----------+\n",
            "only showing top 3 rows\n",
            "\n",
            "\n",
            " ðŸ’Ž impossible negative samples dataframe ðŸ’Ž len:  56664 \n",
            "\n",
            "+--------+---+-----+---+----------+--------------------+--------------------+------------+----------+\n",
            "|sampleID|qID|seqID|cID|negativity|              source|            question|answer_start|answer_end|\n",
            "+--------+---+-----+---+----------+--------------------+--------------------+------------+----------+\n",
            "|   10542|  4|    0|  0|         2|EXHIBIT 10.2   EN...|Highlight the par...|           0|         0|\n",
            "|   10543|  4|    0|  0|         2|EXHIBIT 10.2   EN...|Highlight the par...|           0|         0|\n",
            "|   10544|  4|    0|  0|         2|EXHIBIT 10.2   EN...|Highlight the par...|           0|         0|\n",
            "+--------+---+-----+---+----------+--------------------+--------------------+------------+----------+\n",
            "only showing top 3 rows\n",
            "\n",
            "\n",
            " ðŸ’Ž count number to balanced for impossible samples dataframe ðŸ’Ž len:  628 \n",
            "\n",
            "+-------+-------+---------+\n",
            "|cID_imp|qID_imp|count_imp|\n",
            "+-------+-------+---------+\n",
            "|      6|     13|        0|\n",
            "|     28|     12|        0|\n",
            "+-------+-------+---------+\n",
            "only showing top 2 rows\n",
            "\n",
            "\n",
            " ðŸ’Ž count number to balanced for possible negative samples dataframe ðŸ’Ž len:  1224 \n",
            "\n",
            "+---------+---------+----------+\n",
            "|cID_possi|qID_possi|count_pneg|\n",
            "+---------+---------+----------+\n",
            "|        0|        1|         7|\n",
            "|        0|        0|         1|\n",
            "+---------+---------+----------+\n",
            "only showing top 2 rows\n",
            "\n",
            "\n",
            " ðŸ’Ž input dataframe to the UDF of balancing the impossible samples ðŸ’Ž len:  628 \n",
            "\n",
            "+---+---+---------+--------------------+------+\n",
            "|cID|qID|count_imp|                 imp|   pos|\n",
            "+---+---+---------+--------------------+------+\n",
            "|  6| 13|        0|[{seqID -> 0, sam...|  null|\n",
            "| 28| 12|        0|[{seqID -> 0, sam...|[5, 6]|\n",
            "+---+---+---------+--------------------+------+\n",
            "only showing top 2 rows\n",
            "\n",
            "\n",
            " ðŸ’Ž output dataframe to the UDF of balancing the impossible samples ðŸ’Ž len:  628 \n",
            "\n",
            "+---+---+-----------+---------+\n",
            "|cID|qID|imp_id_keep|seqID_imp|\n",
            "+---+---+-----------+---------+\n",
            "|  6| 13|         []|       []|\n",
            "| 28| 12|         []|       []|\n",
            "+---+---+-----------+---------+\n",
            "only showing top 2 rows\n",
            "\n",
            "\n",
            " ðŸ’Ž input dataframe to the UDF of balancing the possible negative samples ðŸ’Ž len:  1224 \n",
            "\n",
            "+---+---+----------+--------------------+--------------------+---------+\n",
            "|cID|qID|count_pneg|               possi|                 pos|seqID_imp|\n",
            "+---+---+----------+--------------------+--------------------+---------+\n",
            "|  0|  1|         7|[{seqID -> 1, sam...|[0, 0, 0, 0, 0, 0...|     null|\n",
            "|  0|  0|         1|[{seqID -> 1, sam...|                 [0]|     null|\n",
            "+---+---+----------+--------------------+--------------------+---------+\n",
            "only showing top 2 rows\n",
            "\n",
            "\n",
            " ðŸ’Ž output dataframe to the UDF of balancing the possible negative samples ðŸ’Ž len:  1224 \n",
            "\n",
            "+-------+\n",
            "|    res|\n",
            "+-------+\n",
            "|[10581]|\n",
            "|[10580]|\n",
            "+-------+\n",
            "only showing top 2 rows\n",
            "\n",
            "\n",
            " ðŸ’Ž All selected samples for output ðŸ’Ž len:  9595 \n",
            "\n",
            "+--------------------+--------------------+------------+----------+\n",
            "|              source|            question|answer_start|answer_end|\n",
            "+--------------------+--------------------+------------+----------+\n",
            "|EXHIBIT 10.2   EN...|Highlight the par...|          15|        47|\n",
            "|EXHIBIT 10.2   EN...|Highlight the par...|         221|       232|\n",
            "+--------------------+--------------------+------------+----------+\n",
            "only showing top 2 rows\n",
            "\n",
            "\n",
            " ðŸ’Ž All selected samples with full details ðŸ’Ž \n",
            "\n",
            "+---+---+--------+-----+----------+--------------------+--------------------+------------+----------+----------+---------+\n",
            "|cID|qID|sampleID|seqID|negativity|              source|            question|answer_start|answer_end|count_pneg|count_imp|\n",
            "+---+---+--------+-----+----------+--------------------+--------------------+------------+----------+----------+---------+\n",
            "|  0|  1|   10534|    0|         0|EXHIBIT 10.2   EN...|Highlight the par...|         221|       232|         7|     null|\n",
            "|  0|  1|   10538|    0|         0|EXHIBIT 10.2   EN...|Highlight the par...|         221|       232|         7|     null|\n",
            "|  0|  1|   10539|    0|         0|EXHIBIT 10.2   EN...|Highlight the par...|         221|       232|         7|     null|\n",
            "+---+---+--------+-----+----------+--------------------+--------------------+------------+----------+----------+---------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, ArrayType, BooleanType\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import explode, udf, col, count, monotonically_increasing_id, countDistinct, lit, create_map, collect_list\n",
        "from pyspark.sql import SparkSession\n",
        "import argparse\n",
        "\n",
        "\"\"\" Documentation for the below code\n",
        "\n",
        "The code follows the Table of content in the report.\n",
        "\n",
        "1. Get segments\n",
        "2. Get samples based on segments and answers\n",
        "3. Assign various ids column\n",
        "4. Count for impossible negative samples needed\n",
        "5. Count for possible negative samples needed\n",
        "6. Select impossible based on count and positive sample for uniqueness\n",
        "7. Select possible based on count and positive and impossible sample for uniqueness\n",
        "8. Union all selected sampels\n",
        "9. Output to Json given the filename \n",
        "\n",
        "Variable name:\n",
        "    _imp - impossible sample\n",
        "    _possi / _pneg - possible negative sample\n",
        "    _pos - positive sample\n",
        "\n",
        "Dataframe column name added:\n",
        "    sampleID - unique ID for each sample\n",
        "    cID - ID for each contract\n",
        "    seqID - ID for each split seqence within a context (source of ans)\n",
        "    qID - ID for each question 0 ~ 40 (41 in total)\n",
        "    contract_title - as name\n",
        "    negativity - 0 for positive, 1 for possible negative, 2 for impossible\n",
        "\n",
        "References:\n",
        "    [1] https://stackoverflow.com/questions/46584460/how-can-i-add-continuous-ident-column-to-a-dataframe-in-pyspark-not-as-monoto\n",
        "\"\"\"\n",
        "\n",
        "def p(x):\n",
        "    print('\\n', x, '\\n')\n",
        "def plen(x):\n",
        "    print('\\n', 'len: ', str(x), '\\n')\n",
        "\n",
        "# each value represents a type of sample, three in total\n",
        "\n",
        "negativity_dict = {\n",
        "    'positive': 0,\n",
        "    'possible_negative':1,\n",
        "    'impossible': 2\n",
        "}\n",
        "\n",
        "# test_data = 'CUADv1.json'\n",
        "test_data = 'test.json' \n",
        "output_path = \"output.json\"\n",
        "\n",
        "spark = SparkSession.builder.appName(\"COMP5349 A2\").getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "# # use the small dataset as default input\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument(\"--input\", help=\"the input path\", type=str, default='test.json') \n",
        "# parser.add_argument(\"--output\", help=\"the output path\", type=str, default='output.json')\n",
        "# try:\n",
        "#     args = parser.parse_args()\n",
        "# except Exception as e:\n",
        "#     print(e)\n",
        "#     raise('Insufficient argument provided, please provide the input file name and output filename')\n",
        "\n",
        "# if args.input:\n",
        "#     test_data = args.input\n",
        "\n",
        "# if args.output:\n",
        "#     output_path = args.output\n",
        "\n",
        "print()\n",
        "print(f'ðŸ’Ž Read in filename is: {test_data}')\n",
        "print(f'ðŸ’Ž Output filename is: {output_path}')\n",
        "print()\n",
        "\n",
        "test_df = spark.read.json(test_data)\n",
        "test_data_df= test_df.select((explode(\"data\").alias('data')))\n",
        "test_paragraph_df = test_data_df.select(explode(\"data.paragraphs\").alias('paragraph'))\n",
        "\n",
        "print('\\n', 'ðŸ’Ž The initial input dataframe ðŸ’Ž', 'len: ', test_paragraph_df.count(), '\\n')\n",
        "test_paragraph_df.printSchema()\n",
        "############################################################################################################################################\n",
        "########################################################## Convert to sequences ############################################################\n",
        "############################################################################################################################################\n",
        "\n",
        "seq_schema = ArrayType(\n",
        "    StructType([\n",
        "        StructField('contract_title', StringType()),\n",
        "        StructField('seqID', IntegerType()),\n",
        "        StructField('seq', StructType([\n",
        "                StructField('sent', StringType()),\n",
        "                StructField('s', IntegerType()),\n",
        "                StructField('e', IntegerType())      \n",
        "            ])          \n",
        "        ),\n",
        "        StructField('qas', ArrayType(\n",
        "                StructType([\n",
        "                    StructField('answers', ArrayType(\n",
        "                        StructType([\n",
        "                            StructField('answer_start', StringType()),\n",
        "                            StructField('text', StringType())                              \n",
        "                        ])\n",
        "                    )           \n",
        "                    ),\n",
        "                    StructField('id', IntegerType()),\n",
        "                    StructField('is_impossible', BooleanType()),\n",
        "                    StructField('question', StringType())      \n",
        "                ])    \n",
        "            ) \n",
        "        )\n",
        "    ])   \n",
        ")     \n",
        "def get_default_seq(title_, seqID_, seq_, qas_list):\n",
        "    \"\"\"\n",
        "    {\n",
        "        'seq': (),\n",
        "        'qas': [{\n",
        "            'answers': [{\n",
        "                'answer_start':0,\n",
        "                'text': ''\n",
        "            }],\n",
        "            'id': 0,\n",
        "            'is_impossible':True,\n",
        "            'question': ''\n",
        "        }]\n",
        "    }\n",
        "    \"\"\"\n",
        "    assert type(title_) == str, 'get_default_seq: type(title_)'\n",
        "    assert type(seqID_) == int, 'get_default_seq: type(seqID_)'\n",
        "    assert type(seq_) == tuple, 'get_default_seq: type(seq_)'\n",
        "    assert type(qas_list) == list, 'get_default_seq: type(qas_list) '\n",
        "    return {\n",
        "        'contract_title': title_, # contract title\n",
        "        'seqID': seqID_, # sequence id for each contract\n",
        "        'seq': seq_, # actual seq\n",
        "        'qas': qas_list # list of qas\n",
        "    }\n",
        "\n",
        "# convert to widnow seq with each assigned with a qas list\n",
        "@udf(returnType=seq_schema)\n",
        "def get_seq_list(row, win_size=4096,stride=2048):\n",
        "    \"\"\"\n",
        "    | contract title | seq | qas |\n",
        "\n",
        "    each row is for a contract, contains a list of sliding window\n",
        "    only the text in the context fields are to be segmented\n",
        "    format: (window, start_idx, end_idx)\n",
        "    \"\"\"\n",
        "    context = row['context']\n",
        "    contract_title = row['qas'][0]['id']\n",
        "    idx = contract_title.rfind('__') # last occurrence of a substring in a string\n",
        "    contract_title = contract_title[:idx]\n",
        "\n",
        "    sliding_window = []\n",
        "    seqID = 0\n",
        "    for start in range(0, len(context), stride):\n",
        "        seq = (context[start:start+win_size], start, start+win_size)\n",
        "        seq_dict = get_default_seq(contract_title, seqID, seq, row['qas'])\n",
        "\n",
        "        sliding_window.append(seq_dict)\n",
        "        seqID += 1\n",
        "    return sliding_window\n",
        "############################################################################################################################################\n",
        "################################################################# END ######################################################################\n",
        "############################################################################################################################################\n",
        "\n",
        "# |seq| - convert full context to different sequences of at most 4096 length\n",
        "seq_df_raw = test_paragraph_df.select(explode(get_seq_list('paragraph')).alias('seq')).cache()\n",
        "\n",
        "print('\\n', 'ðŸ’Ž Segments for all contract dataframe (Context to sequences) ðŸ’Ž', 'len: ', seq_df_raw.count(), '\\n')\n",
        "seq_df_raw.show(2)\n",
        "\n",
        "\n",
        "# |cID|contract_title| - [1] create id column for each unique contract for later add contract id to samples, since compare integer is much faster than string contract title\n",
        "contract_df = seq_df_raw.select('seq.contract_title').distinct()\n",
        "contract_df = contract_df.rdd.zipWithIndex().map(lambda x: [x[1]] + [y for y in x[0]]).toDF(['cID']+contract_df.columns)\n",
        "contract_df = contract_df.select('cID', 'contract_title')\n",
        "\n",
        "\n",
        "\n",
        "# |contract_title|seqID|seq| qas| - expand dict to multi cols - extract windows\n",
        "seq_df = seq_df_raw.select(col(\"seq.contract_title\").alias(\"contract_title\"), col('seq.seqID').alias('seqID'), col(\"seq.seq\").alias(\"seq\"), col(\"seq.qas\").alias(\"qas\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################################################################################################################################\n",
        "############################################### get different samples - assign sequences with each questions ###############################\n",
        "############################################################################################################################################\n",
        "sample_schema = ArrayType(\n",
        "    StructType([\n",
        "        StructField('seqID', IntegerType()),\n",
        "        StructField('qID', IntegerType()),\n",
        "        StructField('contract_title', StringType()),\n",
        "        StructField('negativity', IntegerType()),\n",
        "        StructField('source', StringType()),\n",
        "        StructField('question', StringType()),\n",
        "        StructField('answer_start', IntegerType()),\n",
        "        StructField('answer_end', IntegerType())         \n",
        "    ])\n",
        ")\n",
        "\n",
        "def get_default_sample(seqID, qID, c:str, s:str, q:str):\n",
        "    \"\"\"\n",
        "    relation:\n",
        "    answer : seq -> 1 : 1\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    assert c != None, \"get_default_sample()\"\n",
        "    assert s != None, \"get_default_sample()\"\n",
        "    assert q != None, \"get_default_sample()\"\n",
        "\n",
        "    return {\n",
        "        'seqID': seqID,\n",
        "        'qID':qID,\n",
        "        'contract_title': c,\n",
        "        'negativity': negativity_dict['possible_negative'], \n",
        "        'source': s,\n",
        "        'question': q,\n",
        "        'answer_start': 0,\n",
        "        'answer_end': 0\n",
        "    }\n",
        "\n",
        "@udf(returnType=sample_schema)\n",
        "def get_sample_list(seqID, contract_title, window, qas_list):\n",
        "    \"\"\"\n",
        "    | contract_title | source | negativity |\n",
        "    \"\"\"\n",
        "    #contract_title, window, qas_list = row['cID'], row['seq'], row['qas']\n",
        "    \n",
        "    sample_list = []\n",
        "\n",
        "    window_context, start_idx, end_idx = window['sent'], window['s'], window['e']\n",
        "    start_idx = int(start_idx)\n",
        "\n",
        "    # 41 questions of differen categories\n",
        "    qID = 0\n",
        "    for qas_pair in qas_list:\n",
        "        ans_list = qas_pair['answers']\n",
        "        question = qas_pair['question']\n",
        "\n",
        "        # get default sample\n",
        "        sample = get_default_sample(seqID, qID, contract_title, window_context, question)\n",
        "        \n",
        "        # no answer in seq -> 0 for index\n",
        "        \n",
        "        if qas_pair['is_impossible']:\n",
        "            sample['negativity'] = negativity_dict['impossible']\n",
        "            sample_list.append(sample)\n",
        "            continue\n",
        "\n",
        "        # If is_impossible is False, then the question has an answer. \n",
        "        # change answer start and end if the widnow is a part of the answer\n",
        "        has_positive = False\n",
        "        for ans in ans_list:\n",
        "            answer_start, answer_text = ans\n",
        "            answer_start = int(answer_start)\n",
        "            \n",
        "            \"\"\"\n",
        "            In a JSON output object, answer_end = answer_start + length of segment that includes answer \n",
        "            if and only if the object's source field contains part of the answer. Otherwise, answer_end = answer_start = 0.\n",
        "            \"\"\"\n",
        "            # answer_end = len(answer_text) + answer_start \n",
        "            # answer_range_start = start_idx >= answer_start and start_idx <= answer_end\n",
        "            # answer_range_end = end_idx >= answer_start and end_idx <= answer_end\n",
        "\n",
        "            answer_end = len(answer_text) + answer_start \n",
        "            answer_range_start = answer_start >= start_idx and answer_start <= end_idx\n",
        "            answer_range_end = answer_end >= start_idx and answer_end <= end_idx\n",
        "\n",
        "            # positive sample - the widnow context is part of the answer \n",
        "            if answer_range_start or answer_range_end:\n",
        "                \"\"\"\n",
        "                In a JSON output object, answer_start refers to a location in the source field of the same output object; \n",
        "                it does not refer to a location in the context field of the original contract object.\n",
        "                \"\"\"\n",
        "                sample['negativity'] = negativity_dict['positive'] # positive\n",
        "                # relative to the source not the original context position\n",
        "                # max() - aovid negative idx\n",
        "                start = answer_start if answer_start > start_idx else start_idx \n",
        "                source_start = start - start_idx\n",
        "                source_end = answer_end - start_idx\n",
        "                sample['answer_start'] = source_start\n",
        "                sample['answer_end'] = source_end\n",
        "                \"\"\"\n",
        "                If a particular sequence contains more than one answer for a particular question, \n",
        "                then there must be one extra sample for each additional answer.\n",
        "                \"\"\"\n",
        "                sample_list.append(sample)\n",
        "                has_positive = True\n",
        "\n",
        "        # possible negative - seq that has is_impossible = False but does not contains any answer\n",
        "        if not has_positive:\n",
        "            sample_list.append(sample)\n",
        "        qID += 1\n",
        "    return sample_list\n",
        "############################################################################################################################################\n",
        "####################################################################### end ################################################################\n",
        "############################################################################################################################################\n",
        "\n",
        "sample_df_raw = seq_df.select(explode(get_sample_list('seqID','contract_title', 'seq', 'qas')).alias('sample'))\n",
        "\n",
        "\n",
        "# unzip dictionary row and map each key to columns - dataframe contains all samples\n",
        "sample_df_extracted = sample_df_raw.select(monotonically_increasing_id().alias('sampleID'), col('sample.seqID').alias('seqID'), \\\n",
        "                                           col(\"sample.qID\").alias(\"qID\"), col(\"sample.contract_title\").alias(\"contract_title\"), col(\"sample.negativity\").alias(\"negativity\"), \\\n",
        "                                           col(\"sample.source\").alias(\"source\"), col(\"sample.question\").alias(\"question\"), col(\"sample.answer_start\").alias(\"answer_start\"), \\\n",
        "                                           col(\"sample.answer_end\").alias(\"answer_end\"))\n",
        "\n",
        "\n",
        "# add contract ID\n",
        "sample_df = contract_df.join(sample_df_extracted, contract_df.contract_title == sample_df_extracted.contract_title)\n",
        "sample_df = sample_df.select('sampleID', 'qID', 'seqID', 'cID', 'negativity', 'source', 'question', 'answer_start', 'answer_end').cache()\n",
        "\n",
        "print('\\n', 'ðŸ’Ž all types of samples dataframe ðŸ’Ž', 'len: ', sample_df.count(), '\\n')\n",
        "sample_df.show(3)\n",
        "\n",
        "############################################################################################################################################\n",
        "############################################################# show intermeidate result #####################################################\n",
        "############################################################################################################################################\n",
        "\n",
        "# show  positive samples for all questions with possible answers in all contracts\n",
        "pos_tmp = sample_df.filter(f'negativity == {negativity_dict[\"positive\"]}')\n",
        "print('\\n', 'ðŸ’Ž positive samples dataframe ðŸ’Ž', 'len: ', pos_tmp.count(), '\\n')\n",
        "pos_tmp.show(3)\n",
        "\n",
        "# show possible negative samples\n",
        "possi_tmp = sample_df.filter(f'negativity == {negativity_dict[\"possible_negative\"]}')\n",
        "print('\\n', 'ðŸ’Ž possible negative samples dataframe ðŸ’Ž', 'len: ', possi_tmp.count(), '\\n')\n",
        "possi_tmp.show(3)\n",
        "\n",
        "# show impossible negative samples\n",
        "imp_tmp = sample_df.filter(f'negativity == {negativity_dict[\"impossible\"]}')\n",
        "print('\\n', 'ðŸ’Ž impossible negative samples dataframe ðŸ’Ž', 'len: ', imp_tmp.count(), '\\n')\n",
        "imp_tmp.show(3)\n",
        "############################################################################################################################################\n",
        "######################################################################### END ##############################################################\n",
        "############################################################################################################################################\n",
        "\n",
        "############################################################################################################################################\n",
        "############################################################### 2.1: START for count impossible ############################################\n",
        "############################################################################################################################################\n",
        "\n",
        "# get impossible\n",
        "q_imp_df = sample_df.select(col('cID').alias('cID_imp'), col('qID').alias('qID_imp')).\\\n",
        "    filter(f'negativity == {negativity_dict[\"impossible\"]}')\n",
        "\n",
        "# get possible\n",
        "q_pos_df = sample_df.select(col('cID').alias('cID_pos'), col('qID').alias('qID_pos'), col('sampleID').alias('sampleID_pos')).\\\n",
        "    filter(f'negativity == {negativity_dict[\"positive\"]}')\n",
        "\n",
        "\n",
        "# join condition: for every impossible questions in a contract, filter out those positive samples that is not in the same contract\n",
        "imp_pos_filtered_df = q_imp_df.join(q_pos_df, (q_imp_df.qID_imp==q_pos_df.qID_pos) & (q_imp_df.cID_imp != q_pos_df.cID_pos), how='left').\\\n",
        "    select(q_imp_df.cID_imp, 'qID_imp', 'sampleID_pos', 'cID_pos')\n",
        "\n",
        "\n",
        "# sampleID_pos - count distinct contracts for positive sample\n",
        "pos_contract_sum_df = imp_pos_filtered_df.select('cID_imp', 'qID_imp', 'sampleID_pos'). \\\n",
        "    groupBy('cID_imp', 'qID_imp').agg(countDistinct(\"*\").alias('count_contract'))\n",
        "\n",
        "\n",
        "# cID_pos - count distinct sample \n",
        "imp_count_df = imp_pos_filtered_df.select('cID_imp', 'qID_imp', 'cID_pos'). \\\n",
        "    groupBy('cID_imp', 'qID_imp').agg(countDistinct(\"*\").alias('count_pos'))\n",
        "imp_count_df = imp_count_df.select(col('cID_imp').alias('cID_imp2'), col('qID_imp').alias('qID_imp2'), 'count_pos')\n",
        "\n",
        "########################################################## AVERAGE positive samples count result #############################################################\n",
        "tmp_df = pos_contract_sum_df.join(imp_count_df, (pos_contract_sum_df.cID_imp == imp_count_df.cID_imp2) & (pos_contract_sum_df.qID_imp == imp_count_df.qID_imp2))\n",
        "avg_df = tmp_df.select('cID_imp', 'qID_imp', (tmp_df.count_pos / tmp_df.count_contract).alias('avg_pos_sample'))\n",
        "\n",
        "\n",
        "# count\n",
        "avg_count_df = avg_df.select('cID_imp', 'qID_imp',F.round(avg_df[\"avg_pos_sample\"],0).cast(IntegerType()).alias('count_imp'))\n",
        "\n",
        "print('\\n', 'ðŸ’Ž count number to balanced for impossible samples dataframe ðŸ’Ž', 'len: ', avg_count_df.count(), '\\n')\n",
        "avg_count_df.show(2)\n",
        "\n",
        "############################################################################################################################################\n",
        "############################################################## 2.2: START for count possible ###############################################\n",
        "############################################################################################################################################\n",
        "\n",
        "# data\n",
        "q_possi_df = sample_df.select(col('cID').alias('cID_possi'), col('qID').alias('qID_possi')).\\\n",
        "    filter(f'negativity == {negativity_dict[\"possible_negative\"]}')\n",
        "\n",
        "\n",
        "q_pos_df = sample_df.select(col('cID').alias('cID_pos'), col('qID').alias('qID_pos'), \n",
        "                            col('sampleID').alias('sampleID_pos')).filter(f'negativity == {negativity_dict[\"positive\"]}')\n",
        "\n",
        "\n",
        "# filter(): for every impossible questions in a contract, filter out those positive samples that is not in the same contract\n",
        "# join condition: same contract, same question\n",
        "possi_pos_filtered_df = q_possi_df.join(q_pos_df, (q_possi_df.qID_possi==q_pos_df.qID_pos) & (q_possi_df.cID_possi == q_pos_df.cID_pos),how='left' ). \\\n",
        "    select(q_possi_df.cID_possi, 'qID_possi', 'sampleID_pos')\n",
        "\n",
        "###################################### count result ###########################################\n",
        "# count distinct sample\n",
        "possi_count_df = possi_pos_filtered_df.groupBy('cID_possi', 'qID_possi').agg(countDistinct(\"*\").alias('count_pneg'))\n",
        "possi_count_df = possi_count_df.select(col('cID_possi').alias('cID_possi'), col('qID_possi').alias('qID_possi'), 'count_pneg')\n",
        "\n",
        "print('\\n', 'ðŸ’Ž count number to balanced for possible negative samples dataframe ðŸ’Ž', 'len: ', possi_count_df.count(), '\\n')\n",
        "possi_count_df.show(2)\n",
        "\n",
        "############################################################################################################################################\n",
        "############################################################## STEP 2: select based on count ###############################################\n",
        "############################################################################################################################################\n",
        "\n",
        "################################# Prepare possible ################################# \n",
        "# | cID | qID | [possible_negative ids] \n",
        "possible_samples = sample_df.filter(f'negativity == {negativity_dict[\"possible_negative\"]}'). \\\n",
        "    select('cID', 'qID', 'seqID', 'sampleID')\n",
        "\n",
        "\n",
        "possible_samples_df = possible_samples.groupBy('cID', 'qID').agg(collect_list( \n",
        "    create_map(lit(\"seqID\"),\"seqID\",lit(\"sampleID\"),\"sampleID\")).alias(\"possi\"))\n",
        "\n",
        "\n",
        "################################# Prepare impossible ################################# \n",
        "\n",
        "# | cID | qID | [impossible ids] \n",
        "impossible_samples = sample_df.filter(f'negativity == {negativity_dict[\"impossible\"]}'). \\\n",
        "    select('cID', 'qID', 'seqID', 'sampleID')\n",
        "\n",
        "\n",
        "impossible_samples_df = impossible_samples.groupBy('cID', 'qID').agg(collect_list( \n",
        "    create_map(lit(\"seqID\"),\"seqID\",lit(\"sampleID\"),\"sampleID\")).alias(\"imp\"))\n",
        "\n",
        "\n",
        "################################# Prepare positive ################################# \n",
        "# | cID | qID | [pos ids] \n",
        "positive_samples = sample_df.filter(f'negativity == {negativity_dict[\"positive\"]}'). \\\n",
        "    select('cID', 'qID', 'seqID')\n",
        "\n",
        "\n",
        "positive_samples_df = positive_samples.groupBy('cID', 'qID').agg(collect_list('seqID').alias(\"pos\"))\n",
        "\n",
        "#### join with count\n",
        "imp_df_1 = impossible_samples_df.join(avg_count_df, \n",
        "                        (impossible_samples_df.cID==avg_count_df.cID_imp) & (impossible_samples_df.qID==avg_count_df.qID_imp)). \\\n",
        "                        select('cID', 'qID', 'count_imp', 'imp')\n",
        "\n",
        "\n",
        "#### join with positive\n",
        "# same col name can ues []\n",
        "imp_df = imp_df_1.join(positive_samples_df, ['cID', 'qID'], how='left')\n",
        "\n",
        "print('\\n', 'ðŸ’Ž input dataframe to the UDF of balancing the impossible samples ðŸ’Ž', 'len: ', imp_df.count(), '\\n')\n",
        "imp_df.show(2)\n",
        "\n",
        "imp_schema = StructType([\n",
        "        StructField('imp_keep', ArrayType(IntegerType())),\n",
        "        StructField('seqID', ArrayType(IntegerType())) # for compare so that the possible negative sample to not be the same\n",
        "])\n",
        "############################################################################################################################################\n",
        "################################################# based on the count, balance impossible first  ############################################\n",
        "############################################################################################################################################\n",
        "\n",
        "@udf(returnType=imp_schema)\n",
        "def balance_imp(count_imp, imp, pos_seqIDs):\n",
        "    \"\"\"\n",
        "    balance the impossible samples and make unique based on the positive samples\n",
        "    \"\"\"\n",
        "    res = {\n",
        "        'imp_keep': [],\n",
        "        'seqID': []\n",
        "    }\n",
        "    # not such count so no negative sample\n",
        "    if not count_imp or count_imp == 0:\n",
        "        return res\n",
        "\n",
        "    # no such positive sample\n",
        "    if not pos_seqIDs:\n",
        "        pos_seqIDs = []\n",
        "\n",
        "    remain = count_imp  \n",
        "    for cur in imp:\n",
        "        if remain == 0:\n",
        "            break\n",
        "        # not in the positive sampels\n",
        "        if cur['seqID'] in pos_seqIDs:\n",
        "            continue\n",
        "        # unique with itself\n",
        "        if not cur['seqID'] in res['seqID']:\n",
        "            res['imp_keep'].append(cur['sampleID'])\n",
        "            res['seqID'].append(cur['seqID'])\n",
        "\n",
        "        remain -= 1\n",
        "\n",
        "    # if the unique ones is not enough, then append non unique ones\n",
        "    if remain > 0:\n",
        "\n",
        "        sampleIDs = [s['sampleID'] for s in imp]\n",
        "        seqIDs = [s['seqID'] for s in imp]\n",
        "\n",
        "        imp_remain = [id for id in sampleIDs if id not in res['imp_keep']]\n",
        "        res['imp_keep'] += imp_remain[:remain]\n",
        "\n",
        "        seq_remain = [id for id in seqIDs if id not in res['seqID']]\n",
        "        res['seqID'] += seq_remain[:remain]\n",
        "\n",
        "    return res\n",
        "################################################################################################################################\n",
        "\n",
        "# get what impossible sample id to keep for each question in a contract\n",
        "imp_keep_df = imp_df.select('cID', 'qID',balance_imp('count_imp', 'imp', 'pos').alias('res')).\\\n",
        "    select('cID','qID',col('res.imp_keep').alias('imp_id_keep'), col('res.seqID').alias('seqID_imp'))\n",
        "\n",
        "print('\\n', 'ðŸ’Ž output dataframe to the UDF of balancing the impossible samples ðŸ’Ž', 'len: ', imp_keep_df.count(), '\\n')\n",
        "imp_keep_df.show(2)\n",
        "\n",
        "# unflatten list to rows\n",
        "imp_keep_res = imp_keep_df.select(explode(imp_keep_df['imp_id_keep']).alias('imp_id_keep'))\n",
        "\n",
        "\n",
        "possi_df_1 = possible_samples_df.join(possi_count_df, \n",
        "                        (possible_samples_df.cID==possi_count_df.cID_possi) & (possible_samples_df.qID==possi_count_df.qID_possi)). \\\n",
        "                        select('cID', 'qID', 'count_pneg', 'possi')\n",
        "\n",
        "\n",
        "# same col name can ues []\n",
        "possi_df = possi_df_1.join(positive_samples_df, ['cID', 'qID'], how='left')\n",
        "\n",
        "\n",
        "possible_samples_tmp = possi_df.join(imp_keep_df, ['cID', 'qID'], 'left').drop('imp_id_keep')\n",
        "\n",
        "print('\\n', 'ðŸ’Ž input dataframe to the UDF of balancing the possible negative samples ðŸ’Ž', 'len: ', possible_samples_tmp.count(), '\\n')\n",
        "possible_samples_tmp.show(2)\n",
        "\n",
        "############################################################################################################################################\n",
        "################################################# based on the count, balance possible samples second  #####################################\n",
        "############################################################################################################################################\n",
        "possi_schema = ArrayType(IntegerType())\n",
        "\n",
        "@udf(returnType=possi_schema)\n",
        "def balance_possi(count_pneg, possi, seqID_pos, seqID_imp):\n",
        "    \"\"\"balance the possible negative samples and make unique based on the positive and imposible samples\"\"\"\n",
        "    res = []\n",
        "    seq_list = []\n",
        "    # not such count so no negative sample\n",
        "    if not count_pneg or count_pneg == 0:\n",
        "        return res\n",
        "\n",
        "    # no such positive sample\n",
        "    if not seqID_pos:\n",
        "        seqID_pos = []\n",
        "    # no such impossible sample\n",
        "    if not seqID_imp:\n",
        "        seqID_imp = []\n",
        "\n",
        "    remain = count_pneg  \n",
        "    for cur in possi:\n",
        "        if remain == 0:\n",
        "            break\n",
        "        # not in the positive sampels & impossible sample\n",
        "        if cur['seqID'] in seqID_pos or cur['seqID'] in seqID_imp:\n",
        "            continue\n",
        "        # unique with itself\n",
        "        if not cur['seqID'] in seq_list:\n",
        "            res.append(cur['sampleID'])\n",
        "            seq_list.append(cur['seqID'])\n",
        "\n",
        "        remain -= 1\n",
        "\n",
        "    # if the unique ones is not enough, then append non unique ones\n",
        "    if remain > 0:\n",
        "\n",
        "        sampleIDs = [s['sampleID'] for s in possi]\n",
        "\n",
        "        possi_remain = [id for id in sampleIDs if id not in res]\n",
        "        res += possi_remain[:remain]\n",
        "\n",
        "    return res\n",
        "\n",
        "############################################################################################################################################\n",
        "################################################################### Union ##################################################################\n",
        "############################################################################################################################################\n",
        "\n",
        "possi_keep_df = possible_samples_tmp.select(balance_possi('count_pneg','possi', 'pos','seqID_imp').alias('res'))\n",
        "\n",
        "print('\\n', 'ðŸ’Ž output dataframe to the UDF of balancing the possible negative samples ðŸ’Ž', 'len: ', possi_keep_df.count(), '\\n')\n",
        "possi_keep_df.show(2)\n",
        "\n",
        "# unflatten list to rows\n",
        "possi_keep_res = possi_keep_df.select(explode(possi_keep_df['res']).alias('possi_id_keep'))\n",
        "\n",
        "\n",
        "\n",
        "################## union all the selected positive, possible and impossible sampels as the final result #################################\n",
        "\n",
        "\n",
        "possib_selected = sample_df.filter(f'negativity == {negativity_dict[\"possible_negative\"]}').\\\n",
        "    select('sampleID', 'source','question','answer_start','answer_end')\n",
        "\n",
        "\n",
        "possib_selected = possi_keep_res.join(possib_selected, possi_keep_res.possi_id_keep==possib_selected.sampleID).drop('possi_id_keep')\n",
        "\n",
        "\n",
        "imp_selected = sample_df.filter(f'negativity == {negativity_dict[\"impossible\"]}').\\\n",
        "    select('sampleID', 'source','question','answer_start','answer_end')\n",
        "imp_selected = imp_keep_res.join(imp_selected, imp_keep_res.imp_id_keep==imp_selected.sampleID).drop('imp_id_keep')\n",
        "\n",
        "\n",
        "pos_selected = sample_df.filter(f'negativity == {negativity_dict[\"positive\"]}').\\\n",
        "    select('sampleID', 'source','question','answer_start','answer_end')\n",
        "\n",
        "\n",
        "sample_selected = pos_selected.union(imp_selected).union(possib_selected).drop('sampleID')\n",
        "\n",
        "print('\\n', 'ðŸ’Ž All selected samples for output ðŸ’Ž', 'len: ', sample_selected.count(), '\\n')\n",
        "sample_selected.show(2)\n",
        "\n",
        "############################################### union ###############################################################\n",
        "\n",
        "print('\\n', 'ðŸ’Ž All selected samples with full details ðŸ’Ž', '\\n')\n",
        "\n",
        "pneg_tmp = possi_count_df.select(col('cID_possi').alias('cID'), col('qID_possi').alias('qID'), 'count_pneg')\n",
        "imp_tmp = avg_count_df.select(col('cID_imp').alias('cID'), col('qID_imp').alias('qID'), 'count_imp')\n",
        "pos_selected.union(imp_selected).union(possib_selected).select('sampleID').\\\n",
        "    join(sample_df, ['sampleID']).\\\n",
        "    select('sampleID', 'cID', 'qID', 'seqID', 'negativity','source','question', 'answer_start', 'answer_end').\\\n",
        "    join(pneg_tmp, ['cID', 'qID'], 'left').join(imp_tmp, ['cID', 'qID'], 'left').\\\n",
        "    show(3)\n",
        "\n",
        "############################################################################################################################################\n",
        "####################################### Output final selected result to json format with given filename ####################################\n",
        "############################################################################################################################################\n",
        "\n",
        "from pyspark.sql.functions import collect_list, create_map, lit\n",
        "def datafame_to_json(sample_to_json, path_):\n",
        "    \"\"\"dataframe to list of dictionaries\n",
        "    https://stackoverflow.com/questions/61278038/how-to-convert-pyspark-dataframe-to-json\n",
        "    \"\"\"\n",
        "    cols_extracted = create_map(lit(\"source\"),\"source\",lit(\"question\"),\"question\", lit(\"answer_start\"),\"answer_start\", lit(\"answer_end\"),\"answer_end\")\n",
        "\n",
        "    import json\n",
        "    res = sample_to_json.agg(collect_list(cols_extracted).alias(\"stru\")).first()['stru']\n",
        "    with open(path_, \"w\") as f:\n",
        "        f.write(json.dumps(res, indent=4, sort_keys=True))\n",
        "\n",
        "datafame_to_json(sample_selected, output_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "a2-2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
