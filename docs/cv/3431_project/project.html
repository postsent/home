
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Mini self-driving - turtlebot &#8212; My Jupter Notebook on data science.</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Machine Learning (UNSW)" href="../../ds-courses/unsw/9417/9417-intro.html" />
    <link rel="prev" title="Placeholder" href="../cv.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">My Jupter Notebook on data science.</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://postsent.github.io/">
   Back to Blog
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reference.html">
   References
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../ml/regression/regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml/p1-crypto-prediction.html">
     Crypto Prediction
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ml/classification/classification.html">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ml/time-series/time-series.html">
   Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ml/ml.html">
   General
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../dl/dl.html">
   General
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  NLP
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/nlp.html">
   Reading
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../nlp/nlp-pytorch-bk/bk.html">
   NLP with Pytorch book
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../nlp/nlp-pytorch-bk/intro.html">
     Intro
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../nlp/nlp-pytorch-bk/nn.html">
     Feed-Forward Networks for NLP
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Computer Vision
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../cv.html">
   Placeholder
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Mini self-driving - turtlebot
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Science Courses
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../ds-courses/unsw/9417/9417-intro.html">
   Machine Learning (UNSW)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ds-courses/unsw/9417/9417-basic.html">
     Basic with examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ds-courses/unsw/9417/9417-project.html">
     Project - Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../ds-courses/unsw/9418/9418-intro.html">
   PGM (UNSW)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ds-courses/unsw/9418/9418-EDA.html">
     EDA on Times Series Dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ds-courses/unsw/9418/9418-project.html">
     Time series project code
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ds-courses/unsw/9418/9418-project/report.html">
     Time series report
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../ds-courses/unsw/9517/9517-intro.html">
   Computer Vision (UNSW)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ds-courses/unsw/9517/9517-a1-code.html">
     Basic image processing, thresholding, count cells
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ds-courses/unsw/9517/9517-a1/report.html">
     Report on Basic image processing, etc
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ds-courses/unsw/9517/9517-lane_detection.html">
     Lane detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ds-courses/unsw/9517/9517-vehicle-detection.html">
     Vehicle detection
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../ds-courses/unsw/9444/9444-intro.html">
   Deep Learning (UNSW)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ds-courses/unsw/9444/9444-project.html">
     Image classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ds-courses/unsw/9444/assignment1/assignment1.html">
     Characters, Spirals and Hidden Unit Dynamics (Assignment 1)
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Misc
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../misc/math.html">
   Math
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Coding Basic
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../python/basic-intro.html">
   Numpy, Pandas, Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../python/numpy.html">
     Numpy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../python/pandas.html">
     Pandas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../python/leetcode.html">
     Leetcode
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/cv/3431_project/project.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/postsent/nb"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Mini self-driving - turtlebot
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#abstract">
   Abstract
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   1. Introduction
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background-and-related-research">
   2. Background and Related Research
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lane-following">
     2.1 Lane following
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#camera">
     2.2 Camera
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#laser-scan">
     2.3 Laser Scan
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#localisation">
     2.4 Localisation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#breadth-first-search">
     2.5 Breadth-First Search
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#theory">
   3. Theory
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#object-identification">
     3.1 Object identification.
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#path-planning">
     3.2 Path Planning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#map">
     3.3 Map
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation">
   4. Implementation
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation">
   5. Evaluation
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion-and-future-work">
   6. Conclusion and Future Work
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliography">
   7. Bibliography
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendices">
   8. Appendices
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Mini self-driving - turtlebot</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Mini self-driving - turtlebot
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#abstract">
   Abstract
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   1. Introduction
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background-and-related-research">
   2. Background and Related Research
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lane-following">
     2.1 Lane following
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#camera">
     2.2 Camera
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#laser-scan">
     2.3 Laser Scan
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#localisation">
     2.4 Localisation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#breadth-first-search">
     2.5 Breadth-First Search
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#theory">
   3. Theory
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#object-identification">
     3.1 Object identification.
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#path-planning">
     3.2 Path Planning
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#map">
     3.3 Map
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation">
   4. Implementation
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation">
   5. Evaluation
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion-and-future-work">
   6. Conclusion and Future Work
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliography">
   7. Bibliography
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendices">
   8. Appendices
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="mini-self-driving-turtlebot">
<h1>Mini self-driving - turtlebot<a class="headerlink" href="#mini-self-driving-turtlebot" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://postsent.github.io/2021/12/11/courses/#comp3431-robotic-software-architecture">Demo here</a></p>
<p><strong>COMP3431</strong></p>
<p><strong>Project Report</strong></p>
<p><strong>Autonomous driving using TurtleBot in a simulated road environment</strong></p>
<p><code class="docutils literal notranslate">&#160;&#160;&#160;&#160; </code><strong>November 2020</strong></p>
<p>Team name: Boston Static</p>
<p>School of Computer Science and Engineering, The University of New South Wales, Sydney NSW 2052, Australia</p>
<p>This is an individual project report for the assessment in COMP3431 by Jerry</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="abstract">
<h1>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h1>
<p>This project examines the application of autonomous driving on TurtleBot (Waffle Pi). The goal is to achieve lane following of a road map, path planning and object detection such as intersection, robots, pedestrian, traffic sign and traffic light. The methods used include Hough transformation in lane detection and breadth-first search in path planning. Besides, blob detection, morphological transformations and other image processing techniques from OpenCV are implemented for object identification.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="introduction">
<h1>1. Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<p>The TurtleBot is used in the ROS Autorace competition to achieve autonomous driving under various road condition without any external interference.</p>
<p>This project primarily focuses on five tasks described as following:</p>
<ul class="simple">
<li><p>Stop when a stop sign detected at the intersection.</p></li>
<li><p>Obey traffic light which is demonstrated by coloured poles</p></li>
<li><p>Slow down or stop when other TurtleBot detected</p></li>
<li><p>Determine whether to give way to other TurtleBot at the intersection</p></li>
<li><p>Create a map and make a path planning given a destination</p></li>
</ul>
<p>The version of TurtleBot used is Waffle Pi which includes a Raspberry Pi 3, camera, and other sensor devices such as LiDAR, and the development platform involved are ROS Melodic and Ubuntu 18.04.</p>
<p>Based on the demonstration in the lab, the implementation reasonably achieves all five above tasks. However, there are some issues observed during the demonstration. Firstly, the inconsistent network connection leads to a delay in the communication between the remote machine and the TurtleBot. Thus, the processing speed of the camera image and the connection to the robot from Rviz<a class="footnote-reference brackets" href="#id3" id="id1">1</a> are slow. Therefore, the maximum linear velocity of the robot is set to 6 cm per second and angular speed is reduced as well. Besides, the localisation using AMCL<a class="footnote-reference brackets" href="#id4" id="id2">2</a> is observed to be inaccurate after a period. As a result of multiple nodes subscribing to the camera topic, the queue size seems to be filled up at the end of the demonstration, which causes a loss of control on the robot. Moreover, some edge cases such as the stop sign or lane are hidden by other TurtleBot and U-turn at the corner are not demonstrated.</p>
<p>My contribution to the project focuses on processing images using different morphological transformation and thresholds such as HSV, contours and area to identify the coloured poles, stop sign, TurtleBot and intersection. Related function APIs are also created to the corresponding movement code. Besides, a route planning with BFS, a map with the road shown by shifting the path recorded, a node uses laser scan ranges and intensities to identify obstacles (not used), a node makes U-turn and code that finds turning points of the path are implemented. Some minor suggestion is made on the lane following such as reducing the velocity of the robot to compensate the delay in receiving camera image.</p>
<p>The method used in this project includes blob detection, finding contours and applying HSV masks to image for identifying different colours and shapes. Breath first search used in weightless directed path graph for route planning, and Ramer-Douglas-Peucker algorithm (RDP) for getting the turning points of the path graph.</p>
<p>The following section introduces the details of the theory and implementation on path planning, object identification and lane following.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="background-and-related-research">
<h1>2. Background and Related Research<a class="headerlink" href="#background-and-related-research" title="Permalink to this headline">¶</a></h1>
<section id="lane-following">
<h2>2.1 Lane following<a class="headerlink" href="#lane-following" title="Permalink to this headline">¶</a></h2>
<p>It is found that the wall around the map creates noise in data since the edges of the wall are also included by Canny from OpenCV (See Appendix C). My attempt to solve it is to set up HSV threshold to differentiate between the wall and the lane. However, issues occur as the left side of the map is darker than the right side due to the position of the light, which results in lane undetected. Contrast Limited Adaptive Histogram Equalization (CLAHE) combined with calculating the contour area is used. Then, the wall is identified by its area which is the larger than lane. Histogram equalisation adjusts the image intensities with high contrast (Gonzalez &amp; Woods, 2008). Contrast Limited AHE is a variant of adaptive histogram equalization in which the contrast amplification is limited and noise amplification is reduced. (Pizer, et al., 1987)</p>
</section>
<section id="camera">
<h2>2.2 Camera<a class="headerlink" href="#camera" title="Permalink to this headline">¶</a></h2>
<p>As for detecting if the camera is blocked by an object such as a hand, the background subtraction technique is found suitable for generating a foreground mask (Srivastava, 2020). Then the differences between the older frames and recent frames are compared to conclude the result. However, it is realised that if the turtlebot experience a glitch and suddenly rotate with large angle, then it will be treated as if the camera is blocked. Thus, this approach is not used.</p>
</section>
<section id="laser-scan">
<h2>2.3 Laser Scan<a class="headerlink" href="#laser-scan" title="Permalink to this headline">¶</a></h2>
<p>Laser scan is first thought to be used in detecting TurtleBot. It is because the area of black pixels in the camera image between a Turtlebot driving towards or away is almost identical. Moreover, the camera image can hardly determine the distance.</p>
<p>The LIDAR starts anti-clockwise from the wheel side of the robot with an interval of 1 degree. (Robotis, 2020)</p>
<p>It is found that the LiDAR is configured with a minimum distance of 0.1199 metres and a maximum of 3.5 metres (Castillo &amp; Zhang, 2018), which both appear as 0.0 metres if it goes outside the range and it is hard to differentiate whether it is below or above the range. Another issue is that the TurtleBot may treat the wall as another robot when it gets close to the wall during turning.</p>
<p>Intensities in laser scan are not particularly helpful to detect the wall as if another obstacle that has the same intensities appears in front of the TurtleBot, it will be treated as a wall.</p>
<p>Eventually, the masked camera image is used to hide the Turtlebot on the opposite road. Laser scan is planned to be used in the T intersection to detect the robot where the camera cannot capture. It is effective because a wall is placed at T intersection due to the glass reflection, which eliminates the case where detection occurs over 3.5m and so only close detection occurs.</p>
</section>
<section id="localisation">
<h2>2.4 Localisation<a class="headerlink" href="#localisation" title="Permalink to this headline">¶</a></h2>
<p>Simultaneous Localisation and Mapping (SLAM) is used for mapping (from ROS Gmapping package). It is the process by which a mobile robot can build a map of an environment and at the same time use this map to compute its location** (Durrant-Whyte &amp; Bailey, 2006)<strong>.</strong> Gmapping is favoured over other ROS mapping packages due to its simplicity.</p>
<p>Adaptive Monte Carlo Localization (AMCL) is utilised to localise the robot in a given map. It is a probabilistic localization system for a robot moving in 2D. This system uses a particle filter to track the pose of a robot against a known map (Gerkey, 2020).</p>
</section>
<section id="breadth-first-search">
<h2>2.5 Breadth-First Search<a class="headerlink" href="#breadth-first-search" title="Permalink to this headline">¶</a></h2>
<p>Breadth-First Search algorithm traverses a graph in a centre expanding way and uses a queue to remember to get the next vertex to start a search, where the queue is a data structure follows first in fist out rule. The search starts with a single source vertex, then, in phases, finds and labels its neighbours, then the neighbours of its neighbours and it repeats the same step until the destination node is found (Bader, et al., 2011). Depth-first search is not implemented as the number of nodes in the graph is relatively large and A* search is not considered since the lane position is hard to calculate, which then cannot be transformed to the grid A* search relies on.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="theory">
<h1>3. Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h1>
<section id="object-identification">
<h2>3.1 Object identification.<a class="headerlink" href="#object-identification" title="Permalink to this headline">¶</a></h2>
<p><img alt="A screenshot of a video game Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.001.png" /></p>
<p><img alt="" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.002.png" /><img alt="" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.003.png" />  <img alt="" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.004.png" /></p>
<p><em>Figure 1 - poles and stop sign detection</em></p>
<p>Yellow, green, blue colour on the poles are detected using HSV, which is decided with high precision using the trackbar function from OpenCV (see Figure 1). The trackbar function allows manually drag the trackbar for setting the lowest and highest range of HSV to give the best possible mask for the binary image. Thresholds such as area, number of contours and the slope of the line are set to avoid noisy data such as blue shirt, dashed line and light reflection from the ground, which conflicts with the detection on coloured poles and intersection. Stop sign is detected by using blob detection on the red colour, where TurtleBot is on black.</p>
<p>Text detection for stop sign (not used) involves downsampling the image, filtering it with morphological gradient, binarizing, and then connecting horizontally oriented regions with closing. After that, contours with a 45 % ratio of non-zero pixels in the filled region and a ratio of 1.1 between width and height are chosen to minimise the noisy data. For example, the red door behind the stop sign and stop sign locating on the side of the road can confuse the robot. (see Figure 1). Besides, the T intersection is detected by finding a rectangle line in the middle area of the camera frame with other area being cropped and masked (see Appendix A).</p>
<p>Facial recognition for the pedestrian is achieved via Haar cascades machine learning approach where pre-trained classifier for different parts of the face is obtained from online (Menon, 2019).</p>
<p><img alt="" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.005.png" /></p>
</section>
<section id="path-planning">
<h2>3.2 Path Planning<a class="headerlink" href="#path-planning" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.006.png" /></p>
<p><em>Figure 2 - pseudo-code for determining U-turn and if in the right lane, second image contains code decides which turning direction at the intersection</em></p>
<p><em>Figure 3 – original points recorded, path with turning point, connect intersection</em></p>
<p><img alt="A picture containing line chart Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.007.png" /><img alt="" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.008.png" /><img alt="Chart Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.009.png" /><img alt="Chart, line chart Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.010.png" /></p>
<p>Breadth-First Search is used for making a route given a destination and there are two main steps in path planning.</p>
<p>Firstly, depends on the shortest path found from the robot position to the destination (global map frame), TurtleBot needs to decide whether to make a U-turn or continue moving forward.</p>
<p>Secondly, the existence of intersection in the shortest path is checked which determines the direction of turning at the intersection.</p>
<p>For the first step, Breadth-First Search is used to find the shortest path from the TurtleBot to the destination. To ensure the TurtleBot follows the lane during the path following, points from Odom topic are recorded into a file and the records stop when all left lane is visited exactly once (no backward trip on the same road). In this way, the graph built based on the points (using the python dictionary) contains less noise and needs less manipulation. For example, if the right lane is also recorded, then extra work is needed to connect the graph between the left and right lane.</p>
<p>After the points are recorded, it is first to be read as a python list and then translated into a directed graph based on the recording order, where the middle point connect to its previous and next point. The intersection points (three in total for T intersection) are manually connected in the graph (see the black points in Figure 3). Any point within the connection between these three intersection points is deleted, which ensures that the intersection points are connected without noise.</p>
<p>Before determining if Turtlebot needs to make a U-turn or forward, it is checked whether it is on the left lane. To achieve this, points are recorded every 5cm and it is known that the distance between the centre of the left and right lane is more than 20cm. So, it is guaranteed that when turtlebot is on the same lane (left lane) where points are recorded, the distance between its current position and its closest point in the path is always less than 5cm, otherwise, the robot is on the opposite road (right lane).</p>
<p>Then, the order of the first and second point in the path appears in the initial points recording is compared. For example, if the first comes earlier than the second, then the path is in front of the TurtleBot. Otherwise, behind, An edge case is when the first point is the middle intersection point, then it is checked if the next point is also an intersection point, if so, then the path is in front, otherwise, behind.</p>
<p>Now, given direction of the path and the position on the lane is known, it can then be calculated that, for example, if a path is in front and not on opposite road, then the TurtleBot should move forward.</p>
<p>There is an unused idea of a simple transformation between the point recorded and the actual starting point is used. It first calculates the offset between the actual first point from Odom topic and the first one recorded. Then, this offset is added to all points recorded for path calculation purpose. When path publishes to the Rviz, the offset is then removed to match the recorded map frame.</p>
<p>For the second step, as mentioned above, all three intersection points are connected without any other points in between. Then the turning direction at the intersection is determined by comparing the intersection point that comes early in the path and the one that is the next. For instance, if the left intersection point comes before the middle intersection point, then the robot turns right.</p>
<p>Once the turtlebot decides its direction, these instructions are then published to the corresponding topic relative to the turning movement code.</p>
<p>As seen the Figure 3, RDP helps find different segments of line and thus turning point (Marquez &amp; Wu, 2003), which originally is planned to be used for changing the motion of robot at the corner.</p>
</section>
<section id="map">
<h2>3.3 Map<a class="headerlink" href="#map" title="Permalink to this headline">¶</a></h2>
<p><em>Figure 4 - creating road on the map</em></p>
<p><img alt="Diagram, engineering drawing Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.011.png" /><img alt="" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.012.png" /><img alt="Diagram, engineering drawing Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.013.png" /><img alt="A picture containing chart Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.014.jpeg" /></p>
<p>As seen in Figure 4, the white line in image 1 represents the path TurtleBot took and the black lines are the left and right lane. The two black lines are created by shifting the white line to left around 10 cm and to right 60 cm. The white dashed line shown in the second image shifts the path recorded to right by 20 cm and is published with different marker ID, a gap of 4 points and each dashed line corresponds to a length of 8 points (around 40 cm).</p>
<p>The method used is illustrated in the third diagram. The horizontal points are the path recorded and the point on the top and bottom are the left and right lane point relative to the path line.</p>
<p>Firstly, the recorded points list is looped through, with each time two points read which creates a linear line formula for the dashed line seen from the image. Then, a perpendicular line passed through the first of the two points is calculated. After that, the up and down point is found on the perpendicular line based on distance x, y as seen in the diagram.</p>
<p>This, however, creates an issue when the axis changes as seen from the first image in Figure 4, where there are points flip upside down when x and y-axis altered. It is solved by using a cross product formula (equivalently the determinant of a 2D matrix) to ensure the top point is always on the top of the path recorded. In details, the formula is</p>
<p>(Bx - Ax) * (Cy - Ay) - (By - Ay) * (Cx - Ax)</p>
<p>Where A, B are two points on the line and C is the point to check against with. If the determinant is zero, then the point is on the line, if positive or negative then on one side or the other of the line depends on the axis.</p>
<p><img alt="Diagram, engineering drawing Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.015.png" /></p>
<p><img alt="Diagram, engineering drawing Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.016.png" /><img alt="Diagram, engineering drawing Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.017.png" /><img alt="Diagram, engineering drawing Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.018.png" /></p>
<p><em>Figure 5 - path transformation</em></p>
<p>Now, as seen from the first image in Figure 5, since only the left lane is recorded as the path when the path desired is on the right lane, a shift is required. Interestingly, as the direction of the line is opposite to the path recorded, the line needs to be shifted to the left instead of right.</p>
<p>It is found that there are two conditions where the lane needs to be transformed. Firstly, when the robot is not making a U-turn and is on the right lane as seen in image 3. Another condition is when the robot makes U-turn and is not on the right lane as seen in image 2.</p>
<p><img alt="Diagram, engineering drawing Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.019.png" /> <img alt="Diagram, engineering drawing Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.020.png" />   <img alt="Diagram, engineering drawing Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.021.png" />  <img alt="Diagram, engineering drawing Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.022.png" /></p>
<p><em>Figure 6 - path transformation at intersection</em></p>
<p>Apart from the transformation issue mentioned above, another one is observed as seen in Figure 6. Now, two sections of lines are in the opposite lane, one before the intersection and one after. Thus, one of them needs to be transformed accordingly. This happens with the same reason as before, where the order of which the path is recorded consider only left lane, once the right lane is visited, then that section of the path needs to be transformed.</p>
<p>After this transformation, it is believed that the path planning is completed.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="implementation">
<h1>4. Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h1>
<p>The following section introduces all the major steps involved in the system. (ROS nodes used illustrated in Appendix B)</p>
<p>Firstly, a map is created with Gmapping and is saved with the map server. Points subscribed from Odometry are stored into a file in the meantime.</p>
<p>Secondly, the map server is run to send the map to Rviz and AMCL is used to localise the TurtleBot based on the map saved.</p>
<p>Thirdly, path planning starts by receiving a goal sent from the clicked_point topic from Rviz. It creates a shortest path from the current position to the destination using BFS. Then it decides whether to make a U-turn based on if the path is behind the turtlebot. After that, it checks if intersection exists in the path, if so, it sends a direction to the corresponding turning movement code.</p>
<p>Then, detect_lane node is run for lane detection. Hough transformation is used to join the dashed line on the road and convert into a solid straight line. Compressed Image (space-efficient than normal image) is then published for visualisation on the performance of lane detection. Before identifying the image from the camera, the white wall is filtered out based on the contour area and CLAHE. Then the image is a blur in grayscale and transformed into a top-down looking perspective with the masked image to reduce noise in the data.</p>
<p>After that, the remaining image is further filtered based on the perimeter and the area of the line identified, where the threshold is set between the size of the dash and solid lane.</p>
<p>Now, the fraction and the image of the individual solid lane and the dotted line is obtained.</p>
<p>Then, by comparing the threshold set for the line fraction and the actual value, the direction of turning at the corner is determined. To find the centre between two lanes, the fitted line for the left lane and right lane is calculated.</p>
<p>Then control_lane node is run. Based on the centre line data received, the linear and angular velocity is calculated and published, which allows TurtleBot to follow the lane properly.</p>
<p>One interesting point is that the velocity of the robot is changed to two-third of its original because of the low network connection when multiple TurtleBots are activated.</p>
<p>Now, nodes for detecting intersection, stop sign, poles, Turtlebot and pedestrian are run.</p>
<p>As there is no lane to follow at the intersection, movebase node is used for sending a goal for the robot to follow. Hardcoded movement is also used.</p>
<p>Once the robot detects the intersection, which is identified based on the line at the bottom of the camera image with an angle tolerance of 15 degrees (see Appendix A), it then checks if there is a stop sign exists if so, it stops until stop sign is removed. Then it checks if a coloured pole exists, where blue indicates moving forward, yellow is turn left and green is right. A downside is that the speed and distance of turning at the intersection are hardcoded for the lab environment.</p>
<p>In summary, all five required tasks are achieved. A launch file, which contains nodes such as bringup, camera and those relative to object detection, is made to ensure a straightforward execution. Map server and Rviz is launched separately on a different terminal to avoid overload on one machine. Nodes that use camera image are launched on the robot for a faster receiving rate.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="evaluation">
<h1>5. Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h1>
<p>During the demonstration, the above five tasks described achieved reasonably well.</p>
<p>The following section describes the cases where the implementation of TurtleBot could fail.</p>
<p>Turtlebot detection is not implemented during the U-turn, which means that it will turn regardless if another TurtleBot is in its way. Besides, if the TurtleBot is at the turning corner, then U-turn may not be accurate as it is hardcoded based on the width of the road.</p>
<p>In the lab environment, the colour of the stop sign is the same as the door behind which are both red. Thus, the door is closed during the demonstration. Detection on TurtleBot could fail at the corner as the camera cannot catch the frame includes other TurtleBot. TurtleBot may also hit another robot if it is not inside the camera frame. If an object such as shoes has a similar colour and size as the poles, then it may be falsely identified as a pole.</p>
<p>It is found that the reflection of the light from outside the lab on the ground is falsely detected as an intersection line. It is solved by covering the window with a curtain. The low light environment at some corner causes the robot to slow down and calibrate its position.</p>
<p>Facial recognition process on the image with full size. This means that if people are standing outside the road, then their face could be accidentally detected.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="conclusion-and-future-work">
<h1>6. Conclusion and Future Work<a class="headerlink" href="#conclusion-and-future-work" title="Permalink to this headline">¶</a></h1>
<p>In conclusion, the report addresses four main aspects of self-driving vehicle which includes lane following, object detection such as the robot, pedestrian, traffic light, stop sign and intersection, mapping as well as the path planning. It also describes the essential algorithms for solving the four aspects and has summarised the key demonstrations and implementations of the approach.</p>
<p>A possible improvement to the project includes letting the robot automatically calibrated its angular and linear velocity during lane following based on the road environment.</p>
<p>The path planning could be enhanced by changing the current graph to a weighted graph where weight cost such as traffic congestion and traffic light can be considered. Extra features such as auto parking and sounding the horn when in emergency condition could be added to build up a robust and reliable autonomous driving system.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="bibliography">
<h1>7. Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h1>
<p>Bader, D. A., Berry, J., Kahan, S., Murphy, R., Riedy, E. J., &amp; Willcock, J. (2011, September). <em>Graph 500 Benchmark 1 (“Search”)</em>. Retrieved from Graph 500: <a class="reference external" href="https://web.archive.org/web/20150326055019/http://www.graph500.org/specifications#sec-5#sec-5">https://web.archive.org/web/20150326055019/http://www.graph500.org/specifications#sec-5#sec-5</a></p>
<p>Castillo, G., &amp; Zhang, W. (2018). <em>ECE 5463 Introduction to Robotics</em>. Retrieved from <a class="reference external" href="http://www2.ece.ohio-state.edu/~zhang/RoboticsClass/docs/ECE5463_ROSTutorialLecture3.pdf">http://www2.ece.ohio-state.edu/~zhang/RoboticsClass/docs/ECE5463_ROSTutorialLecture3.pdf</a></p>
<p>Durrant-Whyte, H., &amp; Bailey, T. (2006). Simultaneous localization and mapping: part I. In H. Durrant-Whyte, &amp; T. Bailey, <em>Simultaneous Localisation and Mapping (SLAM): Part I The Essential Algorithms</em> (Vol. 13, pp. 99-110). IEEE. doi:10.1109/MRA.2006.1638022</p>
<p>Gerkey, B. P. (2020). <em>AMCL</em>. Retrieved from ROS Organisation: <a class="reference external" href="https://wiki.ros.org/amcl">https://wiki.ros.org/amcl</a></p>
<p>Gonzalez, R. C., &amp; Woods, R. E. (2008). Digital Image Processing, 3rd Edition. In R. C. Gonzalez, &amp; R. E. Woods, <em>Digital Image Processing, 3rd Edition</em> (3nd ed.).</p>
<p>Marquez, &amp; Wu. (2003). <em>A non-self-intersection Douglas-Peucker algorithm.</em> Sao Carlos.</p>
<p>Menon, A. (2019, April). <em>Face Detection in 2 Minutes using OpenCV &amp; Python</em>. Retrieved from towardsdatascience: <a class="reference external" href="https://towardsdatascience.com/face-detection-in-2-minutes-using-opencv-python-90f89d7c0f81">https://towardsdatascience.com/face-detection-in-2-minutes-using-opencv-python-90f89d7c0f81</a></p>
<p>Pizer, S. M., Amburn, E. P., Austin, J. D., Cromartie, R., Geselowitz, A., Greer, T., … Zuiderveld, K. (1987). Adaptive histogram equalization and its variations. In <em>Computer Vision, Graphics, and Image Processing</em> (Vol. 39, pp. 355-368). doi:<a class="reference external" href="https://doi.org/10.1016/S0734-189X(87)80186-X">https://doi.org/10.1016/S0734-189X(87)80186-X</a>.</p>
<p>Robotis. (2020, August 31). <em>Robotis e-manul</em>. Retrieved from <a class="reference external" href="https://emanual.robotis.com/docs/en/platform/turtlebot3/appendix_lds_01/">https://emanual.robotis.com/docs/en/platform/turtlebot3/appendix_lds_01/</a></p>
<p>Srivastava, S. (2020, January 24). <em>Detecting Camera Tampering using OpenCV and Python</em>. Retrieved from medium: <a class="reference external" href="https://medium.com/&#64;simranjenny84/camera-tampering-detection-81b25f0d7383">https://medium.com/&#64;simranjenny84/camera-tampering-detection-81b25f0d7383</a></p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="appendices">
<h1>8. Appendices<a class="headerlink" href="#appendices" title="Permalink to this headline">¶</a></h1>
<p><img alt="" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.023.png" />Appendix A: Turtlebot, T intersection</p>
<p><img alt="" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.024.png" /></p>
<p>Appendix B: ROS rqt graph</p>
<p><img alt="Diagram Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.025.png" /></p>
<p><img alt="A picture containing indoor, floor, empty Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.026.jpeg" />Appendix C: filter wall using CLAHE and lane detection</p>
<p><img alt="A picture containing text Description automatically generated" src="../../_images/Aspose.Words.98518097-67c8-4094-9d2f-cd096ee4a1ef.027.png" /></p>
<p>11</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id3"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>A visualization tool for Turtlebot</p>
</dd>
<dt class="label" id="id4"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>AMCL stands for Adaptive Monte Carlo Localization</p>
</dd>
</dl>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./cv/3431_project"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="../cv.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Placeholder</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../../ds-courses/unsw/9417/9417-intro.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Machine Learning (UNSW)</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Jerry<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>