
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Project - report &#8212; My study notebook on AI</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css?v=c1a6f12f" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=ff8fa330"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../_static/togglebutton.js?v=97881d71"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ds-courses/unsw/9417/9417-report/report';</script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="PGM (UNSW)" href="../../9418/9418-intro.html" />
    <link rel="prev" title="Project - Classification" href="../9417-project.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">My study notebook on AI</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://postsent.github.io/home/">Home</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../reference.html">Acknowledgement</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Science Related Courses</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../9417-intro.html">Machine Learning (UNSW)</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../9417-basic.html">Basic with examples</a></li>







<li class="toctree-l2"><a class="reference internal" href="../9417-project.html">Project - Classification</a></li>








<li class="toctree-l2 current active"><a class="current reference internal" href="#">Project - report</a></li>







</ul><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../9418/9418-intro.html">PGM (UNSW)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../9418/9418-EDA.html">EDA on Times Series Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../9418/9418-project.html">Time series project code</a></li>






<li class="toctree-l2"><a class="reference internal" href="../../9418/9418-project/report.html">Time series report</a></li>





</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../9517/9517-intro.html">Computer Vision (UNSW)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../9517/9517-a1-code.html">Basic image processing, thresholding, count cells</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../9517/9517-a1/report.html">Report on Basic image processing, etc</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../9517/9517-lane_detection.html">Lane detection</a></li>





<li class="toctree-l2"><a class="reference internal" href="../../9517/9517-vehicle-detection.html">Vehicle detection (by teammate)</a></li>




</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../9444/9444-intro.html">Deep Learning (UNSW)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../9444/9444-project.html">Image classification</a></li>








<li class="toctree-l2"><a class="reference internal" href="../../9444/assignment1/assignment1.html">Characters, Spirals and Hidden Unit Dynamics</a></li>




















</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../3900/3900-intro.html">Capstone (UNSW)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../3900/project/3900-project.html"><strong>Chatbot</strong></a></li>






</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../3431/3431-intro.html">ROS &amp; CV (UNSW)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../3431/project/project.html">Mini self-driving</a></li>









</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../usyd/5046/5046-intro.html">NLP (USYD)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../usyd/5046/a2/a2.html">Report - In-game Toxicity Detection</a></li>







<li class="toctree-l2"><a class="reference internal" href="../../../usyd/5046/a2.html">In Game Toxicity seq2seq classification</a></li>





<li class="toctree-l2"><a class="reference internal" href="../../../usyd/5046/a1.html">Binary text classification</a></li>






</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../usyd/5349/5349-intro.html">Cloud Computing (USYD)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../usyd/5349/a2/a2.html">Data Preprocessing and Performance Tuning with Spark</a></li>









<li class="toctree-l2"><a class="reference internal" href="../../../usyd/5349/a2.html">Data Preprocessing and Performance Tuning with Spark</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../usyd/5349/a1.html">Text Analysis with Spark RDD API</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../../usyd/5349/a1-report.html">Report – Text Analysis with Spark RDD API</a></li>






</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../usyd/5338/5338-intro.html">NoSQL &amp; Neo4j (USYD)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../usyd/5338/a1.html">NoSQL basic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../usyd/5338/a2.html">NoSQL Aggregation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../usyd/5338/a2-report/report.html"><strong>Performance Observation Task</strong></a></li>





<li class="toctree-l2"><a class="reference internal" href="../../../usyd/5338/a3.html">Neo4j Basic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../usyd/5338/a4.html">Neo4j Query</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../usyd/5048/5048-intro.html">Visual Analytics Tableau (USYD)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../usyd/5048/individual.html">Individual Report</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../usyd/5048/group.html">Group Report (My part)</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../usyd/5328/5328-intro.html">Advance ML (USYD)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../usyd/5328/a1.html">NMF</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../usyd/5328/a2.html">Label Noise Learning</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ml/regression/regression.html">Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ml/regression/p1-crypto-prediction.html">Crypto Prediction</a></li>



</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ml/classification/classification.html">Classification (placeholder)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ml/time-series/time-series.html">Time Series (placeholder)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ml/ml.html">General</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../dl/dl.html">General</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../nlp/nlp.html">placeholder</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Vision</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../cv/cv.html">Placeholder</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../misc/math.html">Math</a></li>


<li class="toctree-l1"><a class="reference internal" href="../../../../misc/misc.html">Misc</a></li>




<li class="toctree-l1"><a class="reference internal" href="../../../../misc/term.html">Terminology</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../../../misc/todo.html">TODO</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Coding Basic</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../python/basic-intro.html">Numpy, Pandas, Python</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../python/numpy.html">Numpy</a></li>





<li class="toctree-l2"><a class="reference internal" href="../../../../python/pandas.html">Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../python/leetcode.html">Leetcode</a></li>








</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Side Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../side-project/web-scrapter.html">Course Enrolment Scrapter</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unfinished</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../unfinished/pytorch-nlp-bk/nlp-book.html">NLP Book</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../unfinished/pytorch-nlp-bk/intro.html">Intro</a></li>













<li class="toctree-l2"><a class="reference internal" href="../../../../unfinished/pytorch-nlp-bk/nn.html">Feed-Forward Networks for NLP</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../../unfinished/pytorch-nlp-bk/prac-ch3.html">Chapter 3</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../unfinished/jigsaw-intro.html">Jigsaw</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../unfinished/jiagsaw-toxic-comment-serverity-rate/README.html">Folder Structure</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../../unfinished/jiagsaw-toxic-comment-serverity-rate/notebooks/simple-rnn.html">Simple</a></li>













<li class="toctree-l2"><a class="reference internal" href="../../../../unfinished/jiagsaw-toxic-comment-serverity-rate/notebooks/lstm.html">Upgrade RNN</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../../../unfinished/jiagsaw-toxic-comment-serverity-rate/notebooks/helper.html">Common</a></li>

</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../unfinished/nlp-reading.html">Book Reading</a></li>


</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/postsent/home" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../../_sources/ds-courses/unsw/9417/9417-report/report.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Project - report</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Project - report</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exploratory-data-analysis">exploratory data analysis</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-data-distribution-and-feature-correlation"><em>A. Data distribution and feature correlation</em></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#methodology">methodology</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-select-classification-models"><em>A) Select classification models</em></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#b-feature-selection"><em>B. Feature selection</em></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-outlier-removal"><em>C. Outlier removal</em></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interquartile-range"><em>1) Interquartile range</em></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#isolation-forest"><em>2) Isolation forest</em></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#density-based-spatial-clustering-of-application-with-noise"><em>3) Density-based Spatial Clustering of Application with Noise</em></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-parameter-tuning"><em>D. Parameter tuning</em></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#e-voting-ensemble"><em>E. Voting ensemble</em></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#f-final-model"><em>F. Final model</em></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#result">result</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion">discussion</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">conclusion</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#reference">reference</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="project-report">
<h1>Project - report<a class="headerlink" href="#project-report" title="Link to this heading">#</a></h1>
<p>Machine Learning in the Unknown</p>
<p>Group Name: Boston Static</p>
<p>Group Members:</p>
<p>University of New South Wales</p>
<p>Author Note</p>
<p>This is a group project report for the course COMP9417 in 21T1</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h1>
<p>This project aims to create a machine learning model that makes the best prediction on the test set based on class-weighted F1 scores. The machine learning pipeline applied involves pre-processing, feature engineering, removing outliers, parameter tuning and selecting the final model by comparing the weighted F1-score on the validation set.</p>
<p>Based on the project specification, datasets that contain no domain knowledge of the features are given. CSV files are provided for training, validation and testing set, in which the test set contains only instances with no class label. The datasets consist of 8346 training and 2782 validation instances with 128 real-valued features. No headers are provided, and the target variable is composed of 6 possible values, with all features and the target variable being numerical. The test set contains 2782 data with features values only. We were required to build a model using this dataset, which is expected to accurately predict the results for the testing set.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="exploratory-data-analysis">
<h1>exploratory data analysis<a class="headerlink" href="#exploratory-data-analysis" title="Link to this heading">#</a></h1>
<p>This section explains the nature of the dataset such as class distribution and feature importance by graph demonstration. Given the nature of the unknown feature name, the data category is first identified via a box plot. Then value range of each column is visually evaluated to extract the underlying meaning of that feature, which helps to determine how to apply the most appropriate method.</p>
<section id="a-data-distribution-and-feature-correlation">
<h2><em>A. Data distribution and feature correlation</em><a class="headerlink" href="#a-data-distribution-and-feature-correlation" title="Link to this heading">#</a></h2>
<p>Based on the first diagram in Figure 1, the distribution between different classes in the target variable of the training set is not balanced. Class label 6 contains the smallest percentage of data (11.9%) and class label 2 contains the largest (21.4%). No missing values were found in the dataset, and all data was structured i.e., numerical data (Figure 2).</p>
<p>The second diagram in Figure 1 illustrates the top 10 most important features determined by the Extra Trees classifier<a class="footnote-reference brackets" href="#id4" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>. The significance of these features is computed based on the Gini importance, which is a measure of the mean impurity over all splits that includes that feature.  It shows that column 8 has the highest relative importance and all 10 features selected are above 60% relative significance. The fourth diagram illustrates the tuning for the principal component analysis, in which the maximum variance regarding the number of features converges around 10 features.  This is similar to the default value for the parameter max_features in the tree model where the number is determined by the square root of the total number of features, which is 11.</p>
<p><img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.001.png" /> <img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.002.png" /> <img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.003.png" /><img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.004.png" /></p>
<p>Figure 1: target variable distribution in training set, top 10 most important features selected by extra tree classifier,  10 features by ANOVA, PCA <a class="footnote-reference brackets" href="#id5" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>comparison in the number of component vs variance<a class="footnote-reference brackets" href="#id6" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a></p>
<p><img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.005.png" /></p>
<p>Figure 2: boxplot  for original data</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="methodology">
<h1>methodology<a class="headerlink" href="#methodology" title="Link to this heading">#</a></h1>
<p>The details of methods and pipeline are described below. The pipeline involves pre-processing the data, feature selection, outlier removal, parameter tuning and voting ensemble. Dimensionality reduction is applied before outlier removal. It is because if the order is shifted, additional rows would be removed due to the outliers in the redundant feature. 10 folds stratified cross-validation is applied for analysis of the variance for different models and the validation set is used as the final testing (see result section). The models are compared before and after for each processing to achieve the maximum outcome.</p>
<section id="a-select-classification-models">
<h2><em>A) Select classification models</em><a class="headerlink" href="#a-select-classification-models" title="Link to this heading">#</a></h2>
<p>Figure 3 illustrates the comparison of the performance for different classification models on the standardized zero-mean data. The classification models used include Logistic Regression, Linear Discriminant Analysis, K-Nearest Neighbours, Decision Tree, Naive Bayes, Support Vector Machine, Decision Tree Regressor, Random Forest, Extra Trees Classifier, and Multi-layer Perceptron. This process filters the models based on the scores and variance. Results are shown in the table below, and the models selected for the next phase are KNN, Logistic Regression, Random Forest, and Extra Trees Classifier.</p>
<p><code class="docutils literal notranslate">&#160;  </code><img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.006.png" />    <img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.007.png" /><img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.008.png" /></p>
<p>Figure 3: model comparison between unstandardised and standardised data, variance and time comparison, Boxplot for models on standardised data</p>
</section>
<section id="b-feature-selection">
<h2><em>B. Feature selection</em><a class="headerlink" href="#b-feature-selection" title="Link to this heading">#</a></h2>
<p>This section discusses some feature selection techniques regarding numerical input and categorical output.</p>
<p>As seen in Figure 1, the ANOVA f-test is used to select the top 10 most important features. And the result is compared with the Extra Trees classifier selection. ANOVA stands for “analysis of variance” and is a parametric statistical hypothesis that decides if the means of two or more data samples belongs to the same distribution. Such method calculates the maximum relevance between a feature and a class label, and the correlation is measured to minimise redundancy [1].</p>
<p>Besides, a correlation matrix of 128 columns is plotted below. It can be seen that feature No. 107 is highly correlated with the output variable y. Hence, we assigned this as a key feature for predicting the value of y. Since only one feature is found to have an absolute correlation value greater than 0.5 with y, no further step is taken to filter features during this process.</p>
<p><code class="docutils literal notranslate">&#160;  </code><img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.009.png" /> <img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.010.png" /><img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.011.png" /></p>
<p>Figure 4: Correlation Matrix, feature importance by Extra Trees classifier</p>
<p>In this process, tree-based feature selection was used instead of L1-based and PCA selection, as Lasso Regression does not evaluate whether the correct form of the relationship has been chosen between the independent and dependent variables. Furthermore, PCA is not chosen as the measurements from all the original variables are used in the projection to the lower-dimensional space, only linear relationships are considered, and PCA or SVD-based methods, as well as univariate screening methods, do not consider the potential multivariate nature of the data structure.</p>
<p>As seen in Figure 5, zero mean standardised data gives more accurate model performance than min-max normalisation. Based on Figure 6, PCA dimensionality reduction performs better than tree and linear SVM model selection. In general, the optimal number of the component in PCA is around 20. The scoring of the best model - Extra Trees classifier is improved to 0.9965 from 0.9955 (before the changes). (PCA is not applied in the following section due to time constraint)</p>
<p><code class="docutils literal notranslate">&#160;&#160;&#160;&#160; </code><img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.012.png" /> <img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.013.png" /><img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.014.png" /><img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.015.png" /></p>
<p>Figure 5: 10 folds cross-validation results on dimensionality reduction data by Extra Tress classifier, linear SVM (data is normalized min-max and min-max)</p>
<p><code class="docutils literal notranslate">&#160; </code><img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.016.png" /><img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.017.png" /></p>
<p>Figure 6: 10 folds cross-validation results on dimensionality reduction data by PCA, result from component = 20</p>
</section>
<section id="c-outlier-removal">
<h2><em>C. Outlier removal</em><a class="headerlink" href="#c-outlier-removal" title="Link to this heading">#</a></h2>
<p>As mentioned above, only numerical data is concerned in this dataset. Pre-processing for numerical features applies differently to the tree and non-tree models. In general, tree-based models do not depend on scaling for which a non-tree classifier could rely heavily on [2]. Outliers are often considered as a negative factor that skews the tendency of the data which results in poor model scoring. As a result, the outlier removal technique could lead to a more accurate model. Removing outliers involves deletion or replacement by the median value of the data points. Minimum Covariance Determinant is not considered as the data does not follow the pattern of gaussian distribution. Local Outlier Factors is not chosen due to the high dimensionality of the data. Three approaches are evaluated in this section and the results are summarised in the table below.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Outlier method</p></th>
<th class="head text-left"><p>Number of rows removed</p></th>
<th class="head text-left"><p>Percentage of rows removed</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Interquartile range</p></td>
<td class="text-left"><p>2949</p></td>
<td class="text-left"><p>0.3533</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Isolation forest</p></td>
<td class="text-left"><p>1029</p></td>
<td class="text-left"><p>0.1233</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><h3>Density-based spatial clustering</h3></p></td>
<td class="text-left"><p>1316</p></td>
<td class="text-left"><p>0.1577</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Figure 7: comparison between different outlier removal methods</p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
</tr>
</tbody>
</table>
</div>
<section id="interquartile-range">
<h3><em>1) Interquartile range</em><a class="headerlink" href="#interquartile-range" title="Link to this heading">#</a></h3>
<p>The interquartile range is a statistical-based approach that could be applied when the data cannot be normalized by Gaussian distribution [3]. It describes the middle 50% of the data, ordering the data from smallest to largest. The first (25%) and third (75%) quartile is defined by the median value of the lower and upper half of the data. According to [4], such method often assumes that the high probability regions of a stochastic model contain the normal data, whereas the outliers stay in the low probability domain. The threshold for identifying an outlier is set to be 1.5 times the data range away from the middle quartile. As seen below box plot, the y-scale changed from 700000 to 140000 which is 5 times smaller than the original scale.</p>
<p><img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.018.png" /> <img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.019.png" /></p>
<p>Figure 8: box plot of data after interquartile range, after isolation forest</p>
</section>
<section id="isolation-forest">
<h3><em>2) Isolation forest</em><a class="headerlink" href="#isolation-forest" title="Link to this heading">#</a></h3>
<p>According to [5], anomaly detection could be achieved by an unsupervised learning algorithm named isolation forest. While most model-based methods detect outliers via separating those that do not conform to the profile of normal instances, this approach explicitly isolates anomalies and is considered as a fast and low memory demand method. Based on the third graph from Figure 8, the y scale of the data reduced from 700000 to 250000 (around 3 times smaller). Compared to the interquartile range, this approach is less aggressive and preserves more pattern of the original data.</p>
</section>
<section id="density-based-spatial-clustering-of-application-with-noise">
<h3><em>3) Density-based Spatial Clustering of Application with Noise</em><a class="headerlink" href="#density-based-spatial-clustering-of-application-with-noise" title="Link to this heading">#</a></h3>
<p>Another potential method for outlier removal would be Density-based Spatial Clustering of Application with Noise or DBSCAN. DBSCAN is an unsupervised, density-based algorithm, which takes features within the data set, and clusters them based on parameters. Any data points that are not within these clusters and are not ‘<em>density reachable</em>’ from any other point would be considered outliers and can be removed from the data set. By selecting features that have the largest impact on the class feature, we would then be able to find outliers within the data set concerning these selected features. Based on the graph, the y scale of the data gets reduced from 700000 to 300000, acting less aggressive than both Interquartile Range and Isolation Forest, preserving more of the original data</p>
<p><img alt="" src="../../../../_images/Aspose.Words.b67f9915-5a2e-4272-8f87-a899219b12ca.020.png" /></p>
<p>Figure 9：box plot for data filtered by DBSCAN</p>
</section>
</section>
<section id="d-parameter-tuning">
<h2><em>D. Parameter tuning</em><a class="headerlink" href="#d-parameter-tuning" title="Link to this heading">#</a></h2>
<p>10 folds cross-validation was applied for parameter tuning as it provides an unbiased estimation for the final result. Grid search and random search are two commonly used parameter tuning techniques. According to [6], a Grid search evaluates all combination of hyperparameter values and is considered as an exhaustive search among the parameters. This approach gives high accuracy result but is often time inefficient. The random search examines random combinations of the hyperparameters, which allows the users to reduce the search space. Grid search is applied to the candidate models includes the Extra Trees classifier, Random Forest, KNN, Logistic Regression. Based on the below table, the optimal nearest neighbour for KNN is 1, which is led by the high dimensionality of the data.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Classifier</p></th>
<th class="head text-center"><p>Hyperparameter s</p></th>
<th class="head text-center"><p>Weighted F1 scores by 10 folds cross-validation on the training set</p></th>
<th class="head text-center"><p>time</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>KNN</p></td>
<td class="text-center"><p>n_neighbors=1</p></td>
<td class="text-center"><p>0.9943</p></td>
<td class="text-center"><p>10s</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Logistic regression</p></td>
<td class="text-center"><p>C=1, max_iter=200</p></td>
<td class="text-center"><p>0.9915</p></td>
<td class="text-center"><p>10 minutes+</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Extra  Trees classifier</p></td>
<td class="text-center"><p>max_features=11, min_samples_leaf=1, min_samples_split=2, n_estimators=200, random_state=23</p></td>
<td class="text-center"><p>0.9956</p></td>
<td class="text-center"><p>1 hour +</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Random forest</p></td>
<td class="text-center"><p>max_features=11, min_samples_leaf=1, min_samples_split=2, n_estimators=200, random_state=23</p></td>
<td class="text-center"><p>0.9951</p></td>
<td class="text-center"><p>1 hour +</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>MLP</p></td>
<td class="text-center"><p>activation=’relu’, hidden_layer_sizes=(50, 100, 50), learning_rate=’adaptive’, alpha=0.0001, solver=’adam’</p></td>
<td class="text-center"><p>0.9932</p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Figure 10: comparison in score after parameter tuning</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="e-voting-ensemble">
<h2><em>E. Voting ensemble</em><a class="headerlink" href="#e-voting-ensemble" title="Link to this heading">#</a></h2>
<p>To improve the performance of a single model and lower the variance, the machine learning model voting ensemble that combines the predictions from multiple other models is often considered.</p>
<p>A soft voting ensemble that predicts the class label based on the probability of all models, is then applied for the classification problem. The hard voting ensemble was not considered as it anticipates the result based on the most popular vote among the models, which ignores the strength and weakness of each model.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Classifier</p></th>
<th class="head text-left"><p>10 folds cross-validation on the training set</p></th>
<th class="head text-left"><p>Predication on the validation set</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>ExtraTrees classifier + random forest + MLP</p></td>
<td class="text-left"><p>0.9957</p></td>
<td class="text-left"><p>0.9931691</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>ExtraTrees classifier + random forest + MLP + KNN</p></td>
<td class="text-left"><p>0.9952 (variance = 0.001517)</p></td>
<td class="text-left"><p><p>0.9931707</p><p></p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Figure 11: result from the voting ensemble</p></td>
<td class="text-left"><p></p></td>
<td class="text-left"><p></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="f-final-model">
<h2><em>F. Final model</em><a class="headerlink" href="#f-final-model" title="Link to this heading">#</a></h2>
<p>Based on the final performance on the validation set, the voting ensemble (Extra Trees classifier + Random Forest + MLP + KNN) was chosen for the prediction for its low variance (the lowest among all candidate models,  Figure 3) and high F1-score. The hyperparameters of each model are listed above in the parameter tuning section with the unlisted ones as default. The result only differs by 3 compared with the one gained from the extra tree classifier.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="result">
<h1>result<a class="headerlink" href="#result" title="Link to this heading">#</a></h1>
<p>The weighted F1-score is then used as the evaluation metric, and the final testing is performed on the validation set (as no testing set was provided), which is unseen to the model. All models except for MLP and tree model are trained based on standardised data via z-score.</p>
<p>Based on <em>Figure 12</em>, the voting ensemble can be considered the most suitable model to apply for this dataset as it gives a balanced trade-off between the variance (overfitting issue is minimised) and the F1-score. The problem of the curse of dimensionality and outliers is minimised as the voting ensemble includes two tree models, so it is considered flexible and reliable to apply for this numerical dataset.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Tuned Classifier</p></th>
<th class="head"><p>Score on the validation set (4 D.C.)</p></th>
<th class="head"><p>Variance of 10 folds cross-validation on the training set</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Extra Tree classifier</p></td>
<td><p>0.9932</p></td>
<td><p>0.001721</p></td>
</tr>
<tr class="row-odd"><td><p>Random Forest</p></td>
<td><p>0.9910</p></td>
<td><p>0.002135</p></td>
</tr>
<tr class="row-even"><td><p>KNN</p></td>
<td><p>0.9928</p></td>
<td><p>0.004484</p></td>
</tr>
<tr class="row-odd"><td><p>Logistic Regression</p></td>
<td><p>0.9871</p></td>
<td><p>0.001836</p></td>
</tr>
<tr class="row-even"><td><p>Multi-layer Percetron</p></td>
<td><p>0.9932</p></td>
<td><p>0.002563</p></td>
</tr>
<tr class="row-odd"><td><p>Voting Ensemble (Extra Trees classifier + Random Forest + MLP)</p></td>
<td><p>0.9932</p></td>
<td><p>0.001517</p></td>
</tr>
<tr class="row-even"><td><p>Figure 12: Model comparison in the validation set</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="discussion">
<h1>discussion<a class="headerlink" href="#discussion" title="Link to this heading">#</a></h1>
<p>It was observed that the optimal parameters for the extra tree classifier and the random forest are the same (Figure <em>10</em>), and both have a similar F1-score. This is predictable as both ensemble methods are composed of many subtrees, where the random forest applies bootstrap and the extra tree uses the whole sample space. Besides, during the runtime, the random forest chooses the optimal split whereas the extra tree does it randomly, both choosing the best subset of features after the final split point. Thus, it allows the extra tree to run faster than the random forest [7]. Given the high dimension of the data, it is understandable to observe that the tree and MLP model performs better than logistic regression and KNN. And simple model such as Naïve Bayes and linear discriminative analysis performs poorly for this dataset.</p>
<p>The feature importance selected by ANVOA and tree classifier is both similar. The reason for this could be the nature of feature selection primarily focus on the variance and correlation of the data. In addition, it was observed that dimensionality reduction could substantially reduce the training time for the model. However, during the testing, both outlier removal and feature selection did not improve the F1-score for the model selected. It was concluded that the chosen tree model, KNN and multi-layer perceptron deploy sufficient mechanism within its own implementation that pre-processing of the data have minimal impact on them compared to the parameter tuning effect.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="conclusion">
<h1>conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h1>
<p>In conclusion, this report discusses various approaches in the standard machine learning pipeline for classification problems concerning numerical data. This pipeline includes data pre-processing, such as visualising the nature of the data, feature selection, outlier removal using various methods such as Isolation Forest and Interquartile Range, parameter tuning, and comparison between the tree and non-tree models. Based on the final results gained from the validation set, the voting ensemble was determined to be the best of use in the final prediction because of its low variance and high F1 score. Some future improvements include comparing more pre-processing approaches and exploring different open-source libraries that are summarised in [8] or in many great review papers and journal articles. Additionally, more research could have been made into better methods for feature selection and outlier removal, to help tune the model to better predict test data.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="reference">
<h1>reference<a class="headerlink" href="#reference" title="Link to this heading">#</a></h1>
<p>The idea of pipeline and code for comparing model baseline score comes from [2].</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>[1]</p></th>
<th class="head text-left"><p>D. F. G. Zena M. Hira, “A Review of Feature Selection and Feature Extraction Methods Applied on Microarray Data,” <em>Adv Bioinformatics,</em> 2015.</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>[2]</p></td>
<td class="text-left"><p>“A Complete ML Pipeline Tutorial,” 2018. [Online]. Available: <a class="reference external" href="https://www.kaggle.com/pouryaayria/a-complete-ml-pipeline-tutorial-acu-86/notebook">https://www.kaggle.com/pouryaayria/a-complete-ml-pipeline-tutorial-acu-86/notebook</a>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>[3]</p></td>
<td class="text-left"><p>J. Brownlee, “How to Remove Outliers for Machine Learning,” 2018. [Online]. Available: <a class="reference external" href="https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/">https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/</a>.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>[4]</p></td>
<td class="text-left"><p>I. F. Ilyas and X. Chu, in <em>Data Cleaning</em>, New York, 2019, p. 12.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>[5]</p></td>
<td class="text-left"><p>F. T. Liu, K. M. Ting and Z. Zhou, “Isolation Forest,” <em>2008 Eighth IEEE International Conference on Data Mining,</em> pp. 413 - 422, 2008.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>[6]</p></td>
<td class="text-left"><p>P. Worcester, “A Comparison of Grid Search and Randomized Search Using Scikit Learn,” Jun 2019. [Online]. Available: <a class="reference external" href="https://blog.usejournal.com/a-comparison-of-grid-search-and-randomized-search-using-scikit-learn-29823179bc85">https://blog.usejournal.com/a-comparison-of-grid-search-and-randomized-search-using-scikit-learn-29823179bc85</a>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>[7]</p></td>
<td class="text-left"><p>G. Pierre, D. Ernst and L. Wehenkel, “Extremely randomized trees,” pp. 3-42, 2006.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>[8]</p></td>
<td class="text-left"><p>J. Misiti, “Machine learning frameworks, libraries and software,” 2016. [Online]. Available: <a class="github reference external" href="https://github.com/josephmisiti/awesome-machine-learning">josephmisiti/awesome-machine-learning</a>.</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>[9]</p></td>
<td class="text-left"><p>A. Andrade and L. Golab, “DATA SCIENCE GUIDE,” 2016. [Online]. Available: <a class="reference external" href="https://datascienceguide.github.io/exploratory-data-analysis">https://datascienceguide.github.io/exploratory-data-analysis</a>.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>[10]</p></td>
<td class="text-left"><p>M. Alam, “DBSCAN — a density-based unsupervised algorithm for fraud detection,” 2020. [Online]. Available: <a class="reference external" href="https://towardsdatascience.com/dbscan-a-density-based-unsupervised-algorithm-for-fraud-detection-887c0f1016e9">https://towardsdatascience.com/dbscan-a-density-based-unsupervised-algorithm-for-fraud-detection-887c0f1016e9</a>.</p></td>
</tr>
</tbody>
</table>
</div>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id4" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Extra tree stands for extremely randomized trees</p>
</aside>
<aside class="footnote brackets" id="id5" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>PCA stands for principal component analysis, a dimensionality-reduction method</p>
</aside>
<aside class="footnote brackets" id="id6" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>How far the data spreads from its mean</p>
</aside>
</aside>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ds-courses\unsw\9417\9417-report"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../9417-project.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Project - Classification</p>
      </div>
    </a>
    <a class="right-next"
       href="../../9418/9418-intro.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">PGM (UNSW)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Project - report</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exploratory-data-analysis">exploratory data analysis</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-data-distribution-and-feature-correlation"><em>A. Data distribution and feature correlation</em></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#methodology">methodology</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-select-classification-models"><em>A) Select classification models</em></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#b-feature-selection"><em>B. Feature selection</em></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-outlier-removal"><em>C. Outlier removal</em></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interquartile-range"><em>1) Interquartile range</em></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#isolation-forest"><em>2) Isolation forest</em></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#density-based-spatial-clustering-of-application-with-noise"><em>3) Density-based Spatial Clustering of Application with Noise</em></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-parameter-tuning"><em>D. Parameter tuning</em></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#e-voting-ensemble"><em>E. Voting ensemble</em></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#f-final-model"><em>F. Final model</em></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#result">result</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion">discussion</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">conclusion</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#reference">reference</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Jerry
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2021.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>