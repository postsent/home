
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Binary text classification &#8212; My AI Notebook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=c1a6f12f" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=ff8fa330"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=97881d71"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ds-courses/usyd/5046/a1';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Machine Learning (UNSW)" href="../../unsw/9417/9417-intro.html" />
    <link rel="prev" title="In Game Toxicity seq2seq classification" href="a2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">My AI Notebook</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://postsent.github.io/home/">Home</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../reference.html">Acknowledgement</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Research</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../research/polyu/dp/report.html">Data Pruning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../research/polyu/dd/report.html">Dataset Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../research/polyu/story/page.html">DD Motivation</a></li>







<li class="toctree-l1"><a class="reference internal" href="../../../research/nas/doc.html">NAS</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Courses</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../unsw/9444/9444-intro.html">Deep Learning (UNSW)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../unsw/9444/9444-project.html">Image classification</a></li>








<li class="toctree-l2"><a class="reference internal" href="../../unsw/9444/assignment1/assignment1.html">Characters, Spirals and Hidden Unit Dynamics</a></li>




















</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../unsw/3900/3900-intro.html">Capstone (UNSW)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../unsw/3900/project/3900-project.html"><strong>Chatbot</strong></a></li>






</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../unsw/3431/3431-intro.html">ROS &amp; CV (UNSW)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../unsw/3431/project/project.html">Mini self-driving</a></li>









</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="5046-intro.html">NLP (USYD)</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="a2/a2.html">Report - In-game Toxicity Detection</a></li>







<li class="toctree-l2"><a class="reference internal" href="a2.html">In Game Toxicity seq2seq classification</a></li>





<li class="toctree-l2 current active"><a class="current reference internal" href="#">Binary text classification</a></li>






</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../unsw/9417/9417-intro.html">Machine Learning (UNSW)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../unsw/9417/9417-basic.html">Basic with examples</a></li>







<li class="toctree-l2"><a class="reference internal" href="../../unsw/9417/9417-project.html">Project - Classification</a></li>








<li class="toctree-l2"><a class="reference internal" href="../../unsw/9417/9417-report/report.html">Project - report</a></li>







</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../unsw/9517/9517-intro.html">Computer Vision (UNSW)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../unsw/9517/9517-a1-code.html">Basic image processing, thresholding, count cells</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../unsw/9517/9517-a1/report.html">Report on Basic image processing, etc</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../unsw/9517/9517-lane_detection.html">Lane detection</a></li>





<li class="toctree-l2"><a class="reference internal" href="../../unsw/9517/9517-vehicle-detection.html">Vehicle detection (by teammate)</a></li>




</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../5349/5349-intro.html">Cloud Computing (USYD)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../5349/a2/a2.html">Data Preprocessing and Performance Tuning with Spark</a></li>









<li class="toctree-l2"><a class="reference internal" href="../5349/a2.html">Data Preprocessing and Performance Tuning with Spark</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5349/a1.html">Text Analysis with Spark RDD API</a></li>



<li class="toctree-l2"><a class="reference internal" href="../5349/a1-report.html">Report – Text Analysis with Spark RDD API</a></li>






</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../5338/5338-intro.html">NoSQL &amp; Neo4j (USYD)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../5338/a1.html">NoSQL basic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5338/a2.html">NoSQL Aggregation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5338/a2-report/report.html"><strong>Performance Observation Task</strong></a></li>





<li class="toctree-l2"><a class="reference internal" href="../5338/a3.html">Neo4j Basic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5338/a4.html">Neo4j Query</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../5048/5048-intro.html">Visual Analytics (USYD)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../5048/individual.html">Individual Report</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5048/group.html">Group Report (My part)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../5328/5328-intro.html">Advance ML (USYD)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../5328/a1.html">NMF</a></li>
<li class="toctree-l2"><a class="reference internal" href="../5328/a2.html">Label Noise Learning</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../unsw/9418/9418-intro.html">PGM (UNSW)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../unsw/9418/9418-EDA.html">EDA on Times Series Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../unsw/9418/9418-project.html">Time series project code</a></li>






<li class="toctree-l2"><a class="reference internal" href="../../unsw/9418/9418-project/report.html">Time series report</a></li>





</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../ml/regression/regression.html">Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../ml/regression/p1-crypto-prediction.html">Crypto Prediction</a></li>



</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ml/classification/classification.html">Classification (placeholder)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ml/time-series/time-series.html">Time Series (placeholder)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ml/ml.html">General</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../dl/dl.html">General</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../nlp/nlp.html">placeholder</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Vision</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../cv/cv.html">Placeholder</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../misc/math.html">Math</a></li>


<li class="toctree-l1"><a class="reference internal" href="../../../misc/misc.html">Misc</a></li>




<li class="toctree-l1"><a class="reference internal" href="../../../misc/term.html">Terminology</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../../misc/todo.html">TODO</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Coding Basic</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../python/basic-intro.html">Numpy, Pandas, Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../python/numpy.html">Numpy</a></li>





<li class="toctree-l2"><a class="reference internal" href="../../../python/pandas.html">Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../python/leetcode.html">Leetcode</a></li>








</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Side Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../side-project/web-scrapter.html">Course Enrolment Scrapter</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unfinished</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../unfinished/pytorch-nlp-bk/nlp-book.html">NLP Book</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../unfinished/pytorch-nlp-bk/intro.html">Intro</a></li>













<li class="toctree-l2"><a class="reference internal" href="../../../unfinished/pytorch-nlp-bk/nn.html">Feed-Forward Networks for NLP</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../unfinished/pytorch-nlp-bk/prac-ch3.html">Chapter 3</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../unfinished/jigsaw-intro.html">Jigsaw</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../unfinished/jiagsaw-toxic-comment-serverity-rate/README.html">Folder Structure</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../unfinished/jiagsaw-toxic-comment-serverity-rate/notebooks/simple-rnn.html">Simple</a></li>













<li class="toctree-l2"><a class="reference internal" href="../../../unfinished/jiagsaw-toxic-comment-serverity-rate/notebooks/lstm.html">Upgrade RNN</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../../unfinished/jiagsaw-toxic-comment-serverity-rate/notebooks/helper.html">Common</a></li>

</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../unfinished/nlp-reading.html">Book Reading</a></li>


</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/postsent/home/main?urlpath=tree/docs/ds-courses/usyd/5046/a1.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
  </ul>
</div>



<a href="https://github.com/postsent/home" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/ds-courses/usyd/5046/a1.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Binary text classification</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Binary text classification</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#readme">Readme</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preprocessing">1 - Data Preprocessing</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-collection-do-not-modify-this">1.0. Data Collection [DO NOT MODIFY THIS]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#url-removal">1.1. URL Removal</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocess-data-e-g-stop-words-stemming">1.2. Preprocess data (e.g. Stop words, Stemming)</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#input-representation">2 - Input Representation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embedding-construction">2.1. Word Embedding Construction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pretrained-word-embedding">2.2. Pretrained Word Embedding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#input-concatenation">2.3. Input Concatenation</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#model-implementation">3 - Model Implementation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-sequence-model-bi-directional-model">3.1. Build Sequence Model (Bi-directional model)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-sequence-model-bi-directional-model">3.2. Train Sequence Model (Bi-directional model)</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">4 - Evaluation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embedding-evaluation">4.1. Word Embedding Evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-evaluation-with-data-processing-techiques">4.2. Performance Evaluation with Data Processing Techiques</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-evaluation-with-different-input">4.3. Performance Evaluation with Different Input</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-evaluation-with-different-sequence-models">4.4. Performance Evaluation with Different Sequence Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-testing">4.5. HyperParameter Testing</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#object-oriented-programming-codes-here">Object Oriented Programming codes here</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#import-set-seed">Import &amp; Set seed</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#class-sequence-model">class - sequence model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#class-preprocessing">class - Preprocessing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#class-word-embedding">class - Word_embedding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#class-train">class - Train</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#class-instrinsic-eval">class - instrinsic eval</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocess-combo-compare">4.1 preprocess combo compare</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparams-tuning-data">4.5 hyperparams tuning &amp; data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#customdataset">CustomDataSet</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#helper">Helper</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#side-note">Side note</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eda">EDA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#profiling">profiling</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="binary-text-classification">
<h1>Binary text classification<a class="headerlink" href="#binary-text-classification" title="Link to this heading">#</a></h1>
<p>2022 COMP5046 Assignment 1 -
<em>Make sure you change the file name with your unikey.</em></p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="readme">
<h1>Readme<a class="headerlink" href="#readme" title="Link to this heading">#</a></h1>
<p>💎 Please <strong>run all OOP section first</strong> and then main section.💎<br />
💎 U can do it by click run after for the first cell in OOP:) 💎</p>
<ul class="simple">
<li><p>All detailed implmentation is written at the <strong>OOP</strong> section at the bottom.</p></li>
<li><p>Add best model result at section 4.4, which is loaded from gdrive.</p></li>
<li><p>I added batch size to training and better preprocessing after I finished all the evaluation, it turns out I then need to rewrite everything, so the evalution is based on the no batch version but the coding part has changed to the one using batch.</p></li>
</ul>
<p><em><strong>Visualising the comparison of different results is a good way to justify your decision.</strong></em></p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="data-preprocessing">
<h1>1 - Data Preprocessing<a class="headerlink" href="#data-preprocessing" title="Link to this heading">#</a></h1>
<section id="data-collection-do-not-modify-this">
<h2>1.0. Data Collection [DO NOT MODIFY THIS]<a class="headerlink" href="#data-collection-do-not-modify-this" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Code to download file into Colaboratory:
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

id = &#39;16g474hdNsaNx0_SnoKuqj2BuwSEGdnbt&#39;
downloaded = drive.CreateFile({&#39;id&#39;:id}) 
downloaded.GetContentFile(&#39;training_data.csv&#39;)  

id = &#39;1-7hj0sF3Rc5G6POKdkpbDXm_Q6BWFDPU&#39;
downloaded = drive.CreateFile({&#39;id&#39;:id}) 
downloaded.GetContentFile(&#39;testing_data.csv&#39;)  

import pandas as pd
training_data = pd.read_csv(&quot;/content/training_data.csv&quot;)
testing_data = pd.read_csv(&quot;/content/testing_data.csv&quot;)

print(&quot;------------------------------------&quot;)
print(&quot;Size of training dataset: {0}&quot;.format(len(training_data)))
print(&quot;Size of testing dataset: {0}&quot;.format(len(testing_data)))
print(&quot;------------------------------------&quot;)

print(&quot;------------------------------------&quot;)
print(&quot;Sample Data&quot;)
print(&quot;LABEL: {0} / SENTENCE: {1}&quot;.format(training_data.iloc[-1,0], training_data.iloc[-1,1]))
print(&quot;------------------------------------&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>------------------------------------
Size of training dataset: 7808
Size of testing dataset: 867
------------------------------------
------------------------------------
Sample Data
LABEL: F / SENTENCE: &#39;Half of it is going straight to charity, another quarter going straight to scientific research, an eighth to the parkour community, a sixteenth to towards spreading information about health and...|||Find a path or suffer more.|||http://personalitycafe.com/enneagram-personality-theory-forum/85323-enneagram-type-mbti-type-compared-statistics.html yep.|||I kind of anchor on Fi and Ne makes having Ni really fun. INFP for me as they tire me out less and our views tend to align more.|||The two ESTPs I have gotten the chance to know seem to experience much more than other people who have been on the planet for the same amount of time and are quite the renaissance (wo)men.  Is this...|||I don&#39;t really have a best friend ISTP(passion-amateur group co-founder), INTJ(intellectual and various small hobbies talk), ESTP(Bro-in-law, talk about everything kind of like my INTJ friend),...|||Everyone looses their gift if they don&#39;t even consider a different perspective.|||Kansas - ISTJ|||That or if they are normally comfortable with me, such as a friend or close acquaintance, they feel the need to start talking. It&#39;s almost a trap, I&#39;ve noticed for most people feel the need to expose...|||To me, your answers screamed introverted feeling. Answers 2-5, 10, 11, 14, 16, and 17 your last statement were particularly Fi-like. I&#39;m guessing you are an intuitive and possibly and introvert...|||Could you explain your reasoning for these? I saw Mako as an F, Lin as an ES, and have Kya as an F. Never had an idea for Amon&#39;s type.|||This applies to many of these threads.|||With an INFP for over 2 years now.|||After watching tonight&#39;s episode I&#39;m sure that Unalaq is an ENXJ. I&#39;m not sure if it&#39;s Fe or Te at this point but the way he goes about doing and planning things seem like a Je-dom. I&#39;m putting him...|||Parkour is my passion(but I consider it closer to a martial art than a sport). I also enjoy some running and climbing.|||I have many characters but I gravitate towards sneaky archer, Breton, and conjuration. I love doing role plays and think it&#39;s one of, if not the best way to play the game.|||ESFP seems right for Ikki. We may need Jinora to have more interactions for us to tell. Any guesses about Pema, Tenzin&#39;s wife? She said herself that she used to be very shy so I&#39;d put I just from...|||If you don&#39;t mind, please tell me more by what you meant by this bolded part or what happened.|||I think it&#39;s fit to revive this thread seeing as the second season of Korra has started and the second episode of the season is coming up tomorrow. I&#39;d just say beware of spoilers in new posts if you...|||I was thinking more along these lines: 83385|||Yes, a few times in friendships and other things but it was usually spurred on by the idea of not having a second chance. I&#39;ve been trying to make the first move more in life as I&#39;ve realized it just...|||Sorry if my wording was/is confusing or vague. Let me try to explain it better.  As for the first statement: I see the world for all it&#39;s interconnections. If you wish, visualized everything having...|||~I don&#39;t experience it as simply perceiving or creating, for me as I perceive interconnected relationships are formed and realized.   ~I don&#39;t think that I rationalize with my dominate function but...|||I think it&#39;s amusing that, in the leading position I share with an ISTP friend of mine, we both start to embrace our shadows. I Think that&#39;s been my growing point lately, embracing my shadow. We&#39;re...|||I would suggest introspection and relying on your sense of self over tests and I highly suggest looking into the cognitive functions. ISTJ is the complete opposite of INFJ.|||I definitely agree with others on the US- It&#39;s pretty good for an INFJ if you find your niche.  I say the Midwest is generally SJ with women expected to be F and men to be T. It&#39;s nice but annoying....|||Please explain|||I think my own eye movements have almost been changed because of where I was usually placed when talking to someone in normal conversations. See, when I was young I ended up getting permanent spot in...|||Judgmental, critical, somewhat narcissistic, stubborn, possessive, Fe-ishly manipulative, and I have ego issues. Take that with a grain of salt.|||Yes, very much so. I love Spanish so far.|||I have a huge folder of these types of images.|||Aquarian It was just my guess, it doesn&#39;t need that much merit. Personally, I think Se is the hardest function to describe because it is so in the moment.|||Sorry, double post because of connectivity weirdness.|||I don&#39;t know if this has been posted before or if a thread about curses would be the best place but it&#39;ll do just fine. The important part is post #79, the giant wall of text. I think most of it was...|||If anything, imo, Ni would be how objects are interconnected. If I were to follow closely to your model: Introverted Intuition: Understanding how objects are connected Extraverted Intuition:...|||Sometimes you just don&#39;t see them :ninja: Seriously, I thought I was alone in a small town but I was surprised after training for a couple months.  You can easily learn and train by yourself, you...|||82063 Stuff by Andy Day, not only do I like it because it is the stuff of my passion but that new perspective of our surroundings that it brings. This is a great example of that. All those people...|||Sorry for the quality, my relative only gave me a physical copy, it&#39;s a picture of a picture. This is my INFP girlfriend of two years and me.|||If I am with my SO I almost need physical contact in some way.|||I pretty much have a guru dream that involves my SP wannabe passion. Around people I am close to I totally put on the gypsy king face, people are just so interesting. Hahaha can&#39;t stop laughing at ...|||I agree this this post very much, I just can&#39;t shake that vibe. To me it feels like you are an INTP who strongly identifies with INFJs. I think if you want a sound answer form yourself and others we...|||I do pretty well in emergencies, I do very well compared to normal conditions in my opinion. I feel like I become the ideal version of myself, for the most part. It&#39;s hard to describe but it&#39;s like...|||I have a very close INTJ friend. The Te Fe difference is acknowledged very well and I&#39;d say that both of our tertiary functions are well developed which helps a ton. He does not show it often but he...|||Being alone and/or doing something physical that I can naturally and reactively do without thinking or little thought.|||Pretty much this|||If I wear shoes or socks to bed and my feet are not on my bed I will wake up as if I was falling. 2/3 of the time this happens. Any other dreams that I remember(I don&#39;t remember most of my dreams...|||This one still gets me.  What I meant to say was Pass the salt but what I really said was You b****, you ruined my life|||I&#39;m sorry, but I find them so funny because I use them for good reason. They make people uncomfortable at first but then, slowly, make people more comfortable with the idea that people are different...|||XSFJ Mother, ISTJ father, and an XNFJ sister. Yep.|||I love dark jokes, especially racists/stereotypical jokes.&#39;
------------------------------------
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Preview of the data in the csv file, which has two columns: 
# (1)type - label of the post (2)posts - the corresponding post content
training_data.head()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
  <div id="df-c70c6f70-380a-4cf0-9a96-d3d648ee572f">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>type</th>
      <th>posts</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>F</td>
      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>T</td>
      <td>'I'm finding the lack of me in these posts ver...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>T</td>
      <td>'Good one  _____   https://www.youtube.com/wat...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>T</td>
      <td>'Dear INTP,   I enjoyed our conversation the o...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>T</td>
      <td>'You're fired.|||That's another silly misconce...</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-c70c6f70-380a-4cf0-9a96-d3d648ee572f')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-c70c6f70-380a-4cf0-9a96-d3d648ee572f button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-c70c6f70-380a-4cf0-9a96-d3d648ee572f');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  </div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Extract the labels and posts and store into List

# Get the list of training data (posts)
training_posts=training_data[&#39;posts&#39;].tolist()
# Get the list of corresponding labels for the training data (posts)
training_labels=training_data[&#39;type&#39;].tolist()

# Get the list of testing data (posts)
testing_posts=testing_data[&#39;posts&#39;].tolist()
# Get the list of corresponding labels for the testing data (posts)
testing_labels=testing_data[&#39;type&#39;].tolist()
</pre></div>
</div>
</div>
</div>
</section>
<section id="url-removal">
<h2>1.1. URL Removal<a class="headerlink" href="#url-removal" title="Link to this heading">#</a></h2>
<p><em>related to the section 4.2</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Better naming
X_train = training_posts
X_test = testing_posts
y_train = training_labels
y_test = testing_labels

# REmove the URL for both training and testing set
X_train_no_url = Prep.url_removal(X_train)
X_test_no_url = Prep.url_removal(X_test)
</pre></div>
</div>
</div>
</div>
</section>
<section id="preprocess-data-e-g-stop-words-stemming">
<h2>1.2. Preprocess data (e.g. Stop words, Stemming)<a class="headerlink" href="#preprocess-data-e-g-stop-words-stemming" title="Link to this heading">#</a></h2>
<p><em>related to the section 4.2</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># 2 mins+
# The best one
X_train_best = Prep.pre_processing(X_train) # 15mins+ -&gt; 4 mins after use only english stopwords -&gt; 2mins30s: after remove stemming and lemm -&gt; 1min : take firs 512 tokens
X_test_best= Prep.pre_processing(X_test) 

word_list, vocab = Emb.build_vocab(X_train_best)
# Encode input 
X_train_encoded = Prep.encode_and_add_padding(X_train_best, vocab)
X_test_encoded = Prep.encode_and_add_padding(X_test_best, vocab)
y_train_encoded = Prep.encode_label(y_train)
y_test_encoded = Prep.encode_label(y_test)

# 💎Uncomment below to load the preprocess data from gdrive instead of real time processing since too long.
# h.download_best_model_gdrive()
# with open(&#39;X_train_number.json&#39;,&#39;r&#39;) as f:
#     X_train_processed_number = json.load(f)[&#39;data&#39;]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="input-representation">
<h1>2 - Input Representation<a class="headerlink" href="#input-representation" title="Link to this heading">#</a></h1>
<section id="word-embedding-construction">
<h2>2.1. Word Embedding Construction<a class="headerlink" href="#word-embedding-construction" title="Link to this heading">#</a></h2>
<p><em>related to the section 4.1 and 4.3</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># 2mins
# 💎 Preprocess data for word embeddings
# use both the training and testing datasets
X_train_embed= X_train_best + X_test_best

# 💎 Build the best self train model - FastText
model_word2vec = Emb.make_self_trained_gensim_model(X_train_embed, dimension_=150, window_=3) # 5min+

emb_table_word2vec, emb_dim_word2vec = Emb.build_concat_embed_table(word_list, [model_word2vec])
</pre></div>
</div>
</div>
</div>
</section>
<section id="pretrained-word-embedding">
<h2>2.2. Pretrained Word Embedding<a class="headerlink" href="#pretrained-word-embedding" title="Link to this heading">#</a></h2>
<p><em>related to the section 4.3</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># 2mins+
# load pretrain twitter-25
model_twitter = Emb.load_pre_train_gensim(which_=1)

# Extract and apply the embedding
# Note it is applied as this embedding is concatenated in 2.3 which is then trained in 3.2 and evaluated in 4.3
emb_table_twitter, emb_dim_twitter = Emb.build_concat_embed_table(word_list, [model_twitter])

# load pretrain wiki-100
model_wiki = Emb.load_pre_train_gensim(which_=2)

# Extract and apply the embedding 
emb_table_wiki, emb_dim_wiki = Emb.build_concat_embed_table(word_list, [model_wiki])
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[==================================================] 100.0% 104.8/104.8MB downloaded
[==================================================] 100.0% 128.1/128.1MB downloaded
</pre></div>
</div>
</div>
</div>
</section>
<section id="input-concatenation">
<h2>2.3. Input Concatenation<a class="headerlink" href="#input-concatenation" title="Link to this heading">#</a></h2>
<p><em>related to the section 4.3</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Best Concatenation pretrain twitter &amp; wiki
emb_table_concat, emb_dim_concat = Emb.build_concat_embed_table(word_list, [model_twitter, model_wiki]) 

# Below is the concatenation for self train + pretrained
# emb_table_concat, emb_dim_concat = Emb.build_concat_embed_table(word_list, [model_word2vec, model_twitter, model_wiki]) 

# To address the OOV issue, the embedding is extracted the word exists in both the self train and pretrained
# Otherwise, it is pad with 0s.
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="model-implementation">
<h1>3 - Model Implementation<a class="headerlink" href="#model-implementation" title="Link to this heading">#</a></h1>
<section id="build-sequence-model-bi-directional-model">
<h2>3.1. Build Sequence Model (Bi-directional model)<a class="headerlink" href="#build-sequence-model-bi-directional-model" title="Link to this heading">#</a></h2>
<p><em>related to the section 4.4</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>class Bi_LSTM_Emb(nn.Module):
    &quot;&quot;&quot;
    This Bi-LSTM model use the weights from the pretrained word embedding as shown in below: data.copy_
    &quot;&quot;&quot;
    def __init__(self, vocab_size, n_hidden, n_class, emb_dim, emb_table):
        super(Bi_LSTM_Emb, self).__init__()
        self.emb = nn.Embedding(vocab_size, emb_dim)
        # [IMPORTANT] Initialize the Embedding layer with the lookup table we created 
        self.emb.weight.data.copy_(torch.from_numpy(emb_table))
        # Optional: set requires_grad = False to make this lookup table untrainable
        self.emb.weight.requires_grad = False

        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)
        self.linear = nn.Linear(n_hidden*2, n_class)
        
    def forward(self, x):
        # Get the embeded tensor
        x = self.emb(x)        
        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.
        # details of the outputs from nn.LSTM can be found from: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html
        # c_n: containing the final cell state for each element in the sequence.
        _, (h_n, c_n) = self.lstm(x)
        # concat the last hidden state from two direction
        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)
        z = self.linear(hidden_out)
        return z
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-sequence-model-bi-directional-model">
<h2>3.2. Train Sequence Model (Bi-directional model)<a class="headerlink" href="#train-sequence-model-bi-directional-model" title="Link to this heading">#</a></h2>
<p><em>related to the section 4.4</em></p>
<p>Note that it will not be marked if you do not display the Training Loss and the Number of Epochs in the Assignment 1 ipynb.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Define best sequence model params
PARAMS = {
    &#39;vocab_size&#39;: len(word_list),
    &#39;n_class&#39;: 2,
    &#39;n_hidden&#39;: 32,
    &#39;lr&#39;: 1e-3, # learning rate 
    &#39;n_epoch&#39;: 250
}

model_eval, val_history = Train.train_sequence_model(X_train_encoded, y_train_encoded, PARAMS, emb_dim_concat, emb_table_concat, test_data=[X_test_encoded, y_test_encoded])
# dropout give 0.6 poor result
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1, loss: 0.72880, train_acc: 0.45
Epoch: 2, loss: 0.68969, train_acc: 0.55
Epoch: 3, loss: 0.71804, train_acc: 0.42
Epoch: 4, loss: 0.69931, train_acc: 0.39
Epoch: 5, loss: 0.68372, train_acc: 0.58
Epoch: 6, loss: 0.69651, train_acc: 0.50
Epoch: 7, loss: 0.67035, train_acc: 0.67
Epoch: 8, loss: 0.73025, train_acc: 0.42
Epoch: 9, loss: 0.70207, train_acc: 0.50
Epoch: 10, loss: 0.70532, train_acc: 0.50
Epoch: 11, loss: 0.65637, train_acc: 0.66
Epoch: 12, loss: 0.69071, train_acc: 0.58
Epoch: 13, loss: 0.64800, train_acc: 0.69
Epoch: 14, loss: 0.69779, train_acc: 0.55
Epoch: 15, loss: 0.71276, train_acc: 0.52
Epoch: 16, loss: 0.66253, train_acc: 0.62
Epoch: 17, loss: 0.67896, train_acc: 0.62
Epoch: 18, loss: 0.69338, train_acc: 0.52
Epoch: 19, loss: 0.66535, train_acc: 0.62
Epoch: 20, loss: 0.70982, train_acc: 0.45
Epoch: 21, loss: 0.71053, train_acc: 0.52
Epoch: 22, loss: 0.70706, train_acc: 0.47
Epoch: 23, loss: 0.67143, train_acc: 0.59
Epoch: 24, loss: 0.65034, train_acc: 0.69
Epoch: 25, loss: 0.67799, train_acc: 0.56
Epoch: 26, loss: 0.72632, train_acc: 0.41
Epoch: 27, loss: 0.69184, train_acc: 0.53
Epoch: 28, loss: 0.68409, train_acc: 0.56
Epoch: 29, loss: 0.70221, train_acc: 0.45
Epoch: 30, loss: 0.67709, train_acc: 0.64
Epoch: 31, loss: 0.69472, train_acc: 0.53
Epoch: 32, loss: 0.68859, train_acc: 0.53
Epoch: 33, loss: 0.68916, train_acc: 0.50
Epoch: 34, loss: 0.68852, train_acc: 0.58
Epoch: 35, loss: 0.68335, train_acc: 0.61
Epoch: 36, loss: 0.68438, train_acc: 0.59
Epoch: 37, loss: 0.68491, train_acc: 0.58
Epoch: 38, loss: 0.68740, train_acc: 0.58
Epoch: 39, loss: 0.68970, train_acc: 0.53
Epoch: 40, loss: 0.69682, train_acc: 0.45
Epoch: 41, loss: 0.68481, train_acc: 0.61
Epoch: 42, loss: 0.68089, train_acc: 0.61
Epoch: 43, loss: 0.69017, train_acc: 0.59
Epoch: 44, loss: 0.68831, train_acc: 0.53
Epoch: 45, loss: 0.68715, train_acc: 0.45
Epoch: 46, loss: 0.67165, train_acc: 0.67
Epoch: 47, loss: 0.67805, train_acc: 0.59
Epoch: 48, loss: 0.69558, train_acc: 0.50
Epoch: 49, loss: 0.67744, train_acc: 0.56
Epoch: 50, loss: 0.68508, train_acc: 0.53
Epoch: 51, loss: 0.69412, train_acc: 0.45
Epoch: 52, loss: 0.68798, train_acc: 0.53
Epoch: 53, loss: 0.66841, train_acc: 0.64
Epoch: 54, loss: 0.68946, train_acc: 0.52
Epoch: 55, loss: 0.69977, train_acc: 0.47
Epoch: 56, loss: 0.72027, train_acc: 0.47
Epoch: 57, loss: 0.68787, train_acc: 0.52
Epoch: 58, loss: 0.68141, train_acc: 0.55
Epoch: 59, loss: 0.68993, train_acc: 0.58
Epoch: 60, loss: 0.66717, train_acc: 0.62
Epoch: 61, loss: 0.68970, train_acc: 0.45
Epoch: 62, loss: 0.69212, train_acc: 0.53
Epoch: 63, loss: 0.67888, train_acc: 0.55
Epoch: 64, loss: 0.68026, train_acc: 0.52
Epoch: 65, loss: 0.66219, train_acc: 0.72
Epoch: 66, loss: 0.70327, train_acc: 0.39
Epoch: 67, loss: 0.67479, train_acc: 0.59
Epoch: 68, loss: 0.68111, train_acc: 0.59
Epoch: 69, loss: 0.68708, train_acc: 0.56
Epoch: 70, loss: 0.68283, train_acc: 0.59
Epoch: 71, loss: 0.66753, train_acc: 0.64
Epoch: 72, loss: 0.69108, train_acc: 0.53
Epoch: 73, loss: 0.67624, train_acc: 0.62
Epoch: 74, loss: 0.67380, train_acc: 0.59
Epoch: 75, loss: 0.68352, train_acc: 0.56
Epoch: 76, loss: 0.69304, train_acc: 0.52
Epoch: 77, loss: 0.67913, train_acc: 0.58
Epoch: 78, loss: 0.70486, train_acc: 0.39
Epoch: 79, loss: 0.68238, train_acc: 0.58
Epoch: 80, loss: 0.67106, train_acc: 0.62
Epoch: 81, loss: 0.66899, train_acc: 0.64
Epoch: 82, loss: 0.67540, train_acc: 0.61
Epoch: 83, loss: 0.64932, train_acc: 0.67
Epoch: 84, loss: 0.67540, train_acc: 0.59
Epoch: 85, loss: 0.68331, train_acc: 0.55
Epoch: 86, loss: 0.68140, train_acc: 0.59
Epoch: 87, loss: 0.65818, train_acc: 0.62
Epoch: 88, loss: 0.65803, train_acc: 0.61
Epoch: 89, loss: 0.68274, train_acc: 0.53
Epoch: 90, loss: 0.65861, train_acc: 0.56
Epoch: 91, loss: 0.63702, train_acc: 0.64
Epoch: 92, loss: 0.65337, train_acc: 0.62
Epoch: 93, loss: 0.68744, train_acc: 0.50
Epoch: 94, loss: 0.66810, train_acc: 0.61
Epoch: 95, loss: 0.67296, train_acc: 0.56
Epoch: 96, loss: 0.66790, train_acc: 0.56
Epoch: 97, loss: 0.63115, train_acc: 0.66
Epoch: 98, loss: 0.67776, train_acc: 0.53
Epoch: 99, loss: 0.66676, train_acc: 0.56
Epoch: 100, loss: 0.66058, train_acc: 0.58
Epoch: 101, loss: 0.65182, train_acc: 0.61
Epoch: 102, loss: 0.66751, train_acc: 0.56
Epoch: 103, loss: 0.64848, train_acc: 0.61
Epoch: 104, loss: 0.65897, train_acc: 0.58
Epoch: 105, loss: 0.69033, train_acc: 0.56
Epoch: 106, loss: 0.66561, train_acc: 0.58
Epoch: 107, loss: 0.68354, train_acc: 0.56
Epoch: 108, loss: 0.69341, train_acc: 0.55
Epoch: 109, loss: 0.65446, train_acc: 0.61
Epoch: 110, loss: 0.65870, train_acc: 0.64
Epoch: 111, loss: 0.68442, train_acc: 0.53
Epoch: 112, loss: 0.63514, train_acc: 0.62
Epoch: 113, loss: 0.67721, train_acc: 0.52
Epoch: 114, loss: 0.69076, train_acc: 0.55
Epoch: 115, loss: 0.69089, train_acc: 0.59
Epoch: 116, loss: 0.63042, train_acc: 0.64
Epoch: 117, loss: 0.62311, train_acc: 0.67
Epoch: 118, loss: 0.67252, train_acc: 0.58
Epoch: 119, loss: 0.67428, train_acc: 0.59
Epoch: 120, loss: 0.67606, train_acc: 0.53
Epoch: 121, loss: 0.70212, train_acc: 0.55
Epoch: 122, loss: 0.64510, train_acc: 0.69
Epoch: 123, loss: 0.70854, train_acc: 0.58
Epoch: 124, loss: 0.71131, train_acc: 0.50
Epoch: 125, loss: 0.63990, train_acc: 0.67
Epoch: 126, loss: 0.65303, train_acc: 0.61
Epoch: 127, loss: 0.64462, train_acc: 0.56
Epoch: 128, loss: 0.63462, train_acc: 0.73
Epoch: 129, loss: 0.66310, train_acc: 0.56
Epoch: 130, loss: 0.60972, train_acc: 0.75
Epoch: 131, loss: 0.66658, train_acc: 0.58
Epoch: 132, loss: 0.65860, train_acc: 0.62
Epoch: 133, loss: 0.64643, train_acc: 0.62
Epoch: 134, loss: 0.61493, train_acc: 0.69
Epoch: 135, loss: 0.65780, train_acc: 0.61
Epoch: 136, loss: 0.62209, train_acc: 0.66
Epoch: 137, loss: 0.64578, train_acc: 0.66
Epoch: 138, loss: 0.63392, train_acc: 0.73
Epoch: 139, loss: 0.60288, train_acc: 0.67
Epoch: 140, loss: 0.62510, train_acc: 0.61
Epoch: 141, loss: 0.63981, train_acc: 0.62
Epoch: 142, loss: 0.65158, train_acc: 0.59
Epoch: 143, loss: 0.60841, train_acc: 0.67
Epoch: 144, loss: 0.60714, train_acc: 0.70
Epoch: 145, loss: 0.59772, train_acc: 0.70
Epoch: 146, loss: 0.56079, train_acc: 0.69
Epoch: 147, loss: 0.56101, train_acc: 0.69
Epoch: 148, loss: 0.69210, train_acc: 0.53
Epoch: 149, loss: 0.55292, train_acc: 0.69
Epoch: 150, loss: 0.72478, train_acc: 0.52
Epoch: 151, loss: 0.62389, train_acc: 0.67
Epoch: 152, loss: 0.64197, train_acc: 0.56
Epoch: 153, loss: 0.64345, train_acc: 0.64
Epoch: 154, loss: 0.63635, train_acc: 0.61
Epoch: 155, loss: 0.67547, train_acc: 0.64
Epoch: 156, loss: 0.61874, train_acc: 0.69
Epoch: 157, loss: 0.62959, train_acc: 0.66
Epoch: 158, loss: 0.65143, train_acc: 0.72
Epoch: 159, loss: 0.62030, train_acc: 0.67
Epoch: 160, loss: 0.63204, train_acc: 0.70
Epoch: 161, loss: 0.67315, train_acc: 0.56
Epoch: 162, loss: 0.69654, train_acc: 0.52
Epoch: 163, loss: 0.66523, train_acc: 0.58
Epoch: 164, loss: 0.62529, train_acc: 0.59
Epoch: 165, loss: 0.66593, train_acc: 0.52
Epoch: 166, loss: 0.61960, train_acc: 0.67
Epoch: 167, loss: 0.66273, train_acc: 0.59
Epoch: 168, loss: 0.63960, train_acc: 0.67
Epoch: 169, loss: 0.59990, train_acc: 0.73
Epoch: 170, loss: 0.67795, train_acc: 0.56
Epoch: 171, loss: 0.65175, train_acc: 0.47
Epoch: 172, loss: 0.65383, train_acc: 0.58
Epoch: 173, loss: 0.63681, train_acc: 0.72
Epoch: 174, loss: 0.64704, train_acc: 0.69
Epoch: 175, loss: 0.61254, train_acc: 0.64
Epoch: 176, loss: 0.64633, train_acc: 0.61
Epoch: 177, loss: 0.64688, train_acc: 0.61
Epoch: 178, loss: 0.63242, train_acc: 0.64
Epoch: 179, loss: 0.62932, train_acc: 0.66
Epoch: 180, loss: 0.59415, train_acc: 0.75
Epoch: 181, loss: 0.72029, train_acc: 0.55
Epoch: 182, loss: 0.59124, train_acc: 0.72
Epoch: 183, loss: 0.55698, train_acc: 0.73
Epoch: 184, loss: 0.62658, train_acc: 0.66
Epoch: 185, loss: 0.55122, train_acc: 0.80
Epoch: 186, loss: 0.63266, train_acc: 0.67
Epoch: 187, loss: 0.64340, train_acc: 0.64
Epoch: 188, loss: 0.62189, train_acc: 0.69
Epoch: 189, loss: 0.68467, train_acc: 0.59
Epoch: 190, loss: 0.60776, train_acc: 0.67
Epoch: 191, loss: 0.63616, train_acc: 0.62
Epoch: 192, loss: 0.65918, train_acc: 0.62
Epoch: 193, loss: 0.64863, train_acc: 0.66
Epoch: 194, loss: 0.65934, train_acc: 0.62
Epoch: 195, loss: 0.60676, train_acc: 0.62
Epoch: 196, loss: 0.71262, train_acc: 0.55
Epoch: 197, loss: 0.71657, train_acc: 0.52
Epoch: 198, loss: 0.67196, train_acc: 0.58
Epoch: 199, loss: 0.67525, train_acc: 0.56
Epoch: 200, loss: 0.68847, train_acc: 0.55
Epoch: 201, loss: 0.61188, train_acc: 0.69
Epoch: 202, loss: 0.61397, train_acc: 0.67
Epoch: 203, loss: 0.62681, train_acc: 0.59
Epoch: 204, loss: 0.72129, train_acc: 0.56
Epoch: 205, loss: 0.72388, train_acc: 0.56
Epoch: 206, loss: 0.56958, train_acc: 0.70
Epoch: 207, loss: 0.65051, train_acc: 0.64
Epoch: 208, loss: 0.66935, train_acc: 0.62
Epoch: 209, loss: 0.67628, train_acc: 0.58
Epoch: 210, loss: 0.64608, train_acc: 0.69
Epoch: 211, loss: 0.62052, train_acc: 0.64
Epoch: 212, loss: 0.68947, train_acc: 0.50
Epoch: 213, loss: 0.68163, train_acc: 0.58
Epoch: 214, loss: 0.65261, train_acc: 0.67
Epoch: 215, loss: 0.68551, train_acc: 0.58
Epoch: 216, loss: 0.61836, train_acc: 0.72
Epoch: 217, loss: 0.64999, train_acc: 0.67
Epoch: 218, loss: 0.63876, train_acc: 0.69
Epoch: 219, loss: 0.63720, train_acc: 0.70
Epoch: 220, loss: 0.68033, train_acc: 0.58
Epoch: 221, loss: 0.64662, train_acc: 0.66
Epoch: 222, loss: 0.66086, train_acc: 0.58
Epoch: 223, loss: 0.63562, train_acc: 0.66
Epoch: 224, loss: 0.64000, train_acc: 0.64
Epoch: 225, loss: 0.63015, train_acc: 0.58
Epoch: 226, loss: 0.65848, train_acc: 0.55
Epoch: 227, loss: 0.62636, train_acc: 0.66
Epoch: 228, loss: 0.61165, train_acc: 0.66
Epoch: 229, loss: 0.66929, train_acc: 0.59
Epoch: 230, loss: 0.63789, train_acc: 0.69
Epoch: 231, loss: 0.65141, train_acc: 0.56
Epoch: 232, loss: 0.60014, train_acc: 0.67
Epoch: 233, loss: 0.61863, train_acc: 0.66
Epoch: 234, loss: 0.65078, train_acc: 0.59
Epoch: 235, loss: 0.63005, train_acc: 0.69
Epoch: 236, loss: 0.62589, train_acc: 0.64
Epoch: 237, loss: 0.61588, train_acc: 0.59
Epoch: 238, loss: 0.64900, train_acc: 0.67
Epoch: 239, loss: 0.58109, train_acc: 0.70
Epoch: 240, loss: 0.58100, train_acc: 0.72
Epoch: 241, loss: 0.61628, train_acc: 0.69
Epoch: 242, loss: 0.63850, train_acc: 0.64
Epoch: 243, loss: 0.57982, train_acc: 0.70
Epoch: 244, loss: 0.72113, train_acc: 0.55
Epoch: 245, loss: 0.62411, train_acc: 0.69
Epoch: 246, loss: 0.70003, train_acc: 0.58
Epoch: 247, loss: 0.68028, train_acc: 0.61
Epoch: 248, loss: 0.58521, train_acc: 0.72
Epoch: 249, loss: 0.53862, train_acc: 0.78
Epoch: 250, loss: 0.65668, train_acc: 0.64
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="evaluation">
<h1>4 - Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading">#</a></h1>
<section id="word-embedding-evaluation">
<h2>4.1. Word Embedding Evaluation<a class="headerlink" href="#word-embedding-evaluation" title="Link to this heading">#</a></h2>
<p>You are to apply Semantic-Syntactic word relationship tests for the trained word embeddings and visualise the result of Semantic-Syntactic word relationship tests.
Note that it will not be marked if you do not display it in the ipynb file.</p>
<p>(<em>Please show your empirical evidence and justification</em>)</p>
<p>🎶 <strong>Table</strong></p>
<p>Different feature <strong>dimension</strong> :</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Dim</p></th>
<th class="head"><p>Window Size</p></th>
<th class="head"><p>Sem</p></th>
<th class="head"><p>Syn</p></th>
<th class="head"><p>Tot.</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>100</p></td>
<td><p>5</p></td>
<td><p>12.75%  (38/298)</p></td>
<td><p>48.41%  (990/2045)</p></td>
<td><p>43.88%  (1028/2343)</p></td>
</tr>
<tr class="row-odd"><td><p>150</p></td>
<td><p>5</p></td>
<td><p>12.42%  (37/298)</p></td>
<td><p>50.86%  (1040/2045)</p></td>
<td><p>45.97%  (1077/2343)</p></td>
</tr>
<tr class="row-even"><td><p>200</p></td>
<td><p>5</p></td>
<td><p>8.72%  (26/298)</p></td>
<td><p>51.39%  (1051/2045)</p></td>
<td><p>45.97%  (1077/2343)</p></td>
</tr>
</tbody>
</table>
</div>
<hr class="docutils" />
<p>Different <strong>window size</strong>:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Dim</p></th>
<th class="head"><p>Window Size</p></th>
<th class="head"><p>Sem</p></th>
<th class="head"><p>Syn</p></th>
<th class="head"><p>Tot.</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>150</p></td>
<td><p>3</p></td>
<td><p>10.74%  (32/298)</p></td>
<td><p>55.31%  (1131/2045)</p></td>
<td><p>49.64%  (1163/2343)</p></td>
</tr>
<tr class="row-odd"><td><p>150</p></td>
<td><p>5</p></td>
<td><p>12.42%  (37/298)</p></td>
<td><p>50.86%  (1040/2045)</p></td>
<td><p>45.97%  (1077/2343)</p></td>
</tr>
<tr class="row-even"><td><p>150</p></td>
<td><p>7</p></td>
<td><p>14.77%  (44/298)</p></td>
<td><p>47.33%  (968/2045)</p></td>
<td><p>43.19%  (1012/2343)</p></td>
</tr>
<tr class="row-odd"><td><p>150</p></td>
<td><p>10</p></td>
<td><p>15.44%  (46/298)</p></td>
<td><p>42.25%  (864/2045)</p></td>
<td><p>38.84%  (910/2343)</p></td>
</tr>
</tbody>
</table>
</div>
<p>🎈 Model used: FastText &amp; skipgram - self train gensim</p>
<p>🍔 <strong>1. Observation &amp; Explain the pattern</strong></p>
<p>💎 <strong>1.1 Window size</strong></p>
<p>As seem from above table and below chart:</p>
<ul class="simple">
<li><p>(1) the semantic accuracy goes up as the window size increases while</p></li>
<li><p>(2) the syntactic and overall accuracy goes down.</p></li>
</ul>
<p>(1) is because:</p>
<ul class="simple">
<li><p><strong>Context</strong> of a word is defined by its <strong>surrounding</strong> words</p></li>
<li><p>As the window size goes large, more words are used to be trained for the model to understand the context of that word. Thus, the larger the window size, the higher the semantic accruacy.</p></li>
</ul>
<p>(2) is because:</p>
<ul class="simple">
<li><p>With the increase window size and thus the context words, the model needs to then focus on too many grammar/syntactic meaning at a time which confuse the model and thus lower the syntactic accuracy. Such decrease in syntactic is higher than the increase in semantic one and thus the overal accuracy goes down.</p></li>
</ul>
<p>Below <strong>references</strong> basically means <strong>higher window size, better semantic understnading</strong> for the model.</p>
<blockquote>
<div><p>A window size of 5 is commonly used to capture broad
<strong>topical</strong> content, whereas smaller windows contain
more focused <strong>information</strong> about the target word - Levy, O., &amp; Goldberg, Y. (2014, June)</p>
</div></blockquote>
<blockquote>
<div><p>To <strong>maximize the accuracy</strong> on the phrase analogy task, we increased the amount of the training data by using a dataset with about 33 billion words. We used the hierarchical softmax, dimensionality
of 1000, and the <strong>entire sentence for the context</strong>
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013)</p>
</div></blockquote>
<p>💎 <strong>1.2 Dimension Size</strong></p>
<p>The higher the dimension size, (1) the higher the overal and syntactic accuracy, meanwhile, (2) the lower the semantic arruacy.</p>
<p>(1) is because:</p>
<ul class="simple">
<li><p>There are more feature dimension to represent a word and so the syntactic/grammar understanding will be greater. The overall accuracy is higher because the the increase in syntactic accuacy is higher than the reduce in semantic.</p></li>
</ul>
<p>(2) is because:</p>
<ul class="simple">
<li><p>As the dimension get larger, the model is less capable to find the connection between different words of such much larger feature space. And thus poorer at understanding the context/semantic.</p></li>
</ul>
<p>🍔 <strong>Justify my decision</strong></p>
<p>My final choice for the model is (1) dimension size = 150 and (2) window size = 3.</p>
<p>(1) is because:</p>
<ul class="simple">
<li><p>Dimension size of 150 and 200 has the highest overall accuracy but size 150 is much smaller than 200 and it is less likely to overfit and faster to train.</p></li>
</ul>
<blockquote>
<div><p>“Given two models with the same error on the whole instance space X, choose the simpler one” - Occam’s Razor.</p>
</div></blockquote>
<p>(2) is because:</p>
<ul class="simple">
<li><p>Window size of 3 has the highest accuracy amongs other choice and that the samller the window size, the faster to train and less resource (e.g. ram) required.</p></li>
</ul>
<p><strong>Problem</strong></p>
<p>Note that the current approach fixed all other variables and varise one could miss the correlation between different params, sicne the combo of different params is not tested e.g. via gridsearch, the reference is thus used to ensure the observation is correct.</p>
<p><strong>Reference</strong>:</p>
<p>[1]: Levy, O., &amp; Goldberg, Y. (2014, June). Dependency-based word embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (pp. 302-308).</p>
<p>[2]: Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Use self train gensim word2vec model as instructed, this word2vec one is the best

# Save gensim weight
path = &#39;model_word2vec_eval.txt&#39;
model_word2vec.wv.save_word2vec_format(path, binary=False)

# Performace instrinsic evalution using standford provided ones
Eval.instrinsic_eval(path)

# Recorded data with plot for different window size and dimension comparison
plot_multi_lines(window_size_list, &#39;Window Size&#39;, w_semantic_list, w_syntactic_list, w_overal_list)
plot_multi_lines(dimension_list, &#39;Dimension Size&#39;, d_semantic_list, d_syntactic_list, d_overal_list)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Vocab size:  18464
capital-common-countries.txt:
ACCURACY TOP1: 1.39% (1/72)
capital-world.txt:
ACCURACY TOP1: 0.00% (0/39)
currency.txt:
ACCURACY TOP1: 0.00% (0/10)
city-in-state.txt:
ACCURACY TOP1: 1.23% (1/81)
family.txt:
ACCURACY TOP1: 24.51% (75/306)
gram1-adjective-to-adverb.txt:
ACCURACY TOP1: 73.03% (593/812)
gram2-opposite.txt:
ACCURACY TOP1: 84.98% (430/506)
gram3-comparative.txt:
ACCURACY TOP1: 67.30% (848/1260)
gram4-superlative.txt:
ACCURACY TOP1: 82.34% (578/702)
gram5-present-participle.txt:
ACCURACY TOP1: 74.87% (566/756)
gram6-nationality-adjective.txt:
ACCURACY TOP1: 34.24% (252/736)
gram7-past-tense.txt:
ACCURACY TOP1: 25.31% (284/1122)
gram8-plural.txt:
ACCURACY TOP1: 54.07% (571/1056)
gram9-plural-verbs.txt:
ACCURACY TOP1: 73.52% (372/506)
Questions seen/total: 40.75% (7964/19544)
Semantic accuracy: 15.16%  (77/508)
Syntactic accuracy: 60.27%  (4494/7456)
Total accuracy: 57.40%  (4571/7964)
</pre></div>
</div>
<img alt="../../../_images/8564c22f8cfba559112ee75e7d0101ae7c4da0a37deecbdf63e6677e27a01299.png" src="../../../_images/8564c22f8cfba559112ee75e7d0101ae7c4da0a37deecbdf63e6677e27a01299.png" />
<img alt="../../../_images/cdd5eac52efcc6f560906fdb451d11448d3723f8a87652bd272f7e1fdbba8cf4.png" src="../../../_images/cdd5eac52efcc6f560906fdb451d11448d3723f8a87652bd272f7e1fdbba8cf4.png" />
</div>
</div>
</section>
<section id="performance-evaluation-with-data-processing-techiques">
<h2>4.2. Performance Evaluation with Data Processing Techiques<a class="headerlink" href="#performance-evaluation-with-data-processing-techiques" title="Link to this heading">#</a></h2>
<p>You are required to evaluate with the testing dataset and provide the table with f1 of test set.
Note that it will not be marked if you do not display it in the ipynb file.</p>
<p>(<em>Please show your empirical evidence and justification</em>)</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>F1</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Bi-LSTM With URL</p></td>
<td><p>0.3771, precision:  0 for class 1, division by 0 issue</p></td>
</tr>
<tr class="row-odd"><td><p>Bi-LSTM Without URL</p></td>
<td><p>0.3792, precision:  0.5 for class 1</p></td>
</tr>
<tr class="row-even"><td><p>Bi-LSTM Without url, puncation, stopwords, Lemmatisation, lowercase,stemming, number</p></td>
<td><p>0.440213</p></td>
</tr>
<tr class="row-odd"><td><p>Bi-LSTM Without url, puncation, stopwords, Lemmatisation, lowercase,stemming</p></td>
<td><p>0.489996</p></td>
</tr>
<tr class="row-even"><td><p>Bi-LSTM Without url, puncation, stopwords(english), max len=128 (best model)</p></td>
<td><p>0.52864</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Note that</strong> the last row is trained with 250 epoch while others are 10.</p>
<p>🍔 <strong>Explain the pattern</strong></p>
<p>Based on the above table, (1) dataset without url is slightly better while (2) the preprocessing combination with the number is better that without it.</p>
<p>(1) is because:</p>
<ul class="simple">
<li><p>It could be that url is not a valuable information to determine whether a person is a thinker or feeler and that given it is a small dataset, the url may not be processed by the model effectively to highlights its importance.</p></li>
</ul>
<p>(2) is because:</p>
<ul class="simple">
<li><p>This is explainable as number could be a key factor of the personality type thinker whereas a feeler may use more adjective instead of number to address their comments.</p></li>
</ul>
<p>🍔 <strong>Justify my decision</strong></p>
<p>For the final decision on pre processing, I choose the one <strong>Without url, puncation, stopwords(english)</strong> as it gives the highest f1 score. It is because:</p>
<ul class="simple">
<li><p><strong>Lemmatisation and stemming not used</strong> not used because it is important to make the vocab as close as possible to the pretrain embedding to maximise the use of it. Both pretrained used are glove and the preprocessing for glove do not involved Lemmatisation and stemming.</p></li>
<li><p><strong>Remove URL</strong> because the result difference is minimal with or without URL and URL could introduce a big over head in creatign the vocab and the later training process.</p></li>
<li><p><strong>Remove english stopwords</strong> because it does not help decide personality.</p></li>
</ul>
<p>Nan / none value</p>
<ul class="simple">
<li><p>Min len in training is 57 so nothing is done here.</p></li>
</ul>
<p>Length</p>
<ul class="simple">
<li><p>Note that the length of sentence is cut off at 512 for training the word embedding and 128 for the model because the embedding need large context words to ensure a better semantic understanding and 128 because I want to train the result fast to make more comparison.</p></li>
</ul>
<p>Some improvement could be:</p>
<ul class="simple">
<li><p>instead of remove the url, count the number of it and add it as a feature.</p></li>
<li><p>Before lower case, count number of upper case and add it to features.</p></li>
<li><p>add occurance of punction clusters as feature since a feeler may use more punctions</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Best Preprocessing combinations 
# ONE test case
p2 = {
    &#39;vocab_size&#39;: len(word_list),
    &#39;n_class&#39;: 2,
    &#39;n_hidden&#39;: 32,
    &#39;lr&#39;: 1e-3, # learning rate 
    &#39;n_epoch&#39;: 20
}
model_preprocessed, _ = Train.train_sequence_model(X_train_encoded, y_train_encoded, p2, emb_dim_concat, emb_table_concat, test_data=[X_test_encoded, y_test_encoded])
print(&#39;f1 score: &#39;, Eval.eval_f1_score(model_preprocessed, X_test_encoded, y_test_encoded))

# Because it is a control and treatment group, the model used here is not the best one which is ok based on Ed 


# URL section 
# # Encode input 
# X_train_no_url_encoded = Prep.encode_and_add_padding(X_train_no_url, vocab)
# X_test_no_url_encoded = Prep.encode_and_add_padding(X_test_no_url, vocab)

# model_no_url = Train.train_sequence_model(X_train_no_url_encoded, y_train_encoded, PARAMS, emb_dim_concat, emb_table_concat)
# print()
# Eval.eval_f1_score(model_no_url, X_test_no_url_encoded, y_test_encoded, is_report=True)
# model_preprocessed = Train.train_sequence_model(X_train_encoded, y_train_encoded, PARAMS, emb_dim_concat, emb_table_concat)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1, loss: 0.70424, train_acc: 0.44
Epoch: 2, loss: 0.69765, train_acc: 0.50
Epoch: 3, loss: 0.68138, train_acc: 0.56
Epoch: 4, loss: 0.68906, train_acc: 0.55
Epoch: 5, loss: 0.69850, train_acc: 0.44
Epoch: 6, loss: 0.69582, train_acc: 0.42
Epoch: 7, loss: 0.67972, train_acc: 0.58
Epoch: 8, loss: 0.69424, train_acc: 0.58
Epoch: 9, loss: 0.68024, train_acc: 0.58
Epoch: 10, loss: 0.68372, train_acc: 0.58
Epoch: 11, loss: 0.65952, train_acc: 0.70
Epoch: 12, loss: 0.67881, train_acc: 0.58
Epoch: 13, loss: 0.72361, train_acc: 0.42
Epoch: 14, loss: 0.64051, train_acc: 0.67
Epoch: 15, loss: 0.69414, train_acc: 0.59
Epoch: 16, loss: 0.69932, train_acc: 0.56
Epoch: 17, loss: 0.69692, train_acc: 0.55
Epoch: 18, loss: 0.68037, train_acc: 0.58
Epoch: 19, loss: 0.70640, train_acc: 0.48
Epoch: 20, loss: 0.69266, train_acc: 0.56
f1 score:  0.3966393075271114
</pre></div>
</div>
</div>
</div>
</section>
<section id="performance-evaluation-with-different-input">
<h2>4.3. Performance Evaluation with Different Input<a class="headerlink" href="#performance-evaluation-with-different-input" title="Link to this heading">#</a></h2>
<p>You are required to evaluate with the testing dataset and provide the table with f1 of test set.
Note that it will not be marked if you do not display it in the ipynb file.</p>
<p>(<em>Please show your empirical evidence and justification</em>)</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>F1</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Bi-LSTM with Word2vec (SG)</p></td>
<td><p>0.4446</p></td>
</tr>
<tr class="row-odd"><td><p>Bi-LSTM with fasttext (SG)</p></td>
<td><p>0.3771</p></td>
</tr>
<tr class="row-even"><td><p>Bi-LSTM with glove-twitter-25</p></td>
<td><p>0.3933</p></td>
</tr>
<tr class="row-odd"><td><p>Bi-LSTM with glove-wiki-gigaword-100</p></td>
<td><p>0.3823</p></td>
</tr>
<tr class="row-even"><td><p>Bi-LSTM with + glove-twitter-25 +  glove-wiki-gigaword-100</p></td>
<td><p>0.4790</p></td>
</tr>
<tr class="row-odd"><td><p>Bi-LSTM with Word2vec (SG) + fasttext (SG) + glove-twitter-25 + glove-wiki-gigaword-100</p></td>
<td><p>0.4330</p></td>
</tr>
</tbody>
</table>
</div>
<p>Note that above is tested with 10 epoch.</p>
<p>🎶 <strong>1 Explain the pattern</strong></p>
<p>🦉 <strong>1.1 Observatoin</strong></p>
<p>Based on the above table and the model parameters e.g. 10 epoch only,</p>
<ul class="simple">
<li><p>(1) For <em><strong>self training</strong></em> word embedding, word2vec seems performace better than fasttext by 0.07</p></li>
<li><p>(2) For <em><strong>pretrain</strong></em> embedding, the twitter pretrain embedding better than wiki one by 0.01</p></li>
<li><p>(3) For <em><strong>concatentation</strong></em>, the embedding concatenation of twitter and wiki is better than concatenation with all the self train and pretrain embedding. (0.05+) It is also the best embedding model among other choices.</p></li>
<li><p>(4) <strong>Compare</strong> self train and pretrain, Word2vec self train is better than any other single word embedding model. Other three models perform similarly.</p></li>
<li><p>(5) In general, it seems like <strong>the more</strong> different word embeddings, <strong>the</strong> <strong>better</strong> result.</p></li>
</ul>
<p>🐨 <strong>1.2 Explanation</strong></p>
<p>(1), (4) is because:</p>
<ul class="simple">
<li><p>It could be since this is a <strong>small dataset</strong>, fasttext n-gram approach and the glove may need more data to be effective to capture the fine representation whereas word2vec is simpler and may not need big data to capture the vector space properly.</p></li>
</ul>
<p>(2) is because:</p>
<ul class="simple">
<li><p>Twitter may contain more <strong>emotional words</strong> which is helpful in determining the feeler and thus the thinker. Wiki is more <strong>formal</strong> and so not as effective.</p></li>
</ul>
<p>(3), (5) is because:</p>
<ul class="simple">
<li><p>It is likely that the diversity is more important the quantity for embedding concatenation.This could be why the pretrain concatenation is better than the self train one as pretrain one is from a large corpus with large size and thus diversity.</p></li>
</ul>
<p>🎶 <strong>2 Justify my decision</strong></p>
<p>I choose <strong>Bi-LSTM with + glove-twitter-25 +  glove-wiki-gigaword-100</strong>.</p>
<ul class="simple">
<li><p>Firstly, it gives the highest f1 score compared with others.</p></li>
<li><p>Secondly, there two mebedding are pretrained with large corpus, the model is thus likely to generate better than other options.<br />
These two Glove pretrained embedding are considered over other gensim pretrained provided because they are the smallest available which makes the trainign process faster and that the dataset is small so no need for large embedding.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Load best model for evalution
# One test case
p2 = {
    &#39;vocab_size&#39;: len(word_list),
    &#39;n_class&#39;: 2,
    &#39;n_hidden&#39;: 32,
    &#39;lr&#39;: 1e-3, # learning rate 
    &#39;n_epoch&#39;: 20
}
model_input, _ = Train.train_sequence_model(X_train_encoded, y_train_encoded, p2, emb_dim_concat, emb_table_concat, test_data=[X_test_encoded, y_test_encoded])
print(&#39;f1 score: &#39;, Eval.eval_f1_score(model_input, X_test_encoded, y_test_encoded))


# Other ones
# model_word2vec_eval = Train.train_sequence_model(X_train_encoded, y_train_encoded, PARAMS, emb_dim_word2vec, emb_table_word2vec)
# model_fastext_eval = Train.train_sequence_model(X_train_encoded, y_train_encoded, PARAMS, emb_dim_fastext, emb_table_fastext)
# model_twitter_eval = Train.train_sequence_model(X_train_encoded, y_train_encoded, PARAMS, emb_dim_twitter, emb_table_twitter)
# model_wiki_eval = Train.train_sequence_model(X_train_encoded, y_train_encoded, PARAMS, emb_dim_wiki, emb_table_wiki)
# etc..
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1, loss: 0.70374, train_acc: 0.45
Epoch: 2, loss: 0.69720, train_acc: 0.44
Epoch: 3, loss: 0.71608, train_acc: 0.36
Epoch: 4, loss: 0.72377, train_acc: 0.34
Epoch: 5, loss: 0.69968, train_acc: 0.45
Epoch: 6, loss: 0.68566, train_acc: 0.58
Epoch: 7, loss: 0.68755, train_acc: 0.59
Epoch: 8, loss: 0.69987, train_acc: 0.39
Epoch: 9, loss: 0.68075, train_acc: 0.58
Epoch: 10, loss: 0.68097, train_acc: 0.59
Epoch: 11, loss: 0.69924, train_acc: 0.45
Epoch: 12, loss: 0.68422, train_acc: 0.58
Epoch: 13, loss: 0.69794, train_acc: 0.52
Epoch: 14, loss: 0.69408, train_acc: 0.58
Epoch: 15, loss: 0.67980, train_acc: 0.58
Epoch: 16, loss: 0.70852, train_acc: 0.47
Epoch: 17, loss: 0.68791, train_acc: 0.55
Epoch: 18, loss: 0.70869, train_acc: 0.48
Epoch: 19, loss: 0.70748, train_acc: 0.53
Epoch: 20, loss: 0.67323, train_acc: 0.62
f1 score:  0.3940615472208672
</pre></div>
</div>
</div>
</div>
</section>
<section id="performance-evaluation-with-different-sequence-models">
<h2>4.4. Performance Evaluation with Different Sequence Models<a class="headerlink" href="#performance-evaluation-with-different-sequence-models" title="Link to this heading">#</a></h2>
<p>You are required to evaluate with the testing dataset and provide the table with f1 of test set.
Note that it will not be marked if you do not display it in the ipynb file.</p>
<p>(<em>Please show your empirical evidence and justification</em>)</p>
<p>All below model has the word mebedding with: <code class="docutils literal notranslate"><span class="pre">glove-twitter-25</span> <span class="pre">+</span>&#160; <span class="pre">glove-wiki-gigaword-100</span></code>, 10 epoches (except for the 150 one).</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>F1</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Bi-RNN</p></td>
<td><p>0.5174</p></td>
</tr>
<tr class="row-odd"><td><p>Bi-LSTM</p></td>
<td><p>0.4790</p></td>
</tr>
<tr class="row-even"><td><p>Bi-GRU</p></td>
<td><p>0.3874</p></td>
</tr>
<tr class="row-odd"><td><p>Bi-LSTM (150 epoch)</p></td>
<td><p>0.65</p></td>
</tr>
</tbody>
</table>
</div>
<p>🎶 <strong>1 Explain the pattern</strong></p>
<p>🦉 <strong>1.1 Observatoin</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">Bi-RNN</span> <span class="pre">&gt;</span> <span class="pre">Bi-LSTM</span> <span class="pre">&gt;</span> <span class="pre">Bi-GRU</span></code> : For f1 score from highest to lowerst.</p>
<p>🐨 <strong>1.2 Explanation</strong></p>
<p>Given this is a relatively small dataset, the vanishing / exploding graident for RNN may not occur here. Besides, RNN is a relatively simpler model which is perhaps more suitable for this dataset. LSTM and GRU may be a bit more complex for this small dataset.
Therefore, RNN outperformace the others. GRU performs the poorest, it may because it needs some more fine-tuning which is not done or blow justify the reason.</p>
<blockquote>
<div><p>“The GRU outperformed the LSTM on all tasks with the exception of language modelling” - [1].</p>
</div></blockquote>
<p>[1] An Empirical Exploration of Recurrent Network Architectures, Google (Ilya, Wojciech, Rafal): <a class="reference external" href="http://proceedings.mlr.press/v37/jozefowicz15.pdf">http://proceedings.mlr.press/v37/jozefowicz15.pdf</a></p>
<p>🎈 <strong>2 Justify my decision</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">LSTM</span> <span class="pre">+</span> <span class="pre">glove-twitter-25</span> <span class="pre">+</span>&#160; <span class="pre">glove-wiki-gigaword-100</span></code> is used for the final model choice.<br />
It is because LSTM can handle longer sequence better than RNN and that its performance is not far from RNN given that the epoch used here is <code class="docutils literal notranslate"><span class="pre">10</span></code> only and no parameters tuning is added yet. GRU is not considered given its low f1 score.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># One test case
p2 = {
    &#39;vocab_size&#39;: len(word_list),
    &#39;n_class&#39;: 2,
    &#39;n_hidden&#39;: 32,
    &#39;lr&#39;: 1e-3, # learning rate 
    &#39;n_epoch&#39;: 20
}
model_seq, _ = Train.train_sequence_model(X_train_encoded, y_train_encoded, p2, emb_dim_concat, emb_table_concat, test_data=[X_test_encoded, y_test_encoded])
print(&#39;f1 score: &#39;, Eval.eval_f1_score(model_seq, X_test_encoded, y_test_encoded))


# IMPORTANT: Below result is based on the best model, the table result is determined by a non-best one.
best_model = h.load_best_model()
print(&#39;best f1 score: &#39;, Eval.eval_f1_score(best_model, X_test_encoded, y_test_encoded))

# Below is score for the other 2 models, RNN &amp; GRU
# GRU
# model_gru_eval = Train.train_sequence_model(X_train_encoded, y_train_encoded, PARAMS, emb_dim_concat, emb_table_concat, which_=2)
# Eval.eval_f1_score(model_gru_eval, X_test_encoded, y_test_encoded, is_report=True)
# RNN
# model_rnn_eval = Train.train_sequence_model(X_train_encoded, y_train_encoded, PARAMS, emb_dim_concat, emb_table_concat, which_=3)
# Eval.eval_f1_score(model_rnn_eval, X_test_encoded, y_test_encoded, is_report=True)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1, loss: 0.69798, train_acc: 0.45
Epoch: 2, loss: 0.71091, train_acc: 0.44
Epoch: 3, loss: 0.70474, train_acc: 0.47
Epoch: 4, loss: 0.69291, train_acc: 0.52
Epoch: 5, loss: 0.69493, train_acc: 0.53
Epoch: 6, loss: 0.69029, train_acc: 0.56
Epoch: 7, loss: 0.68786, train_acc: 0.61
Epoch: 8, loss: 0.69325, train_acc: 0.53
Epoch: 9, loss: 0.70703, train_acc: 0.45
Epoch: 10, loss: 0.70170, train_acc: 0.47
Epoch: 11, loss: 0.68408, train_acc: 0.62
Epoch: 12, loss: 0.68418, train_acc: 0.53
Epoch: 13, loss: 0.66303, train_acc: 0.67
Epoch: 14, loss: 0.70325, train_acc: 0.48
Epoch: 15, loss: 0.68086, train_acc: 0.56
Epoch: 16, loss: 0.72416, train_acc: 0.44
Epoch: 17, loss: 0.70445, train_acc: 0.47
Epoch: 18, loss: 0.69098, train_acc: 0.50
Epoch: 19, loss: 0.68671, train_acc: 0.55
Epoch: 20, loss: 0.67623, train_acc: 0.59
f1 score:  0.44357715691040633
best f1 score:  0.5559555298167024
</pre></div>
</div>
</div>
</div>
</section>
<section id="hyperparameter-testing">
<h2>4.5. HyperParameter Testing<a class="headerlink" href="#hyperparameter-testing" title="Link to this heading">#</a></h2>
<p><em>You are required to draw a graph(y-axis: f1, x-axis: epoch) for test set and explain the optimal number of epochs based on the learning rate you have already chosen.</em> Note that it will not be marked if you do not display it in the ipynb file.</p>
<p>(<em>Please show your empirical evidence and justification</em>)</p>
<p><strong>README</strong>:</p>
<ul class="simple">
<li><p>Below explanation is <strong>based on the first two charts</strong>.</p></li>
<li><p>Note that the first 2 chart is the result without using batch size.</p></li>
<li><p>The last three using the batch size of 64 (added late).</p></li>
</ul>
<p>🎶 <strong>1 Explain the pattern</strong></p>
<p>🦉 <strong>1.1 Observation</strong></p>
<p>The model learns (improves it f1 socre) quickly during 0 ~ 40 epoch. Then the learning starts to slow down during 41~100. And after 90th epoch, the model starts to converge and little improvement is observed afte 100th epoch. <strong>(1)</strong></p>
<p>The learning process for learing rate (lr) of 1e-3 starts <strong>converging</strong> after 100 epoch with f1 score around 0.63 whereas lr of 1e-4 is <strong>still learning</strong> after 150 epoch with f1 score of 0.52 at the 149th epoch.<br />
<strong>(2)</strong> Given epoch of 150, model with bigger lr converges faster.</p>
<p>🐨 <strong>1.2 Explanation</strong></p>
<p>(1) is because:</p>
<ul class="simple">
<li><p>At the <strong>early</strong> stage, the model quickly forms its representation to understand the relationship between the sequence input and the target type and therefore f1 score improves quickly. During the <strong>middle</strong> stage, such representation is twisted and adjusted with less significant changes on the parameters. At the <strong>late</strong> stage, the represenatation could not be improved due to the <strong>limitation</strong> of:<br />
💎 small sample size / the low complexity of the model / less fine tuned parameters / the inpropriate preprocessing / the less diverse word embedding.</p></li>
</ul>
<p>(2) is because:</p>
<ul class="simple">
<li><p>A bigger lr means a <strong>greater update</strong> in the graident which then changes the weights faster after every epoch of training and thus it will learn &amp; converge faster.</p></li>
</ul>
<p>🎈 <strong>2 Justify my decision</strong></p>
<p>The <strong>optimal epoch</strong> chosen is <strong>150</strong>. It is because the later ones seems to be oscillating with little improve in the validation (on testing set). I think further training may lead to overfitting issue that make the model less generalise. And so early stopping at 100 epoch is applied.</p>
<p><strong>Lr = 1e-3 is chosen</strong>. This is because this lr helps the model to achieve a reasonable f1 score given the small dataset and the simple model. The smaller learning rate is not considered as it converges too slowly. As seen from below, it is at 0.52 f1 score after 150 epoch compared to 0.65. And the smaller one, if <strong>continues this trend</strong> of learning (epoch 130 ~ 150 improve by 0.02), would take about (0.65 - 0.52) / 0.02 * (150-130) = 130 epoch more to reach 0.65 score though this is likely to be a bad estimation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># 💎💎💎 Uncommented Below if use real time data  💎💎💎
# plot_val_history(val_history, title_=&#39;Epoch vs F1 score: Bi-LSTM Learnign rate = 1e-3&#39;)

plot_val_history(lstm_ep_1e_3, title_=&#39;No batch but all training set with epoch 150: Epoch vs F1 score: Bi-LSTM Learnign rate = 1e-3&#39;)
plot_val_history(lstm_ep_1e_4, title_=&#39;No batch but all training set with epoch 150: : Epoch vs F1 score: Bi-LSTM Learnign rate = 1e-4&#39;)
plot_val_history(plot_res_final, title_=&#39;Batch 64 with epoch 250: Epoch vs F1 score: Bi-LSTM Learnign rate = 1e-3&#39;)
plot_val_history(lstm_1e_3_batch_64, title_=&#39;Batch 64 with epoch 500: Epoch vs F1 score: Bi-LSTM Learnign rate = 1e-3&#39;)
plot_val_history(lstm_1e_4_batch_64, title_=&#39;Batch 64with epoch 500: Epoch vs F1 score: Bi-LSTM Learnign rate = 1e-4&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/ebb045c776275b3dc1cc6680dd774dd1f938fc6a3d743c7e730c8c1c61813828.png" src="../../../_images/ebb045c776275b3dc1cc6680dd774dd1f938fc6a3d743c7e730c8c1c61813828.png" />
<img alt="../../../_images/663ce06e4f55e4112f3dcd502e20f9de1f00d46a318db0fe3bdeb3fa99048490.png" src="../../../_images/663ce06e4f55e4112f3dcd502e20f9de1f00d46a318db0fe3bdeb3fa99048490.png" />
<img alt="../../../_images/9522c84b29547cbeca45e9307fd39886415effce813ce7c6dc5cff56fa5aeff9.png" src="../../../_images/9522c84b29547cbeca45e9307fd39886415effce813ce7c6dc5cff56fa5aeff9.png" />
<img alt="../../../_images/02a7beb172d66eb1516def754ec5f1844f2bba16f8c7bdfc2ab1771706065ace.png" src="../../../_images/02a7beb172d66eb1516def754ec5f1844f2bba16f8c7bdfc2ab1771706065ace.png" />
<img alt="../../../_images/5a60f2a3b7fe63b5c07ba47955fb1697b75d7e81584765c30a3ced3163f47504.png" src="../../../_images/5a60f2a3b7fe63b5c07ba47955fb1697b75d7e81584765c30a3ced3163f47504.png" />
</div>
</div>
<p>#5 - Test your model via Colab Form Fields User Interface</p>
<p>You are required to design a user interface so that user can input a textual sentence via the colab form fields user interface to get the personality type classification result from your trained model. <em>You can just modify based on the following Colab Form Fields template</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>#@title Personality Type Prediction

text = &#39;I am thinking logically&#39;  #@param {type: &quot;string&quot;}

from pathlib import Path
import re
import numpy as np
import os
import json
from google.colab import drive

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords as sw
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

import nltk
nltk.download(&#39;punkt&#39;, quiet=True)
nltk.download(&#39;stopwords&#39;, quiet=True)
nltk.download(&#39;wordnet&#39;, quiet=True)

from gensim.models import Word2Vec # TODO: Glove
from gensim.models import FastText

import gensim.downloader as api

from typing import * # type hint

import warnings
warnings.filterwarnings(&quot;ignore&quot;, category=DeprecationWarning) 

%matplotlib inline 
# from IPython.core.interactiveshell import InteractiveShell
# from IPython import get_ipython
# get_ipython().ast_node_interactivity = &#39;all&#39;

CONFIG = {
    &#39;seed&#39;: 23
}

def set_seed(seed=42):
    &#39;&#39;&#39;Sets seed so result unchanged - reproducibility&#39;&#39;&#39;
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    # When running on the CuDNN backend, two further options must be set
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    # Set a fixed value for the hash seed
    os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)
    
set_seed(CONFIG[&#39;seed&#39;])
device = torch.device(&#39;cpu&#39;) # torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

#################################################################
sww = set(sw.words(&#39;english&#39;)) # 179 vs 7110 if without specify english
#################################################################
class Bi_LSTM_Emb(nn.Module):
    def __init__(self, vocab_size_, n_hidden_, n_class_, emb_dim_, emb_table_):
        super(Bi_LSTM_Emb, self).__init__()
        self.emb = nn.Embedding(vocab_size_, emb_dim_)
        # [IMPORTANT] Initialize the Embedding layer with the lookup table we created 
        self.emb.weight.data.copy_(torch.from_numpy(emb_table_))
        # Optional: set requires_grad = False to make this lookup table untrainable
        self.emb.weight.requires_grad = False

        self.lstm = nn.LSTM(emb_dim_, n_hidden_, batch_first =True, bidirectional=True)
        self.linear = nn.Linear(n_hidden_*2, n_class_)

    def forward(self, x):
        # Get the embeded tensor
        x = self.emb(x)        
        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.
        # details of the outputs from nn.LSTM can be found from: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html
        # c_n: containing the final cell state for each element in the sequence.
        _, (h_n, c_n) = self.lstm(x)
        # concat the last hidden state from two direction
        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)
        z = self.linear(hidden_out)
        return z

class Preprocessing:
    def __init__(self):
        self.lEnc = LabelEncoder()
    #################################### 1.1 URL Removal #############################
    &quot;&quot;&quot;
    You are asked to remove the URL from the post. 
    You are asked to compare, by experimental results (in Section 4.2), 
    when you remove the URL from the post versus keeping the URL. 
    Which will you use? (Section 4.2., Justify your decision)
    &quot;&quot;&quot;
    def template4comment(var_: object) -&gt; object:
        &quot;&quot;&quot;summary
        Args:
            var(type): comment
        Returns:
            var(type): comment
        &quot;&quot;&quot;
    def url_removal(self, posts_:List[str]) -&gt; List[str]:
        &quot;&quot;&quot;remove url from given post
        1. use ||| to split different posts by the same user
        2. use re to remove url with http pattern
        Args:
            post_(str): one instance of the training set
        Returns:
            str: with url removed
        &quot;&quot;&quot;
        res = []
        for x in posts_:
            # x = x.replace(&#39;|||&#39;, &#39; &#39;)
            x = re.sub(r&#39;http\S+&#39;, &#39;&#39;, x) #TODO: explain
            res.append(x)
        return res

    #################################### 1.2 Preprocess data: for the trained model #############################
    &quot;&quot;&quot;
    you are asked to pre-process the training set by integrating several 
    tept pre-processing techniques [Lab5] (e.g. tokenisation, removing numbers,
    converting to lowercase, removing stop words, stemming, etc.). 
    You should test and justify the reason why you apply the specific 
    preprocessing techniques based on the test result 
    in section 4.2 (Section 4.2., Justify your decision)
    &quot;&quot;&quot;
    &quot;&quot;&quot;
    #Note:
    Order of pre-processing matters, i.e. 
    stop before stemming otherwise word chanegd and cannot be 
    &quot;&quot;&quot;
    def pre_processing(self, posts_:List[str], is_urls=False, is_stopwords=False, is_number=False, is_encoded=False) -&gt; List[List[str]]:
        &quot;&quot;&quot;pre-process the training set
        Args:
            posts_(List[str]): comment
        Returns:
            var( List[List[str]] ): a outer list contains a list of word pre-processed tokens, where each inner list represents a post
        &quot;&quot;&quot;
        res = []
        for x in posts_:
            # separate different posts from the same user
            x = x.replace(&#39;|||&#39;, &#39; &#39;)

            if not is_urls:
                x = re.sub(r&#39;http\S+&#39;, &#39;&#39;, x) 

            # remove puncation
            x = re.sub(r&#39;[^\w\s]&#39;,&#39;&#39;, x) 
            ############################################################## before tokenisation
            # word tokenisation
            x = word_tokenize(x)

            # remove puncation
            x = [re.sub(r&#39;[^\w\s]&#39;,&#39;&#39;, w) for w in x]

            # remove stop words
            if not is_stopwords:
                
                x = [w for w in x if not w in sww]

            # remove number, note this will not remove string contain integer  
            if not is_number:
                x = [w for w in x if not w.isdigit()]
            ############################################################## lab5

            # Lemmatisation 
            lemmatizer = WordNetLemmatizer()
            x = [lemmatizer.lemmatize(w) for w in x]
            ############################################################## lab5
            # convert the tokens into lowercase
            x = [t.lower() for t in x]
            
            # stemming
            stemmer = PorterStemmer()
            x = [stemmer.stem(w) for w in x]

            # remove empty word
            x = [w for w in x if w]

            # assignment
            res.append(x)

        if is_encoded:
            _, vocab_ = self.build_vocab(res)
            return self.encode_and_add_padding(res, vocab_)
        return res
    #################################### 2.1 Word Embedding Construction #############################

    ############################### 2.1 (1): Preprocess data for word embeddings  #########################
        
    def encode_label(self, labels:List[str]) -&gt; List[str]:
        &quot;&quot;&quot;encode the label from form of str to integer
        Args:
            labels(List[str]): a list of string: either F - Feeler or T - Thinker
        Returns:
            (List[str]): a list of integer labels instead of str labels
        &quot;&quot;&quot;
        
        # encode the labels 
        # Hint: Try to understand the difference between fit_transform and transform
        return self.lEnc.fit_transform(labels)

    #################### Prepare data ####################
    def encode_and_add_padding(self, sentences:List[str], word_index:dict) -&gt; List[List[int]]:
        &quot;&quot;&quot;
        Convert the sentences to the word index that aligns with the lookup table
        Args:
            sentences(List[str]): pre-processed posts
            word_index(dict): mapping between unique word to integer
        Returns:
            sent_encoded(List[List[int]]): a list of list containing integer that correspond to each word in a sentece
        &quot;&quot;&quot;
        
        len_list = [len(s) for s in sentences]
        max_seq_length = 256 #max(len_list) #max length for all input sequence / post
        
        sent_encoded = []

        for sent in sentences:
            # if in then add idx, else integer for the unknown token
            temp_encoded = [word_index[word] if word in word_index else word_index[&#39;[UNKNOWN]&#39;] for word in sent] 
            # if len is less than pre-defined, add pad token until same len
            if len(temp_encoded) &lt; max_seq_length:
                temp_encoded += [word_index[&#39;[PAD]&#39;]] * (max_seq_length - len(temp_encoded)) 
            # exceed. then cutoff
            if len(temp_encoded) &gt; max_seq_length:
                temp_encoded = temp_encoded[:max_seq_length]
            sent_encoded.append(temp_encoded)
        return sent_encoded

Prep_ = Preprocessing()

class Word_embedding:
    def __init__(self):
        pass
    
    ############################### 2.1 (2): Build training model for word embeddings  #########################  
  
    &quot;&quot;&quot;
    Build training model for word embeddings: You are to build a training model for word embeddings. 
    You are required to articulate the hyperparameters [Lab2] you choose (dimension of embeddings and window size) 
    in Section 4.1. Note that any word embeddings model [Lab2] (e.g. word2vec-CBOW, word2vec-Skip gram, fasttext, glove) 
    can be applied. (Section 4.1. and Section 4.3., Justify your decision)
    &quot;&quot;&quot;
    def make_self_trained_gensim_model(self, posts_, dimension_=25, window_=3, which_=1) -&gt; Type[Word2Vec]:
        &quot;&quot;&quot;contruct gensim word2vec model
        Args:
            posts_(List[str]): a list of string: either F - Feeler or T - Thinker
        Returns:
            (gensim.model.Word2Vec): gensim API to create word2vec model
        &quot;&quot;&quot;
        #TODO: params
        models = {
            1: FastText,
            2: Word2Vec
        }
        return models[which_](sentences=posts_, size=dimension_, window=window_, min_count=5, workers=4, sg=1)

    ############################################### 2.2  Pretrained Word Embedding ##################################################
    &quot;&quot;&quot;
    You are asked to extract and apply the pretrained word embedding. 
    Gensim provides several pretrained word embeddings, you can find those in the gensim github. 
    You can select the pretrained word embedding that would be useful for the assignment 1 task, 
    personality type classification.(Section 4.3., Justify your decision)
    &quot;&quot;&quot;

    ######### load gensim pretrain model #########
    &quot;&quot;&quot;
    Pre trained model based on link here: https://github.com/RaRe-Technologies/gensim-data#models
    &quot;&quot;&quot;
    def load_pre_train_gensim(self, which_=1) -&gt; object:
        # load the pretrained embedding
        models = {
            1:&#39;glove-twitter-25&#39;,
            2: &#39;glove-wiki-gigaword-100&#39;
        }

        return api.load(models[which_]) # NOTE: Download an embedding other than glove-twitter-255

    ######### build_vocab #########
    def build_vocab(self, docs_:List[List[str]]) -&gt; dict:
        &quot;&quot;&quot; build vocabulary based on given dataset
        Args:
            docs_: list of documents that contains string of sentence post
        Returns:
            vocab with key as unique word, value as its mapping integer
        &quot;&quot;&quot;
        word_set = set() 
        for sent in docs_:
            for word in sent:
                word_set.add(word)
        word_set.add(&#39;[PAD]&#39;)
        word_set.add(&#39;[UNKNOWN]&#39;)

        word_list = list(word_set) 
        word_list.sort() # just to make sure order persistent

        word_index = {}
        ind = 0
        for word in word_list:
            word_index[word] = ind
            ind += 1
        return word_list, word_index

    ############################################### 2.3 Input Concatenation ##################################################
    def build_concat_embed_table(self, word_list_: list, word_emb_models:List[object]) -&gt; np.array:
        &quot;&quot;&quot; Build the embedding table from the pre-train model.
        Extract those word embedding that exists in the vocab of the current dataset
        Args:
            word_list_(list): a list of unique word in the corpus
        Returns:
            word_emb_models(List[gensim.model]): a list of gensim models
        &quot;&quot;&quot;
        emb_dim = 0
        for m in word_emb_models:
            emb_dim += m.vector_size

        emb_table = []
        
        for i, word in enumerate(word_list_):
            is_existed = False # initially, assume word does not exist in any embedding 
            concat = []
            if all(word in w_emb for w_emb in word_emb_models): # must all in to ensure shape consistent 
                for w_emb in word_emb_models: # concat all
                    if concat == []: # when empty initalise
                        concat = w_emb[word]
                    else:
                        concat = np.concatenate((concat, w_emb[word]), 0)
                emb_table.append(concat)
            else:
                # The pretrained glove twitter does not contain the embeddings for the [PAD] and [UNKNOWN] tokens we defined
                # Here, we just use all 0 for both [PAD] and [UNKNOWN] tokens for simplicity
                #TODO
                emb_table.append(np.array([0]*emb_dim))

        return np.array(emb_table), emb_dim
Emb_ = Word_embedding()


############################################### 3.2. Train Sequence Model (Bi-directional model) ###############################################
class Train_model:
    def __init__(self):
        pass
    def train_sequence_model(self, X_train_, y_train_, params:dict, emb_dim, emb_table, which_=1, test_data=[]):
        &quot;&quot;&quot;train the sequence model&quot;&quot;&quot;
        assert type(which_) == int, &#39;type(which_) == int&#39;
        assert type(test_data) == list, &#39;type(test_data) == list&#39;
        assert type(emb_dim) == int, &#39;type(emb_dim) == int&#39;

        p = params
        vocab_size, n_class, n_hidden, lr_, n_epoch = p[&#39;vocab_size&#39;], p[&#39;n_class&#39;], p[&#39;n_hidden&#39;], p[&#39;lr&#39;], p[&#39;n_epoch&#39;]

        model = None
        # Move the model to GPU
        if which_ == 1:
            model = Bi_LSTM_Emb(vocab_size, n_hidden, n_class, emb_dim, emb_table).to(device)
        elif which_ == 2:
            model = Bi_GRU(vocab_size, n_hidden, n_class, emb_dim, emb_table).to(device)
        elif which_ ==3:
            model = Bi_RNN(vocab_size, n_hidden, n_class, emb_dim, emb_table).to(device)
        # Loss function and optimizer
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=lr_)

        # Preparing input
        input_torch = torch.from_numpy(np.array(X_train_)).to(device)
        targe_torch = torch.from_numpy(np.array(y_train_)).view(-1).to(device)
        
        plot = []

        for epoch in range(n_epoch):  
            
            # Set the flag to training
            model.train()
            
            # forward + backward + optimize
            optimizer.zero_grad()
            outputs = model(input_torch) 
            loss = criterion(outputs, targe_torch)
            loss.backward()
            optimizer.step()
            
            predicted = torch.argmax(outputs, -1)
            acc = -1
            
            acc= accuracy_score(predicted.cpu().numpy(),targe_torch.cpu().numpy())
            print(&#39;Epoch: %d, loss: %.5f, train_acc: %.2f&#39; %(epoch + 1, loss.item(), acc))
            eval_tmp = Instrinsic_eval()
            if test_data != [] and (epoch % 5 == 0 or epoch == n_epoch - 1):
                f1 = eval_tmp.eval_f1_score(model, test_data[0], test_data[1])
                plot.append([epoch, f1])
        if test_data != []:
            return model, plot
        return model
        
Train_ = Train_model()

########################################################################################################################
############################################### Full run logic #########################################################
########################################################################################################################

def run_pipeline():
    
    X_train_ori = training_posts
    y_train_ori = training_labels

    X_train_ = Prep_.pre_processing(X_train_ori)
    #TODO: donwload
    X_train_encoded_ = Prep_.encode_and_add_padding(X_train_, vocab)
    y_train_encoded_ = Prep_.encode_label(y_train_ori)

    word_list_, vocab_ = Emb_.build_vocab(X_train_)

    PARAMS_ = {
            &#39;vocab_size&#39;: 69071, # 69071, # len(word_list)
            &#39;n_class&#39;: 2,
            &#39;n_hidden&#39;: 32,
            &#39;lr&#39;: 1e-3, # learning rate 
            &#39;n_epoch&#39;: 150
        }

    model_twitter_ = Emb_.load_pre_train_gensim(which_=1)
    model_gigaword_ = Emb_.load_pre_train_gensim(which_=2)

    emb_table_concat_, emb_dim_concat_ = Emb_.build_concat_embed_table(word_list_, [model_twitter_, model_gigaword_]) 

    return Train_.train_sequence_model(X_train_encoded_, y_train_encoded_, PARAMS_, emb_dim_concat_, emb_table_concat_) 

########################################################################################################################
############################################### Load downloaded torch model ############################################
########################################################################################################################

def load_local_torch_model(model_, path_:str):
    
    model_.load_state_dict(torch.load(path_))
    return model_

def load_local_numpy_txt(path_:str):
        
    return np.loadtxt(path_, dtype=np.float64)

def download_best_model_gdrive():
    # Code to download file into Colaboratory:
    !pip install -U -q PyDrive
    from pydrive.auth import GoogleAuth
    from pydrive.drive import GoogleDrive
    from google.colab import auth
    from oauth2client.client import GoogleCredentials
    # Authenticate and create the PyDrive client.
    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)


    id = &#39;16g474hdNsaNx0_SnoKuqj2BuwSEGdnbt&#39;
    downloaded = drive.CreateFile({&#39;id&#39;:id}) 
    downloaded.GetContentFile(&#39;training_data.csv&#39;)  

    id = &#39;1-7hj0sF3Rc5G6POKdkpbDXm_Q6BWFDPU&#39;
    downloaded = drive.CreateFile({&#39;id&#39;:id}) 
    downloaded.GetContentFile(&#39;testing_data.csv&#39;)  

    import pandas as pd
    training_data = pd.read_csv(&quot;/content/training_data.csv&quot;)
    testing_data = pd.read_csv(&quot;/content/testing_data.csv&quot;)


    # Extract the labels and posts and store into List

    # Get the list of training data (posts)
    training_posts=training_data[&#39;posts&#39;].tolist()
    # Get the list of corresponding labels for the training data (posts)
    training_labels=training_data[&#39;type&#39;].tolist()

    # Get the list of testing data (posts)
    testing_posts=testing_data[&#39;posts&#39;].tolist()
    # Get the list of corresponding labels for the testing data (posts)
    testing_labels=testing_data[&#39;type&#39;].tolist()

    ############################################################################################################

    id = &#39;15w8MO5FjQAxt9chIOEWEBWiXktA5xOjz&#39;
    downloaded = drive.CreateFile({&#39;id&#39;:id}) 
    downloaded.GetContentFile(best_model_path) 

    id = &#39;1-CKuXmWSb75rejxAWpaHZE7PWQ_YuoiZ&#39;
    downloaded = drive.CreateFile({&#39;id&#39;:id}) 
    downloaded.GetContentFile(best_embed_path)  

    id = &#39;1LDzY7S-Mqbm4xGWdVNvS4BaBS1QZ8waz&#39;
    downloaded = drive.CreateFile({&#39;id&#39;:id}) 
    downloaded.GetContentFile(vocab_path)  
    
def make_prediction(input_):
     
    model_file = Path(best_model_path)
    embed_file = Path(best_embed_path)
    vocab_file = Path(vocab_path)
    
    PARAMS_ = {
        &#39;vocab_size&#39;: 69071, # len(word_list)
        &#39;n_class&#39;: 2,
        &#39;n_hidden&#39;: 32,
        &#39;lr&#39;: 1e-3, # learning rate 
        &#39;n_epoch&#39;: 150
    }
    if not all([model_file.is_file(), embed_file.is_file(), vocab_file.is_file()]): # check if file exists
        
        # download the saved word embedding for the model and the model weight in order to reuse the best model
        download_best_model_gdrive()

    # Load data from json file
    with open(vocab_path,&#39;r&#39;) as f:
        vocab_ = json.load(f)[&#39;data&#39;]
    emb_dim_concat_ = 125 # 25 (twitter) + 100 (wiki)
    emb_table_concat_ = load_local_numpy_txt(best_embed_path)
    model_ = Bi_LSTM_Emb(PARAMS_[&#39;vocab_size&#39;], PARAMS_[&#39;n_hidden&#39;], PARAMS_[&#39;n_class&#39;], emb_dim_concat_, emb_table_concat_)
    model_ = load_local_torch_model(model_, best_model_path)
        
    # 💎 Uncomment below to run the full pipeline from preprocessing to training
    # model_ = run_pipeline()
    
    text_processed = Prep_.pre_processing([input_])
    text_encoded = Prep_.encode_and_add_padding(text_processed, vocab_)

    input_torch = torch.from_numpy(np.array(text_encoded)).to(device)
    outputs = model_(input_torch) 
    predicted = torch.argmax(outputs, -1)
    return predicted.cpu().numpy()

best_model_path = &#39;best_model.pth&#39;
best_embed_path = &#39;best_embed.txt&#39;
vocab_path = &#39;vocab.json&#39;

res1 = make_prediction(text)[0]
res2 = &#39;F&#39; if res1 == 0 else &#39;T&#39;

print(&quot;Predicted Personality Type: &quot;, res2) # transform back from integer to its orginal label
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted Personality Type:  T
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="object-oriented-programming-codes-here">
<h1>Object Oriented Programming codes here<a class="headerlink" href="#object-oriented-programming-codes-here" title="Link to this heading">#</a></h1>
<p><em>You can use multiple code snippets. Just add more if needed</em></p>
<section id="import-set-seed">
<h2>Import &amp; Set seed<a class="headerlink" href="#import-set-seed" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import re
import numpy as np
import os
import json
from google.colab import drive
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords as sw
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

import nltk
nltk.download(&#39;punkt&#39;, quiet=True)
nltk.download(&#39;stopwords&#39;, quiet=True)
nltk.download(&#39;wordnet&#39;, quiet=True)

from gensim.models import Word2Vec # TODO: Glove
from gensim.models import FastText

import gensim.downloader as api

from typing import * # type hint

import warnings
warnings.filterwarnings(&quot;ignore&quot;, category=DeprecationWarning) 

%matplotlib inline 
# from IPython.core.interactiveshell import InteractiveShell
# from IPython import get_ipython
# get_ipython().ast_node_interactivity = &#39;all&#39;

CONFIG = {
    &#39;seed&#39;: 23
}

def set_seed(seed=42):
    &#39;&#39;&#39;Sets seed so result unchanged - reproducibility&#39;&#39;&#39;
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    # When running on the CuDNN backend, two further options must be set
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    # Set a fixed value for the hash seed
    os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)
    
set_seed(CONFIG[&#39;seed&#39;])
device = torch.device(&#39;cpu&#39;) #torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

!git clone --quiet https://github.com/stanfordnlp/GloVe.git # for the instrinic evalution

sww = set(sw.words(&#39;english&#39;)) # 179 vs 7110 if without specify english
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fatal: destination path &#39;GloVe&#39; already exists and is not an empty directory.
</pre></div>
</div>
</div>
</div>
</section>
<section id="class-sequence-model">
<h2>class - sequence model<a class="headerlink" href="#class-sequence-model" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>############################################### 3.1 Build Sequence Model (Bi-directional model) ##################################################

#################### Bi-GRU ####################
class Bi_GRU(nn.Module):
    def __init__(self, vocab_size, n_hidden, n_class, emb_dim, emb_table):
        super(Bi_GRU, self).__init__()
        self.emb = nn.Embedding(vocab_size, emb_dim)
        # [IMPORTANT] Initialize the Embedding layer with the lookup table we created 
        self.emb.weight.data.copy_(torch.from_numpy(emb_table))
        # Optional: set requires_grad = False to make this lookup table untrainable
        self.emb.weight.requires_grad = False

        self.gru = nn.GRU(emb_dim, n_hidden, batch_first =True, bidirectional=True)
        self.linear = nn.Linear(n_hidden*2, n_class)

    def forward(self, x):
        # Get the embeded tensor
        x = self.emb(x)        
        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.
        # details of the outputs from nn.LSTM can be found from: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html
        # c_n: containing the final cell state for each element in the sequence.
        _, h_n = self.gru(x)
        # concat the last hidden state from two direction
        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)
        z = self.linear(hidden_out)
        return z
#################### RNN ####################
class Bi_RNN(nn.Module):
    def __init__(self, vocab_size, n_hidden, n_class, emb_dim, emb_table):
        super(Bi_RNN, self).__init__()
        self.emb = nn.Embedding(vocab_size, emb_dim)
        # [IMPORTANT] Initialize the Embedding layer with the lookup table we created 
        self.emb.weight.data.copy_(torch.from_numpy(emb_table))
        # Optional: set requires_grad = False to make this lookup table untrainable
        self.emb.weight.requires_grad = False

        self.rnn = nn.RNN(emb_dim, n_hidden, batch_first =True, bidirectional=True)
        self.linear = nn.Linear(n_hidden*2, n_class)

    def forward(self, x):
        # Get the embeded tensor
        x = self.emb(x)        
        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.
        # details of the outputs from nn.LSTM can be found from: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html
        # c_n: containing the final cell state for each element in the sequence.
        _, h_n = self.rnn(x)
        # concat the last hidden state from two direction
        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)
        z = self.linear(hidden_out)
        return z
</pre></div>
</div>
</div>
</div>
</section>
<section id="class-preprocessing">
<h2>class - Preprocessing<a class="headerlink" href="#class-preprocessing" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>class Preprocessing:
    def __init__(self):
        self.lEnc = LabelEncoder()
    #################################### 1.1 URL Removal #############################
    &quot;&quot;&quot;
    You are asked to remove the URL from the post. 
    You are asked to compare, by experimental results (in Section 4.2), 
    when you remove the URL from the post versus keeping the URL. 
    Which will you use? (Section 4.2., Justify your decision)
    &quot;&quot;&quot;
    def template4comment(var_: object) -&gt; object:
        &quot;&quot;&quot;summary
        Args:
            var(type): comment
        Returns:
            var(type): comment
        &quot;&quot;&quot;
    def url_removal(self, posts_:List[str]) -&gt; List[str]:
        &quot;&quot;&quot;remove url from given post
        1. use ||| to split different posts by the same user
        2. use re to remove url with http pattern
        Args:
            post_(str): one instance of the training set
        Returns:
            str: with url removed
        &quot;&quot;&quot;
        res = []
        for x in posts_:
            # x = x.replace(&#39;|||&#39;, &#39; &#39;)
            x = re.sub(r&#39;http\S+&#39;, &#39;&#39;, x) #TODO: explain
            res.append(x)
        return res

    #################################### 1.2 Preprocess data: for the trained model #############################
    &quot;&quot;&quot;
    you are asked to pre-process the training set by integrating several 
    tept pre-processing techniques [Lab5] (e.g. tokenisation, removing numbers,
    converting to lowercase, removing stop words, stemming, etc.). 
    You should test and justify the reason why you apply the specific 
    preprocessing techniques based on the test result 
    in section 4.2 (Section 4.2., Justify your decision)
    &quot;&quot;&quot;
    &quot;&quot;&quot;
    #Note:
    Order of pre-processing matters, i.e. 
    stop before stemming otherwise word chanegd and cannot be 
    &quot;&quot;&quot;
    def pre_processing(self, posts_:List[str], is_urls=False, is_stopwords=False, is_number=True, is_lemm=False, is_stem=False) -&gt; List[List[str]]:
        &quot;&quot;&quot;pre-process the training set
        Args:
            posts_(List[str]): comment
        Returns:
            var( List[List[str]] ): a outer list contains a list of word pre-processed tokens, where each inner list represents a post
        &quot;&quot;&quot;
        res = []
        for x in posts_:
            # separate different posts from the same user
            x = x.replace(&#39;|||&#39;, &#39; &#39;)

            if not is_urls:
                x = re.sub(r&#39;http\S+&#39;, &#39;&#39;, x) 

            ############################################################## before tokenisation
            # word tokenisation
            x = word_tokenize(x)
            max_len = 512
            x = x[:max_len]
            # remove puncation
            x = [re.sub(r&#39;[^\w\s]&#39;,&#39;&#39;, w) for w in x]

            # remove stop words
            if not is_stopwords:
                
                x = [w for w in x if not w in sww]

            # remove number, note this will not remove string contain integer  
            if not is_number:
                x = [w for w in x if not w.isdigit()]
            ############################################################## lab5

            # Lemmatisation 
            if is_lemm:
                lemmatizer = WordNetLemmatizer()
                x = [lemmatizer.lemmatize(w) for w in x]
            ############################################################## lab5
            # convert the tokens into lowercase
            x = [t.lower() for t in x]
            
            # stemming
            if is_stem:
                stemmer = PorterStemmer()
                x = [stemmer.stem(w) for w in x]

            # remove empty word
            x = [w for w in x if w]

            # assignment
            res.append(x)

        return res
    #################################### 2.1 Word Embedding Construction #############################

    ############################### 2.1 (1): Preprocess data for word embeddings  #########################
        
    def encode_label(self, labels:List[str]) -&gt; List[str]:
        &quot;&quot;&quot;encode the label from form of str to integer
        Args:
            labels(List[str]): a list of string: either F - Feeler or T - Thinker
        Returns:
            (List[str]): a list of integer labels instead of str labels
        &quot;&quot;&quot;
        
        # encode the labels 
        # Hint: Try to understand the difference between fit_transform and transform
        return self.lEnc.fit_transform(labels)

    #################### Prepare data ####################
    def encode_and_add_padding(self, sentences:List[str], word_index:dict) -&gt; List[List[int]]:
        &quot;&quot;&quot;
        Convert the sentences to the word index that aligns with the lookup table
        Args:
            sentences(List[str]): pre-processed posts
            word_index(dict): mapping between unique word to integer
        Returns:
            sent_encoded(List[List[int]]): a list of list containing integer that correspond to each word in a sentece
        &quot;&quot;&quot;
        
        len_list = [len(s) for s in sentences]
        max_seq_length = 128 #max(len_list) #max length for all input sequence / post
        
        sent_encoded = []

        for sent in sentences:
            # if in then add idx, else integer for the unknown token
            temp_encoded = [word_index[word] if word in word_index else word_index[&#39;[UNKNOWN]&#39;] for word in sent] 
            # if len is less than pre-defined, add pad token until same len
            if len(temp_encoded) &lt; max_seq_length:
                temp_encoded += [word_index[&#39;[PAD]&#39;]] * (max_seq_length - len(temp_encoded)) 
            # exceed. then cutoff
            if len(temp_encoded) &gt; max_seq_length:
                temp_encoded = temp_encoded[:max_seq_length]
            sent_encoded.append(temp_encoded)
        return sent_encoded

Prep = Preprocessing()
</pre></div>
</div>
</div>
</div>
</section>
<section id="class-word-embedding">
<h2>class - Word_embedding<a class="headerlink" href="#class-word-embedding" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>class Word_embedding:
    def __init__(self):
        pass
    
    ############################### 2.1 (2): Build training model for word embeddings  #########################  
  
    &quot;&quot;&quot;
    Build training model for word embeddings: You are to build a training model for word embeddings. 
    You are required to articulate the hyperparameters [Lab2] you choose (dimension of embeddings and window size) 
    in Section 4.1. Note that any word embeddings model [Lab2] (e.g. word2vec-CBOW, word2vec-Skip gram, fasttext, glove) 
    can be applied. (Section 4.1. and Section 4.3., Justify your decision)
    &quot;&quot;&quot;
    def make_self_trained_gensim_model(self, posts_, dimension_=25, window_=3, which_=1) -&gt; Type[Word2Vec]:
        &quot;&quot;&quot;contruct gensim word2vec model
        Args:
            posts_(List[str]): a list of string: either F - Feeler or T - Thinker
        Returns:
            (gensim.model.Word2Vec): gensim API to create word2vec model
        &quot;&quot;&quot;
        #TODO: params
        models = {
            1: FastText,
            2: Word2Vec
        }
        return models[which_](sentences=posts_, size=dimension_, window=window_, min_count=5, workers=4, sg=1)

    ############################################### 2.2  Pretrained Word Embedding ##################################################
    &quot;&quot;&quot;
    You are asked to extract and apply the pretrained word embedding. 
    Gensim provides several pretrained word embeddings, you can find those in the gensim github. 
    You can select the pretrained word embedding that would be useful for the assignment 1 task, 
    personality type classification.(Section 4.3., Justify your decision)
    &quot;&quot;&quot;

    ######### load gensim pretrain model #########
    &quot;&quot;&quot;
    Pre trained model based on link here: https://github.com/RaRe-Technologies/gensim-data#models
    &quot;&quot;&quot;
    def load_pre_train_gensim(self, which_=1) -&gt; object:
        # load the pretrained embedding
        models = {
            1:&#39;glove-twitter-25&#39;,
            2: &#39;glove-wiki-gigaword-100&#39;,
            # 3: &#39;fasttext-wiki-news-subwords-300&#39;
        }

        return api.load(models[which_]) # NOTE: Download an embedding other than glove-twitter-255

    ######### build_vocab #########
    def build_vocab(self, docs_:List[List[str]]) -&gt; dict:
        &quot;&quot;&quot; build vocabulary based on given dataset
        Args:
            docs_: list of documents that contains string of sentence post
        Returns:
            vocab with key as unique word, value as its mapping integer
        &quot;&quot;&quot;
        word_set = set() 
        for sent in docs_:
            for word in sent:
                word_set.add(word)
        word_set.add(&#39;[PAD]&#39;)
        word_set.add(&#39;[UNKNOWN]&#39;)

        word_list = list(word_set) 
        word_list.sort() # just to make sure order persistent

        word_index = {}
        ind = 0
        for word in word_list:
            word_index[word] = ind
            ind += 1
        return word_list, word_index

    ############################################### 2.3 Input Concatenation ##################################################
    def build_concat_embed_table(self, word_list_: list, word_emb_models:List[object]) -&gt; np.array:
        &quot;&quot;&quot; Build the embedding table from the pre-train model.
        Extract those word embedding that exists in the vocab of the current dataset
        Args:
            word_list_(list): a list of unique word in the corpus
        Returns:
            word_emb_models(List[gensim.model]): a list of gensim models
        &quot;&quot;&quot;
        emb_dim = 0
        for m in word_emb_models:
            emb_dim += m.vector_size

        emb_table = []
        
        for i, word in enumerate(word_list_):
            is_existed = False # initially, assume word does not exist in any embedding 
            concat = []
            if all(word in w_emb for w_emb in word_emb_models): # must all in to ensure shape consistent 
                for w_emb in word_emb_models: # concat all
                    if concat == []: # when empty initalise
                        concat = w_emb[word]
                    else:
                        concat = np.concatenate((concat, w_emb[word]), 0)
                emb_table.append(concat)
            else:
                # The pretrained glove twitter does not contain the embeddings for the [PAD] and [UNKNOWN] tokens we defined
                # Here, we just use all 0 for both [PAD] and [UNKNOWN] tokens for simplicity
                #TODO
                emb_table.append(np.array([0]*emb_dim))

        return np.array(emb_table), emb_dim
Emb = Word_embedding()
</pre></div>
</div>
</div>
</div>
</section>
<section id="class-train">
<h2>class - Train<a class="headerlink" href="#class-train" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>############################################### 3.2. Train Sequence Model (Bi-directional model) ###############################################
class Train_model:
    def __init__(self):
        pass
    def train_sequence_model(self, X_train_, y_train_, params:dict, emb_dim, emb_table, which_=1, test_data=[]):
        &quot;&quot;&quot;train the sequence model&quot;&quot;&quot;
        assert type(which_) == int, &#39;type(which_) == int&#39;
        assert type(test_data) == list, &#39;type(test_data) == list&#39;
        assert type(emb_dim) == int, &#39;type(emb_dim) == int&#39;
        p = params
        vocab_size, n_class, n_hidden, lr_, n_epoch = p[&#39;vocab_size&#39;], p[&#39;n_class&#39;], p[&#39;n_hidden&#39;], p[&#39;lr&#39;], p[&#39;n_epoch&#39;]

        model = None
        # Move the model to GPU
        if which_ == 1:
            model = Bi_LSTM_Emb(vocab_size, n_hidden, n_class, emb_dim, emb_table).to(device)
        elif which_ == 2:
            model = Bi_GRU(vocab_size, n_hidden, n_class, emb_dim, emb_table).to(device)
        elif which_ ==3:
            model = Bi_RNN(vocab_size, n_hidden, n_class, emb_dim, emb_table).to(device)

        # Loss function and optimizer
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=lr_)
        
        plot = []
        # Parameters
        params_dataset = {&#39;batch_size&#39;: 64,
                &#39;shuffle&#39;: True,
                &#39;num_workers&#39;: 2}

        # Generators
        X_torch = torch.from_numpy(np.array(X_train_))
        y_torch = torch.from_numpy(np.array(y_train_)).view(-1) # view(-1) because it is pass in as a list
        
        training_set = CustomDataSet(X_torch, y_torch)
        training_generator = torch.utils.data.DataLoader(training_set, **params_dataset)
        
        # Loop over epochs

        for epoch in range(n_epoch):
            # Training
            for local_batch, local_labels in training_generator:
                # Transfer to GPU
                local_batch, local_labels = local_batch.to(device), local_labels.to(device)

            # Model computations
            
            # Set the flag to training
            model.train()
            
            # forward + backward + optimize
            optimizer.zero_grad()
            outputs = model(local_batch) 
            loss = criterion(outputs, local_labels)
            loss.backward()
            optimizer.step()
            
            predicted = torch.argmax(outputs, -1)            
            
            acc= accuracy_score(predicted.cpu().numpy(),local_labels.cpu().numpy())
            print(&#39;Epoch: %d, loss: %.5f, train_acc: %.2f&#39; %(epoch + 1, loss.item(), acc))
            
            if test_data != [] and (epoch % 5 == 0 or epoch == n_epoch - 1):
                eval_tmp = Instrinsic_eval()
                f1 = eval_tmp.eval_f1_score(model, test_data[0], test_data[1])
                plot.append([epoch, f1])

        if test_data != []:
            return model, plot
        return model
        
        #####################################################################################
Train = Train_model()
</pre></div>
</div>
</div>
</div>
</section>
<section id="class-instrinsic-eval">
<h2>class - instrinsic eval<a class="headerlink" href="#class-instrinsic-eval" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> class Instrinsic_eval:
    def __init__(self):
        pass
    
    def eval_f1_score(self, model_, X_test_, y_test_, is_report=False):
        model_.eval()
        input_torch = torch.from_numpy(np.array(X_test_)).to(device)

        outputs = model_(input_torch) 
        predicted = torch.argmax(outputs, -1)
        if is_report:
            print(&#39;F1 score: &#39;, f1_score(y_test_, predicted.cpu().numpy(), average=&#39;weighted&#39;))
            print()
            print(classification_report(y_test_,predicted.cpu().numpy()))
            return
        return f1_score(y_test_, predicted.cpu().numpy(), average=&#39;weighted&#39;)

    ############################################### 4.1. instrinsic evaluation ###############################################

    def instrinsic_eval(self, path:str):
        &quot;&quot;&quot;
        This is for the model that is trained
        The example code is provided here - Word Embedding Intrinsic Evaluation
        ou also are to visualise the result (the example can be found in the Table 2 and Figure 2 from the Original GloVe Paper).
        code: https://colab.research.google.com/drive/1VdNkQpeI6iLPHeTsGe6sdHQFcGyV1Kmi?usp=sharing#scrollTo=n-LHSUrn7IKh
        &quot;&quot;&quot;

        ################# 
        # Download Google Analogy Test Set questions
        

        ################# 
        # The following code will save the trained embedding vectors in a text format.
        &quot;&quot;&quot;
        9: 9 vectors, 4: dimension of 4
        Below is actual file content for the first few lines

        ```txt
        9 4
        word1 0.123 0.134 0.532 0.152
        word2 0.934 0.412 0.532 0.159
        ```

        &quot;&quot;&quot;

        #################
        # Below based on evaluation code in stanfordnlp glove github
        # Basic get embedding for later intrinsic evalution
        vectors_file=path

        with open(vectors_file, &#39;r&#39;) as f:
            vectors = {}
            for line in f.readlines()[1:]: # we only need the embedding vectors starting from the second line 
                vals = line.rstrip().split(&#39; &#39;)
                vectors[vals[0]] = [float(x) for x in vals[1:]]

        #################
        # convert the format to desire mapping
        vocab_words=list(vectors.keys())
        vocab_size = len(vocab_words)
        print(&quot;Vocab size: &quot;,str(vocab_size))

        # create word-&gt;index and index-&gt;word converter
        vocab = {w: idx for idx, w in enumerate(vocab_words)}
        ivocab = {idx: w for idx, w in enumerate(vocab_words)}
        #################
        # create matrix &amp; normalisation

        # create the embedding matrix of shape (vocab_size, dim)
        vector_dim = len(vectors[ivocab[0]])
        W = np.zeros((vocab_size, vector_dim))
        for word, v in vectors.items():
            if word == &#39;&lt;unk&gt;&#39;:
                continue
            W[vocab[word], :] = v
        #################
        # normalize each word vector to unit length
        # Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.
        W_norm = np.zeros(W.shape)
        d = (np.sum(W ** 2, 1) ** (0.5))
        W_norm = (W.T / d).T
        #################

        correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = self.evaluate_vectors(W_norm, vocab, prefix=&#39;GloVe/eval/question-data&#39;)
        print(&#39;Questions seen/total: %.2f%% (%d/%d)&#39; %
            (100 * count_tot / float(full_count), count_tot, full_count))
        print(&#39;Semantic accuracy: %.2f%%  (%i/%i)&#39; %
            (100 * correct_sem / float(count_sem), correct_sem, count_sem))
        print(&#39;Syntactic accuracy: %.2f%%  (%i/%i)&#39; %
            (100 * correct_syn / float(count_syn), correct_syn, count_syn))
        print(&#39;Total accuracy: %.2f%%  (%i/%i)&#39; % (100 * correct_tot / float(count_tot), correct_tot, count_tot))

    def evaluate_vectors(self, W, vocab, prefix=&#39;./eval/question-data/&#39;):
        &quot;&quot;&quot;Evaluate the trained word vectors on a variety of tasks&quot;&quot;&quot;

        filenames = [
            &#39;capital-common-countries.txt&#39;, &#39;capital-world.txt&#39;, &#39;currency.txt&#39;,
            &#39;city-in-state.txt&#39;, &#39;family.txt&#39;, &#39;gram1-adjective-to-adverb.txt&#39;,
            &#39;gram2-opposite.txt&#39;, &#39;gram3-comparative.txt&#39;, &#39;gram4-superlative.txt&#39;,
            &#39;gram5-present-participle.txt&#39;, &#39;gram6-nationality-adjective.txt&#39;,
            &#39;gram7-past-tense.txt&#39;, &#39;gram8-plural.txt&#39;, &#39;gram9-plural-verbs.txt&#39;,
            ]

        # to avoid memory overflow, could be increased/decreased
        # depending on system and vocab size
        split_size = 100

        correct_sem = 0; # count correct semantic questions
        correct_syn = 0; # count correct syntactic questions
        correct_tot = 0 # count correct questions
        count_sem = 0; # count all semantic questions
        count_syn = 0; # count all syntactic questions
        count_tot = 0 # count all questions
        full_count = 0 # count all questions, including those with unknown words

        for i in range(len(filenames)):
            with open(&#39;%s/%s&#39; % (prefix, filenames[i]), &#39;r&#39;) as f:
                full_data = [line.rstrip().split(&#39; &#39;) for line in f]
                full_count += len(full_data)
                data = [x for x in full_data if all(word in vocab for word in x)]

            if len(data) == 0:
                print(&quot;ERROR: no lines of vocab kept for %s !&quot; % filenames[i])
                print(&quot;Example missing line:&quot;, full_data[0])
                continue

            indices = np.array([[vocab[word] for word in row] for row in data])
            ind1, ind2, ind3, ind4 = indices.T

            predictions = np.zeros((len(indices),))
            num_iter = int(np.ceil(len(indices) / float(split_size)))
            for j in range(num_iter):
                subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))

                pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]
                    +  W[ind3[subset], :])

                #cosine similarity if input W has been normalized
                dist = np.dot(W, pred_vec.T)


                for k in range(len(subset)):
                    dist[ind1[subset[k]], k] = -np.Inf
                    dist[ind2[subset[k]], k] = -np.Inf
                    dist[ind3[subset[k]], k] = -np.Inf

                # predicted word index
                predictions[subset] = np.argmax(dist, 0).flatten()

            
            val = (ind4 == predictions) # correct predictions
            count_tot = count_tot + len(ind1)
            correct_tot = correct_tot + sum(val)
            if i &lt; 5:
                count_sem = count_sem + len(ind1)
                correct_sem = correct_sem + sum(val)
            else:
                count_syn = count_syn + len(ind1)
                correct_syn = correct_syn + sum(val)

            print(&quot;%s:&quot; % filenames[i])
            print(&#39;ACCURACY TOP1: %.2f%% (%d/%d)&#39; %
                (np.mean(val) * 100, np.sum(val), len(val)))
            
        return correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count
Eval = Instrinsic_eval()
</pre></div>
</div>
</div>
</div>
</section>
<section id="preprocess-combo-compare">
<h2>4.1 preprocess combo compare<a class="headerlink" href="#preprocess-combo-compare" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt
# Dimension feature
dimension_list = [100, 150, 200]
d_semantic_list = [12.75, 12.42, 8.72]
d_syntactic_list = [48.41, 50.86, 51.39]
d_overal_list = [43.88, 45.97, 45.97]

# Window size 
window_size_list = [3, 5, 7, 10]
w_semantic_list = [10.74, 12.42, 14.77, 15.44]
w_syntactic_list = [55.31, 50.86, 47.33, 42.25]
w_overal_list = [49.64, 45.97, 43.19, 38.84]

def plot_multi_lines(x, x_label, semantic_list_, syntactic_list_, overal_list_):

    labels  = {&#39;Semantic&#39;: semantic_list_, 
               &#39;Syntactic&#39;: syntactic_list_, 
               &#39;Overall&#39;: overal_list_
               }
    
    plt.figure(0, figsize=(10,5))
    
    for k, y in labels.items():
        plt.plot(x, y, marker=&#39;o&#39;)
        plt.text(x[-1], y[-1], f&quot;{k}&quot;) # text(x, y, some_text), where x is last index for each line
        # add text at end of line (10 is the orginal x-axis label which is chanegd later on)
        
    #plt.legend(labels, loc=&#39;upper right&#39;)
    plt.ylabel(&#39;Accuracy [%]&#39;)
    plt.xlabel(x_label)
    # positions = [i for i in range(11)]
    # x_labels = sizes
    # plt.xticks(positions, x_labels)
    
    plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="hyperparams-tuning-data">
<h2>4.5 hyperparams tuning &amp; data<a class="headerlink" href="#hyperparams-tuning-data" title="Link to this heading">#</a></h2>
<p>Contain manually save data, and plot function</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>
# Learning rate = 1e-3
lstm_ep_1e_3 = [[0, 0.39722835039723087],
 [5, 0.4423827587975953],
 [10, 0.42984313547950204],
 [15, 0.4926813580401134],
 [20, 0.4992443784462104],
 [25, 0.5245338268877596],
 [30, 0.5306104496008943],
 [35, 0.571148231826966],
 [40, 0.5858739622895017],
 [45, 0.5957303518771649],
 [50, 0.6020399043944912],
 [55, 0.614119366403292],
 [60, 0.6152170210349143],
 [65, 0.625110054864376],
 [70, 0.6298528726404834],
 [75, 0.6229937133550588],
 [80, 0.6287271067130272],
 [85, 0.627513288903641],
 [90, 0.63708728564391],
 [95, 0.6406995533900709],
 [100, 0.6394967646501325],
 [105, 0.6496326661647691],
 [110, 0.6429373137472711],
 [115, 0.6434909601240305],
 [120, 0.6334229428466362],
 [125, 0.6494375198520522],
 [130, 0.6393020458593476],
 [135, 0.6460219453556341],
 [140, 0.6441044978063931],
 [145, 0.647501483381419],
 [149, 0.6429745321971257]]

# learning rate = 1e-4 

lstm_ep_1e_4 = [[0, 0.4523876403925913],
 [5, 0.41798941072273454],
 [10, 0.41216740632084864],
 [15, 0.40267291445184034],
 [20, 0.40024746705331604],
 [25, 0.4020764254516451],
 [30, 0.41154427747616645],
 [35, 0.4109207172903444],
 [40, 0.4162233872394977],
 [45, 0.41559035667904637],
 [50, 0.42022412401525167],
 [55, 0.42806439409999625],
 [60, 0.4341450946228735],
 [65, 0.44536590124890546],
 [70, 0.4432629830791194],
 [75, 0.44109912579157584],
 [80, 0.4444444470026792],
 [85, 0.44138306405150607],
 [90, 0.4429774411197592],
 [95, 0.45055341441534363],
 [100, 0.4559701893042565],
 [105, 0.4707978418741552],
 [110, 0.478475389974241],
 [115, 0.49025470500383994],
 [120, 0.49206442773521886],
 [125, 0.5004443663505393],
 [130, 0.500165534942555],
 [135, 0.507110374240826],
 [140, 0.5143752540607812],
 [145, 0.5175457304692035],
 [149, 0.5229774691199987]]
# hidden = 32
lstm_1e_3_batch_64 = \
[[0, 0.47919457279822786],
 [5, 0.4999697266630849],
 [10, 0.4437178192414033],
 [15, 0.4060926393619775],
 [20, 0.42763861868516967],
 [25, 0.43123462199570695],
 [30, 0.4352723383568304],
 [35, 0.4275300852110676],
 [40, 0.40717003933262425],
 [45, 0.45241747266283433],
 [50, 0.5476665515163629],
 [55, 0.5432185015283442],
 [60, 0.5206169339788286],
 [65, 0.4913033173933436],
 [70, 0.4860421038899289],
 [75, 0.5014581815322253],
 [80, 0.5666760980828982],
 [85, 0.5708434242983643],
 [90, 0.5322840828415608],
 [95, 0.5022165915094046],
 [100, 0.4844128672411702],
 [105, 0.5573522709090807],
 [110, 0.5067051657120623],
 [115, 0.5501165691958634],
 [120, 0.5108916609826263],
 [125, 0.4970844546969114],
 [130, 0.55402462658386],
 [135, 0.5500895683645591],
 [140, 0.5622822792517499],
 [145, 0.5893549136573506],
 [150, 0.6031532831116099],
 [155, 0.5918066302378698],
 [160, 0.5981896844845174],
 [165, 0.5766750018430263],
 [170, 0.5762285367850134],
 [175, 0.5434906977811584],
 [180, 0.5994304010227302],
 [185, 0.4618307978991708],
 [190, 0.5375042743652012],
 [195, 0.5560940598700497],
 [200, 0.5397791734388953],
 [205, 0.5755286770959477],
 [210, 0.6077266541580119],
 [215, 0.5274067733430385],
 [220, 0.6164755059460085],
 [225, 0.6127152635016919],
 [230, 0.6101742665256591],
 [235, 0.5875667544653554],
 [240, 0.6093183047595814],
 [245, 0.6212028562138994],
 [250, 0.6300481356301687],
 [255, 0.6199111687725465],
 [260, 0.627022272083034],
 [265, 0.6296538958237122],
 [270, 0.628831600086741],
 [275, 0.6277966658762477],
 [280, 0.64487174574721],
 [285, 0.5899821610302591],
 [290, 0.5835005748221509],
 [295, 0.6360361057221893],
 [300, 0.6176269585921745],
 [305, 0.6230845432513964],
 [310, 0.6304356331227516],
 [315, 0.6509565613517861],
 [320, 0.6446956364968478],
 [325, 0.6463484580978437],
 [330, 0.6492676704067548],
 [335, 0.5474612718328334],
 [340, 0.633760598937994],
 [345, 0.6171299893677721],
 [350, 0.6082293663472247],
 [355, 0.6088397542731745],
 [360, 0.6297542348292059],
 [365, 0.639434010664856],
 [370, 0.6346010004083809],
 [375, 0.634195072219936],
 [380, 0.6244746056913296],
 [385, 0.5886745413815772],
 [390, 0.6204238568717076],
 [395, 0.5742709887048895],
 [400, 0.5919894189102074],
 [405, 0.6393414603523793],
 [410, 0.6429767389677897],
 [415, 0.6271375615342775],
 [420, 0.6257825486465812],
 [425, 0.6319611256598014],
 [430, 0.6417103775465354],
 [435, 0.6417406204183367],
 [440, 0.6436901484296242],
 [445, 0.647611226584176],
 [450, 0.6064711179195017],
 [455, 0.6375820113643694],
 [460, 0.6092058460177848],
 [465, 0.6046497883040685],
 [470, 0.6444287254355036],
 [475, 0.6352424770755322],
 [480, 0.6070902217095989],
 [485, 0.6488359442605428],
 [490, 0.6466099817117477],
 [495, 0.6430978962186006],
 [499, 0.6309111880046137]]


lstm_1e_4_batch_64 = \
 [[0, 0.4934724377436604],
 [5, 0.5180258889824689],
 [10, 0.5004939344906022],
 [15, 0.47749057190622457],
 [20, 0.4555492074192456],
 [25, 0.43609022816061427],
 [30, 0.4235088743427147],
 [35, 0.4219576124190175],
 [40, 0.42046912313755896],
 [45, 0.41212690673165714],
 [50, 0.41458071141031466],
 [55, 0.42984313547950204],
 [60, 0.45092799499524405],
 [65, 0.4543528441152135],
 [70, 0.4578802783267923],
 [75, 0.46683641425697037],
 [80, 0.46246552286739256],
 [85, 0.4594424408102081],
 [90, 0.4489174214103962],
 [95, 0.44430802510663275],
 [100, 0.44589701961991596],
 [105, 0.43966930695501494],
 [110, 0.43972365748830844],
 [115, 0.439000631217474],
 [120, 0.4446256933315248],
 [125, 0.44113553936052713],
 [130, 0.4234285242777012],
 [135, 0.42025290761107936],
 [140, 0.42025290761107936],
 [145, 0.4234285242777012],
 [150, 0.43685820610153053],
 [155, 0.4354617884910907],
 [160, 0.43107120798212467],
 [165, 0.4303852967561496],
 [170, 0.43116990515306125],
 [175, 0.43689397899077453],
 [180, 0.4424709604202399],
 [185, 0.44965887051000225],
 [190, 0.44817575436865387],
 [195, 0.45068621666180647],
 [200, 0.4662512425944868],
 [205, 0.4758546632906944],
 [210, 0.48109611665778756],
 [215, 0.48500343192363543],
 [220, 0.4929154063649951],
 [225, 0.4920579542082748],
 [230, 0.5044519741542047],
 [235, 0.5110127760203719],
 [240, 0.5215280367725184],
 [245, 0.5215280367725184],
 [250, 0.5173569791024616],
 [255, 0.5020251502069153],
 [260, 0.4945809077827374],
 [265, 0.47975355128172353],
 [270, 0.46098495281636764],
 [275, 0.456311841163198],
 [280, 0.4494376872300918],
 [285, 0.44869220569242957],
 [290, 0.44743386289639747],
 [295, 0.4546550485205109],
 [300, 0.4469002683571702],
 [305, 0.4521836339271986],
 [310, 0.4558711146634093],
 [315, 0.4597957085130668],
 [320, 0.4597957085130668],
 [325, 0.4716021073303817],
 [330, 0.48574566245316875],
 [335, 0.510779856740659],
 [340, 0.5158002605402944],
 [345, 0.49631489034784587],
 [350, 0.4928422445723484],
 [355, 0.4910337217142292],
 [360, 0.4901146617816677],
 [365, 0.4944740919710569],
 [370, 0.49631489034784587],
 [375, 0.5014827773825111],
 [380, 0.5023257524274365],
 [385, 0.4892758821198595],
 [390, 0.47792436494350476],
 [395, 0.47975355128172353],
 [400, 0.47666754306182707],
 [405, 0.47287996321173603],
 [410, 0.47367020678605437],
 [415, 0.47746272503544634],
 [420, 0.47176525376004025],
 [425, 0.47746272503544634],
 [430, 0.4825562075721428],
 [435, 0.48463707286291613],
 [440, 0.48619277433859254],
 [445, 0.4910337217142292],
 [450, 0.4909533013144025],
 [455, 0.4888209442908248],
 [460, 0.49019231422163395],
 [465, 0.5105100081887007],
 [470, 0.5322840828415608],
 [475, 0.5491301797553828],
 [480, 0.5583154054797947],
 [485, 0.5638753953697754],
 [490, 0.5622822792517499],
 [495, 0.5714936257481199],
 [499, 0.569475332669558]]


plot_res_final = \
[[0, 0.29130924387954726],
 [5, 0.43144516781030867],
 [10, 0.3921541860475242],
 [15, 0.39134791526405616],
 [20, 0.3863003876004812],
 [25, 0.3989829001039234],
 [30, 0.5005761324650939],
 [35, 0.4889974918152105],
 [40, 0.5105283234417715],
 [45, 0.5578370336704936],
 [50, 0.4373743852116897],
 [55, 0.43875034818122455],
 [60, 0.5118128937217583],
 [65, 0.5631041909874451],
 [70, 0.5617070357554786],
 [75, 0.5727985998274531],
 [80, 0.5621413020991778],
 [85, 0.5169467520823869],
 [90, 0.4585416615737298],
 [95, 0.501715153376459],
 [100, 0.5515223998550054],
 [105, 0.5575187875661773],
 [110, 0.5660485658827364],
 [115, 0.5637135701676076],
 [120, 0.5704240042951958],
 [125, 0.5737185885927196],
 [130, 0.5931486457428043],
 [135, 0.5850662295530101],
 [140, 0.6081993242942484],
 [145, 0.5884633919070942],
 [150, 0.6044978902711747],
 [155, 0.5754123715967085],
 [160, 0.5782906316929962],
 [165, 0.5864141060558729],
 [170, 0.5956774458309013],
 [175, 0.6115744832817899],
 [180, 0.5996130154837997],
 [185, 0.6023020337889038],
 [190, 0.6036617389490718],
 [195, 0.4860529441233696],
 [200, 0.5919708939395723],
 [205, 0.5386011918033841],
 [210, 0.5934544278037063],
 [215, 0.5825590764783072],
 [220, 0.6032295271049596],
 [225, 0.5855819991734224],
 [230, 0.5666160783718335],
 [235, 0.6070644311100708],
 [240, 0.6028422360442516],
 [245, 0.5979440107329906],
 [249, 0.5559555298167024]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt

def plot_val_history(plot_data, title_):
    
    x = []
    y = []
    for p in plot_data:
        x.append(p[0])
        y.append(p[1])

    import numpy as np
    import matplotlib.pyplot as plt

    plt.figure(0, figsize=(15,7))
    # create some x data and some integers for the y axis
    # x = np.array([3,5,2,4])
    # y = np.arange(4)

    # plot the data
    plt.plot(x,y, marker=&#39;o&#39;, color=&#39;b&#39;,)

    # # tell matplotlib which yticks to plot 
    # ax.set_yticks([0,1,2,3])

    # # labelling the yticks according to your list
    # ax.set_yticklabels([&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;])
    
   
    
    # for i in range(len(labels)):
    #     plt.plot(raw_time_list[i])
    #     plt.text( 10, raw_time_list[i][-1], f&quot;{labels[i]}&quot;) # text(x, y, some_text), where x is last index for each line
    #     # add text at end of line (10 is the orginal x-axis label which is chanegd later on)
        
    plt.ylabel(&#39;F1 score&#39;)
    plt.xlabel(&#39;Number of epoch&#39;)
    # positions = [i for i in range(11)]
    # x_labels = sizes
    # plt.xticks(positions, x_labels)
    plt.title(title_)
    plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="customdataset">
<h2>CustomDataSet<a class="headerlink" href="#customdataset" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># create custom dataset class
class CustomDataSet(torch.utils.data.Dataset):
    &quot;&quot;&quot;
    References:
    standford: https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel
    &quot;&quot;&quot;
    def __init__(self, text:list, labels:list):
        &quot;&quot;&quot;Initialization&quot;&quot;&quot;
        self.X = text
        self.y = labels

    def __len__(self):
        &quot;&quot;&quot;Denotes the total number of samples&quot;&quot;&quot;
        return len(self.y)

    def __getitem__(self, idx):
        &quot;&quot;&quot;Generates one sample of data&quot;&quot;&quot;
        X = self.X[idx]
        y = self.y[idx]
        
        return X, y
</pre></div>
</div>
</div>
</div>
</section>
<section id="helper">
<h2>Helper<a class="headerlink" href="#helper" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from google.colab import drive
drive.mount(&#39;/content/gdrive&#39;)

class Helper:
    
    def __init__(self):
        self.model_path = &#39;/content/gdrive/MyDrive/comp5046/a1/models/&#39;
        self.data_path = &#39;/content/gdrive/MyDrive/comp5046/a1/preprocessed_data/&#39;

    def save_numpy_txt_grive(self, data_, fname:str):
        path = f&#39;{self.data_path}/{fname}.txt&#39;
        np.savetxt(path, data_, fmt=&#39;%.5f&#39;) # float64, emb_table_concat_tmp[emb_table_concat_tmp!=0] gives most result as 5 float number

    def load_local_numpy_txt(self, fname:str):
        path = f&#39;{fname}.txt&#39;
        return np.loadtxt(path, dtype=np.float64)

    def save_json_grive(self, data_, fname:str) -&gt; None:
        data_dict = {&#39;data&#39;: data_}
        # Save data to json file
        path = f&#39;{self.data_path}/{fname}.json&#39;
        with open(path,&#39;w&#39;) as f:
            json.dump(data_dict,f)

    def load_json_grive(self, fname:str) -&gt; dict:
        # Load data from json file
        path = f&#39;{self.data_path}/{fname}.json&#39;
        with open(path,&#39;r&#39;) as f:
            return json.load(f)[&#39;data&#39;]
    
    &quot;&quot;&quot; load and train model: https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html#save-and-load-the-model
    &quot;&quot;&quot;
    def save_model_weight_gdrive(self, model_, model_name:str):
        
        path = f&#39;{self.model_path}/{model_name}.pth&#39;
        torch.save(model_.state_dict(), path)

    # PicklingError: Can&#39;t pickle &lt;class &#39;__main__.Bi_LSTM_Emb&#39;&gt;: it&#39;s not the same object as __main__.Bi_LSTM_Emb
    # def save_entire_model_gdrive(self, model_, model_name:str):
        
    #     path = f&#39;{self.model_path}/{model_name}.pth&#39;
    #     torch.save(model_, path)
       
    # def load_torch_model_gdrive(self, model_, model_name:str):
        
    #     path = f&#39;{self.model_path}/{model_name}.pth&#39;
    #     model_.load_state_dict(torch.load(path))
    #     return model_

    def load_torch_model(self, model_, model_name:str):
        
        path = f&#39;{self.model_path}/{model_name}.pth&#39;
        model_.load_state_dict(torch.load(path))
        return model_

    def download_best_model_gdrive(self):
        # Code to download file into Colaboratory:
        !pip install -U -q PyDrive
        from pydrive.auth import GoogleAuth
        from pydrive.drive import GoogleDrive
        from google.colab import auth
        from oauth2client.client import GoogleCredentials
        # Authenticate and create the PyDrive client.
        auth.authenticate_user()
        gauth = GoogleAuth()
        gauth.credentials = GoogleCredentials.get_application_default()
        drive = GoogleDrive(gauth) 

        id = &#39;18O4CEbINqkdGtXv8ouafHo7jn_9RJsVY&#39;
        downloaded = drive.CreateFile({&#39;id&#39;:id}) 
        downloaded.GetContentFile(best_model_path) 

        id = &#39;1-9QUH0PSCQgacT9vAkXbHD-CP-yjmGBI&#39;
        downloaded = drive.CreateFile({&#39;id&#39;:id}) 
        downloaded.GetContentFile(best_embed_path)         

    def load_best_model(self):

        model_file = Path(best_model_path)
        embed_file = Path(best_model_path)
        PARAMS_ = {
            &#39;vocab_size&#39;: 72668, # 69071, # len(word_list)
            &#39;n_class&#39;: 2,
            &#39;n_hidden&#39;: 32,
            &#39;lr&#39;: 1e-3, # learning rate 
            &#39;n_epoch&#39;: 150
        }
        if not (model_file.is_file() and embed_file.is_file()): # check if file exists
            
            # download the saved word embedding for the model and the model weight in order to reuse the best model
            self.download_best_model_gdrive()

        emb_dim_concat_ = 125 # 25 (twitter) + 100 (wiki)
        emb_table_concat_ = self.load_local_numpy_txt(best_embed_path)
        model_ = Bi_LSTM_Emb(PARAMS_[&#39;vocab_size&#39;], PARAMS_[&#39;n_hidden&#39;], PARAMS_[&#39;n_class&#39;], emb_dim_concat_, emb_table_concat_)
        model_ = self.load_local_torch_model(model_, best_model_path)
        return model_
    def load_local_numpy_txt(self, path_:str):
        
        return np.loadtxt(path_, dtype=np.float64)
    def load_local_torch_model(self, model_, path_:str):
    
        model_.load_state_dict(torch.load(path_))
        return model_
best_model_path = &#39;model_best_eval.pth&#39;
best_embed_path = &#39;model_best_embed.txt&#39;

h = Helper()
# data = [[&#39;this&#39;,&#39;is&#39;,&#39;a&#39;,&#39;cat&#39;],[&#39;today&#39;,&#39;is&#39;,&#39;a&#39;,&#39;sunny&#39;,&#39;day&#39;]]
# h.save_json(data, fname=&#39;test&#39;)
# h.load_json(data, fname=&#39;test&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(&quot;/content/gdrive&quot;, force_remount=True).
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># h.download_best_model_gdrive()
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># m = Bi_LSTM_Emb(p_tmp[&#39;vocab_size&#39;], p_tmp[&#39;n_hidden&#39;], p_tmp[&#39;n_class&#39;], emb_dim_concat_tmp, emb_table_concat_tmp)
# res_model = h.load_torbch_model(m, &#39;model_best_eval&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># h.save_entire_model_gdrive(model_best_eval, &#39;best_entire_model&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># X_tmp = h.load_json_grive(fname=&#39;X_train_number&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># h.save_model_weight_gdrive(model_eval, &#39;model_best_eval&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># h.save_numpy_txt_grive(emb_table_concat, &#39;model_best_embed&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># a = h.load_local_numpy_txt(&#39;best_embed&#39;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="side-note">
<h2>Side note<a class="headerlink" href="#side-note" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># from transformers import BertTokenizer, BertModel

# bert = BertModel.from_pretrained(&#39;bert-base-uncased&#39;)

# tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)
# encoded_inputs = tokenizer(training_posts, padding=True, 
#                             truncation=True, return_tensors=&quot;pt&quot;)
# ids = encoded_inputs[&#39;input_ids&#39;]
# mask = encoded_inputs[&#39;attention_mask&#39;]
# output = bert(ids, mask)
</pre></div>
</div>
</div>
</div>
<p>I use all as the variable name and realise I also use the all function, 25mins wasted…</p>
<p>Did not use &#64;staticmethod because the class name is a bit long assign a short variable name and call it is better.</p>
<p>should use the same preprocessing as the pretrain word embedding, in this case, below is a translation of ruby to python by someone for standford glove twitter:
<a class="reference external" href="https://www.kaggle.com/code/amackcrane/python-version-of-glove-twitter-preprocess-script/script">https://www.kaggle.com/code/amackcrane/python-version-of-glove-twitter-preprocess-script/script</a></p>
<p>Timing</p>
<section id="eda">
<h3>EDA<a class="headerlink" href="#eda" title="Link to this heading">#</a></h3>
<p>training min file len = 57
testing set min len &gt; 1000</p>
</section>
<section id="profiling">
<h3>profiling<a class="headerlink" href="#profiling" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&#39;&#39;&#39;
def tmp(posts_:List[str]) -&gt; List[List[str]]:
        &quot;&quot;&quot;pre-process the training set
        Args:
            posts_(List[str]): comment
        Returns:
            var( List[List[str]] ): a outer list contains a list of word pre-processed tokens, where each inner list represents a post
        &quot;&quot;&quot;
        res = []
        dict_ = {
            &#39;|||&#39;:0,
            &#39;http&#39;:0,
            &#39;token&#39;:0,
            &#39;puncation&#39;:0,
            &#39;stop&#39;:0,
            &#39;number&#39;:0,
            &#39;Lemmatisation&#39;:0,
            &#39;lowercase&#39;:0,
            &#39;stemming&#39;:0
        }
        for x in posts_:
            # separate different posts from the same user
            start = time.time()
            x = x.replace(&#39;|||&#39;, &#39; &#39;)
            end = time.time()
            dict_[&#39;|||&#39;] += end - start

            start = time.time()

            x = re.sub(r&#39;http\S+&#39;, &#39;&#39;, x) 
            end = time.time()
            dict_[&#39;http&#39;] += end - start
    
            
            ############################################################## before tokenisation
            # word tokenisation
            
            start = time.time()
            # token
            x = word_tokenize(x)
            end = time.time()
            dict_[&#39;token&#39;] += end - start
            
            start = time.time()
            # remove puncation
            x = [re.sub(r&#39;[^\w\s]&#39;,&#39;&#39;, w) for w in x]
            end = time.time()
            dict_[&#39;puncation&#39;] += end - start

            start = time.time()
            # remove stop words
            x = [w for w in x if not w in sww]
            end = time.time()
            dict_[&#39;stop&#39;] += end - start

            # remove number, note this will not remove string contain integer  
            start = time.time()
            x = [w for w in x if not w.isdigit()]
            end = time.time()
            dict_[&#39;number&#39;] += end - start
            

            start = time.time()
            # Lemmatisation 
            lemmatizer = WordNetLemmatizer()
            x = [lemmatizer.lemmatize(w) for w in x]
            end = time.time()
            dict_[&#39;Lemmatisation&#39;] += end - start
            
            
            start = time.time()
            # convert the tokens into lowercase
            x = [t.lower() for t in x]
            end = time.time()
            dict_[&#39;lowercase&#39;] += end - start

            start = time.time()
            # stemming
            stemmer = PorterStemmer()
            x = [stemmer.stem(w) for w in x]
            end = time.time()
            dict_[&#39;stemming&#39;] += end - start
            

            # remove empty word
            x = [w for w in x if w]

            # assignment
            res.append(x)

        return dict_
&#39;&#39;&#39;
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;\ndef tmp(posts_:List[str]) -&gt; List[List[str]]:\n        &quot;&quot;&quot;pre-process the training set\n        Args:\n            posts_(List[str]): comment\n        Returns:\n            var( List[List[str]] ): a outer list contains a list of word pre-processed tokens, where each inner list represents a post\n        &quot;&quot;&quot;\n        res = []\n        dict_ = {\n            \&#39;|||\&#39;:0,\n            \&#39;http\&#39;:0,\n            \&#39;token\&#39;:0,\n            \&#39;puncation\&#39;:0,\n            \&#39;stop\&#39;:0,\n            \&#39;number\&#39;:0,\n            \&#39;Lemmatisation\&#39;:0,\n            \&#39;lowercase\&#39;:0,\n            \&#39;stemming\&#39;:0\n        }\n        for x in posts_:\n            # separate different posts from the same user\n            start = time.time()\n            x = x.replace(\&#39;|||\&#39;, \&#39; \&#39;)\n            end = time.time()\n            dict_[\&#39;|||\&#39;] += end - start\n\n            start = time.time()\n\n            x = re.sub(r\&#39;http\\S+\&#39;, \&#39;\&#39;, x) \n            end = time.time()\n            dict_[\&#39;http\&#39;] += end - start\n    \n            \n            ############################################################## before tokenisation\n            # word tokenisation\n            \n            start = time.time()\n            # token\n            x = word_tokenize(x)\n            end = time.time()\n            dict_[\&#39;token\&#39;] += end - start\n            \n            start = time.time()\n            # remove puncation\n            x = [re.sub(r\&#39;[^\\w\\s]\&#39;,\&#39;\&#39;, w) for w in x]\n            end = time.time()\n            dict_[\&#39;puncation\&#39;] += end - start\n\n            start = time.time()\n            # remove stop words\n            x = [w for w in x if not w in sww]\n            end = time.time()\n            dict_[\&#39;stop\&#39;] += end - start\n\n            # remove number, note this will not remove string contain integer  \n            start = time.time()\n            x = [w for w in x if not w.isdigit()]\n            end = time.time()\n            dict_[\&#39;number\&#39;] += end - start\n            \n\n            start = time.time()\n            # Lemmatisation \n            lemmatizer = WordNetLemmatizer()\n            x = [lemmatizer.lemmatize(w) for w in x]\n            end = time.time()\n            dict_[\&#39;Lemmatisation\&#39;] += end - start\n            \n            \n            start = time.time()\n            # convert the tokens into lowercase\n            x = [t.lower() for t in x]\n            end = time.time()\n            dict_[\&#39;lowercase\&#39;] += end - start\n\n            start = time.time()\n            # stemming\n            stemmer = PorterStemmer()\n            x = [stemmer.stem(w) for w in x]\n            end = time.time()\n            dict_[\&#39;stemming\&#39;] += end - start\n            \n\n            # remove empty word\n            x = [w for w in x if w]\n\n            # assignment\n            res.append(x)\n\n        return dict_\n&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># x = tmp(training_posts[:50])
# sorted(x.items(), key=lambda x: x[1], reverse=True)
# &#39;&#39;&#39;
# [(&#39;stemming&#39;, 1.3638272285461426),
#  (&#39;token&#39;, 0.9907956123352051),
#  (&#39;Lemmatisation&#39;, 0.3365650177001953),
#  (&#39;puncation&#39;, 0.14055538177490234),
#  (&#39;number&#39;, 0.015831470489501953),
#  (&#39;stop&#39;, 0.011178255081176758),
#  (&#39;lowercase&#39;, 0.006746768951416016),
#  (&#39;http&#39;, 0.0018148422241210938),
#  (&#39;|||&#39;, 0.0006151199340820312)]
# &#39;&#39;&#39;
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ds-courses\usyd\5046"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="a2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">In Game Toxicity seq2seq classification</p>
      </div>
    </a>
    <a class="right-next"
       href="../../unsw/9417/9417-intro.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Machine Learning (UNSW)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Binary text classification</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#readme">Readme</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preprocessing">1 - Data Preprocessing</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-collection-do-not-modify-this">1.0. Data Collection [DO NOT MODIFY THIS]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#url-removal">1.1. URL Removal</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocess-data-e-g-stop-words-stemming">1.2. Preprocess data (e.g. Stop words, Stemming)</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#input-representation">2 - Input Representation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embedding-construction">2.1. Word Embedding Construction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pretrained-word-embedding">2.2. Pretrained Word Embedding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#input-concatenation">2.3. Input Concatenation</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#model-implementation">3 - Model Implementation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-sequence-model-bi-directional-model">3.1. Build Sequence Model (Bi-directional model)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-sequence-model-bi-directional-model">3.2. Train Sequence Model (Bi-directional model)</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">4 - Evaluation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embedding-evaluation">4.1. Word Embedding Evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-evaluation-with-data-processing-techiques">4.2. Performance Evaluation with Data Processing Techiques</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-evaluation-with-different-input">4.3. Performance Evaluation with Different Input</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-evaluation-with-different-sequence-models">4.4. Performance Evaluation with Different Sequence Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-testing">4.5. HyperParameter Testing</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#object-oriented-programming-codes-here">Object Oriented Programming codes here</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#import-set-seed">Import &amp; Set seed</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#class-sequence-model">class - sequence model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#class-preprocessing">class - Preprocessing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#class-word-embedding">class - Word_embedding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#class-train">class - Train</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#class-instrinsic-eval">class - instrinsic eval</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocess-combo-compare">4.1 preprocess combo compare</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparams-tuning-data">4.5 hyperparams tuning &amp; data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#customdataset">CustomDataSet</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#helper">Helper</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#side-note">Side note</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eda">EDA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#profiling">profiling</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Jerry
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2021.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>