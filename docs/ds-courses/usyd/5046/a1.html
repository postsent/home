
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Binary text classification &#8212; My Jupter Notebook on data science.</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Cloud Computing (USYD)" href="../5349/5349-intro.html" />
    <link rel="prev" title="In Game Toxicity seq2seq classification" href="a2.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">My Jupter Notebook on data science.</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://postsent.github.io/categories/">
   Back to Blog
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../reference.html">
   Acknowledgement
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data Science Related Courses
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../unsw/9417/9417-intro.html">
   Machine Learning (UNSW)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../unsw/9417/9417-basic.html">
     Basic with examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../unsw/9417/9417-project.html">
     Project - Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../unsw/9417/9417-report/report.html">
     Project - report
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../unsw/9418/9418-intro.html">
   PGM (UNSW)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../unsw/9418/9418-EDA.html">
     EDA on Times Series Dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../unsw/9418/9418-project.html">
     Time series project code
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../unsw/9418/9418-project/report.html">
     Time series report
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../unsw/9517/9517-intro.html">
   Computer Vision (UNSW)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../unsw/9517/9517-a1-code.html">
     Basic image processing, thresholding, count cells
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../unsw/9517/9517-a1/report.html">
     Report on Basic image processing, etc
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../unsw/9517/9517-lane_detection.html">
     Lane detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../unsw/9517/9517-vehicle-detection.html">
     Vehicle detection (by teammate)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../unsw/9444/9444-intro.html">
   Deep Learning (UNSW)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../unsw/9444/9444-project.html">
     Image classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../unsw/9444/assignment1/assignment1.html">
     Characters, Spirals and Hidden Unit Dynamics
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../unsw/3900/3900-intro.html">
   Capstone (UNSW)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../unsw/3900/project/3900-project.html">
     <strong>
      Chatbot
     </strong>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../unsw/3431/3431-intro.html">
   ROS &amp; CV (UNSW)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../unsw/3431/project/project.html">
     Mini self-driving
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="5046-intro.html">
   NLP (USYD)
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="a2/a2.html">
     Report - In-game Toxicity Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="a2.html">
     In Game Toxicity seq2seq classification
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Binary text classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5349/5349-intro.html">
   Cloud Computing (USYD)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5349/a2/a2.html">
     Data Preprocessing and Performance Tuning with Spark
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5349/a2.html">
     Data Preprocessing and Performance Tuning with Spark
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5349/a1.html">
     Text Analysis with Spark RDD API
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5349/a1-report.html">
     Report – Text Analysis with Spark RDD API
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5338/5338-intro.html">
   NoSQL&amp;Graph Data Model(USYD)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5338/a1.html">
     NoSQL basic
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5338/a2.html">
     NoSQL Aggregation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5338/a2-report/report.html">
     <strong>
      Performance Observation Task
     </strong>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5338/a3.html">
     Neo4j Basic
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5338/a4.html">
     Neo4j Query
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../5048/5048-intro.html">
   Visual Analytics Tableau (USYD)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../5048/individual.html">
     Individual Report
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../5048/group.html">
     Group Report (My part)
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../ml/regression/regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../ml/regression/p1-crypto-prediction.html">
     Crypto Prediction
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ml/classification/classification.html">
   Classification (placeholder)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ml/time-series/time-series.html">
   Time Series (placeholder)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../ml/ml.html">
   General
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../dl/dl.html">
   General
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  NLP
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../nlp/nlp.html">
   placeholder
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Computer Vision
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../cv/cv.html">
   Placeholder
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Misc
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../misc/math.html">
   Math
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../misc/misc.html">
   Misc
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../misc/term.html">
   Terminology
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../misc/todo.html">
   TODO
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Coding Basic
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../python/basic-intro.html">
   Numpy, Pandas, Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../python/numpy.html">
     Numpy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../python/pandas.html">
     Pandas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../python/leetcode.html">
     Leetcode
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Side Projects
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../side-project/web-scrapter.html">
   Course Enrolment Scrapter
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unfinished
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../unfinished/pytorch-nlp-bk/nlp-book.html">
   NLP Book
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../unfinished/pytorch-nlp-bk/intro.html">
     Intro
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../unfinished/pytorch-nlp-bk/nn.html">
     Feed-Forward Networks for NLP
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../unfinished/pytorch-nlp-bk/prac-ch3.html">
     Chapter 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../unfinished/jigsaw-intro.html">
   Jigsaw
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../unfinished/jiagsaw-toxic-comment-serverity-rate/README.html">
     Folder Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../unfinished/jiagsaw-toxic-comment-serverity-rate/notebooks/simple-rnn.html">
     Simple
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../unfinished/jiagsaw-toxic-comment-serverity-rate/notebooks/lstm.html">
     Upgrade RNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../unfinished/jiagsaw-toxic-comment-serverity-rate/notebooks/helper.html">
     Common
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../unfinished/nlp-reading.html">
   Book Reading
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/postsent/nb/main?urlpath=tree/docs/ds-courses/usyd/5046/a1.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/postsent/nb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/ds-courses/usyd/5046/a1.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Binary text classification
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#readme">
   Readme
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-preprocessing">
   1 - Data Preprocessing
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-collection-do-not-modify-this">
     1.0. Data Collection [DO NOT MODIFY THIS]
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#url-removal">
     1.1. URL Removal
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preprocess-data-e-g-stop-words-stemming">
     1.2. Preprocess data (e.g. Stop words, Stemming)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#input-representation">
   2 - Input Representation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-embedding-construction">
     2.1. Word Embedding Construction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pretrained-word-embedding">
     2.2. Pretrained Word Embedding
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#input-concatenation">
     2.3. Input Concatenation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-implementation">
   3 - Model Implementation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#build-sequence-model-bi-directional-model">
     3.1. Build Sequence Model (Bi-directional model)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-sequence-model-bi-directional-model">
     3.2. Train Sequence Model (Bi-directional model)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation">
   4 - Evaluation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-embedding-evaluation">
     4.1. Word Embedding Evaluation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-evaluation-with-data-processing-techiques">
     4.2. Performance Evaluation with Data Processing Techiques
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-evaluation-with-different-input">
     4.3. Performance Evaluation with Different Input
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-evaluation-with-different-sequence-models">
     4.4. Performance Evaluation with Different Sequence Models
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparameter-testing">
     4.5. HyperParameter Testing
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#object-oriented-programming-codes-here">
   Object Oriented Programming codes here
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#import-set-seed">
     Import &amp; Set seed
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#class-sequence-model">
     class - sequence model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#class-preprocessing">
     class - Preprocessing
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#class-word-embedding">
     class - Word_embedding
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#class-train">
     class - Train
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#class-instrinsic-eval">
     class - instrinsic eval
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preprocess-combo-compare">
     4.1 preprocess combo compare
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparams-tuning-data">
     4.5 hyperparams tuning &amp; data
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#customdataset">
     CustomDataSet
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#helper">
     Helper
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#side-note">
     Side note
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#eda">
       EDA
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#profiling">
       profiling
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Binary text classification</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Binary text classification
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#readme">
   Readme
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-preprocessing">
   1 - Data Preprocessing
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-collection-do-not-modify-this">
     1.0. Data Collection [DO NOT MODIFY THIS]
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#url-removal">
     1.1. URL Removal
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preprocess-data-e-g-stop-words-stemming">
     1.2. Preprocess data (e.g. Stop words, Stemming)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#input-representation">
   2 - Input Representation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-embedding-construction">
     2.1. Word Embedding Construction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pretrained-word-embedding">
     2.2. Pretrained Word Embedding
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#input-concatenation">
     2.3. Input Concatenation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-implementation">
   3 - Model Implementation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#build-sequence-model-bi-directional-model">
     3.1. Build Sequence Model (Bi-directional model)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-sequence-model-bi-directional-model">
     3.2. Train Sequence Model (Bi-directional model)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation">
   4 - Evaluation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-embedding-evaluation">
     4.1. Word Embedding Evaluation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-evaluation-with-data-processing-techiques">
     4.2. Performance Evaluation with Data Processing Techiques
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-evaluation-with-different-input">
     4.3. Performance Evaluation with Different Input
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-evaluation-with-different-sequence-models">
     4.4. Performance Evaluation with Different Sequence Models
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparameter-testing">
     4.5. HyperParameter Testing
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#object-oriented-programming-codes-here">
   Object Oriented Programming codes here
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#import-set-seed">
     Import &amp; Set seed
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#class-sequence-model">
     class - sequence model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#class-preprocessing">
     class - Preprocessing
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#class-word-embedding">
     class - Word_embedding
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#class-train">
     class - Train
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#class-instrinsic-eval">
     class - instrinsic eval
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preprocess-combo-compare">
     4.1 preprocess combo compare
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hyperparams-tuning-data">
     4.5 hyperparams tuning &amp; data
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#customdataset">
     CustomDataSet
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#helper">
     Helper
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#side-note">
     Side note
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#eda">
       EDA
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#profiling">
       profiling
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="binary-text-classification">
<h1>Binary text classification<a class="headerlink" href="#binary-text-classification" title="Permalink to this headline">#</a></h1>
<p>2022 COMP5046 Assignment 1 -
<em>Make sure you change the file name with your unikey.</em></p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="readme">
<h1>Readme<a class="headerlink" href="#readme" title="Permalink to this headline">#</a></h1>
<p>💎 Please <strong>run all OOP section first</strong> and then main section.💎<br />
💎 U can do it by click run after for the first cell in OOP:) 💎</p>
<ul class="simple">
<li><p>All detailed implmentation is written at the <strong>OOP</strong> section at the bottom.</p></li>
<li><p>Add best model result at section 4.4, which is loaded from gdrive.</p></li>
<li><p>I added batch size to training and better preprocessing after I finished all the evaluation, it turns out I then need to rewrite everything, so the evalution is based on the no batch version but the coding part has changed to the one using batch.</p></li>
</ul>
<p><em><strong>Visualising the comparison of different results is a good way to justify your decision.</strong></em></p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="data-preprocessing">
<h1>1 - Data Preprocessing<a class="headerlink" href="#data-preprocessing" title="Permalink to this headline">#</a></h1>
<section id="data-collection-do-not-modify-this">
<h2>1.0. Data Collection [DO NOT MODIFY THIS]<a class="headerlink" href="#data-collection-do-not-modify-this" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># Code to download file into Colaboratory:
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

id = &#39;16g474hdNsaNx0_SnoKuqj2BuwSEGdnbt&#39;
downloaded = drive.CreateFile({&#39;id&#39;:id}) 
downloaded.GetContentFile(&#39;training_data.csv&#39;)  

id = &#39;1-7hj0sF3Rc5G6POKdkpbDXm_Q6BWFDPU&#39;
downloaded = drive.CreateFile({&#39;id&#39;:id}) 
downloaded.GetContentFile(&#39;testing_data.csv&#39;)  

import pandas as pd
training_data = pd.read_csv(&quot;/content/training_data.csv&quot;)
testing_data = pd.read_csv(&quot;/content/testing_data.csv&quot;)

print(&quot;------------------------------------&quot;)
print(&quot;Size of training dataset: {0}&quot;.format(len(training_data)))
print(&quot;Size of testing dataset: {0}&quot;.format(len(testing_data)))
print(&quot;------------------------------------&quot;)

print(&quot;------------------------------------&quot;)
print(&quot;Sample Data&quot;)
print(&quot;LABEL: {0} / SENTENCE: {1}&quot;.format(training_data.iloc[-1,0], training_data.iloc[-1,1]))
print(&quot;------------------------------------&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>------------------------------------
Size of training dataset: 7808
Size of testing dataset: 867
------------------------------------
------------------------------------
Sample Data
LABEL: F / SENTENCE: &#39;Half of it is going straight to charity, another quarter going straight to scientific research, an eighth to the parkour community, a sixteenth to towards spreading information about health and...|||Find a path or suffer more.|||http://personalitycafe.com/enneagram-personality-theory-forum/85323-enneagram-type-mbti-type-compared-statistics.html yep.|||I kind of anchor on Fi and Ne makes having Ni really fun. INFP for me as they tire me out less and our views tend to align more.|||The two ESTPs I have gotten the chance to know seem to experience much more than other people who have been on the planet for the same amount of time and are quite the renaissance (wo)men.  Is this...|||I don&#39;t really have a best friend ISTP(passion-amateur group co-founder), INTJ(intellectual and various small hobbies talk), ESTP(Bro-in-law, talk about everything kind of like my INTJ friend),...|||Everyone looses their gift if they don&#39;t even consider a different perspective.|||Kansas - ISTJ|||That or if they are normally comfortable with me, such as a friend or close acquaintance, they feel the need to start talking. It&#39;s almost a trap, I&#39;ve noticed for most people feel the need to expose...|||To me, your answers screamed introverted feeling. Answers 2-5, 10, 11, 14, 16, and 17 your last statement were particularly Fi-like. I&#39;m guessing you are an intuitive and possibly and introvert...|||Could you explain your reasoning for these? I saw Mako as an F, Lin as an ES, and have Kya as an F. Never had an idea for Amon&#39;s type.|||This applies to many of these threads.|||With an INFP for over 2 years now.|||After watching tonight&#39;s episode I&#39;m sure that Unalaq is an ENXJ. I&#39;m not sure if it&#39;s Fe or Te at this point but the way he goes about doing and planning things seem like a Je-dom. I&#39;m putting him...|||Parkour is my passion(but I consider it closer to a martial art than a sport). I also enjoy some running and climbing.|||I have many characters but I gravitate towards sneaky archer, Breton, and conjuration. I love doing role plays and think it&#39;s one of, if not the best way to play the game.|||ESFP seems right for Ikki. We may need Jinora to have more interactions for us to tell. Any guesses about Pema, Tenzin&#39;s wife? She said herself that she used to be very shy so I&#39;d put I just from...|||If you don&#39;t mind, please tell me more by what you meant by this bolded part or what happened.|||I think it&#39;s fit to revive this thread seeing as the second season of Korra has started and the second episode of the season is coming up tomorrow. I&#39;d just say beware of spoilers in new posts if you...|||I was thinking more along these lines: 83385|||Yes, a few times in friendships and other things but it was usually spurred on by the idea of not having a second chance. I&#39;ve been trying to make the first move more in life as I&#39;ve realized it just...|||Sorry if my wording was/is confusing or vague. Let me try to explain it better.  As for the first statement: I see the world for all it&#39;s interconnections. If you wish, visualized everything having...|||~I don&#39;t experience it as simply perceiving or creating, for me as I perceive interconnected relationships are formed and realized.   ~I don&#39;t think that I rationalize with my dominate function but...|||I think it&#39;s amusing that, in the leading position I share with an ISTP friend of mine, we both start to embrace our shadows. I Think that&#39;s been my growing point lately, embracing my shadow. We&#39;re...|||I would suggest introspection and relying on your sense of self over tests and I highly suggest looking into the cognitive functions. ISTJ is the complete opposite of INFJ.|||I definitely agree with others on the US- It&#39;s pretty good for an INFJ if you find your niche.  I say the Midwest is generally SJ with women expected to be F and men to be T. It&#39;s nice but annoying....|||Please explain|||I think my own eye movements have almost been changed because of where I was usually placed when talking to someone in normal conversations. See, when I was young I ended up getting permanent spot in...|||Judgmental, critical, somewhat narcissistic, stubborn, possessive, Fe-ishly manipulative, and I have ego issues. Take that with a grain of salt.|||Yes, very much so. I love Spanish so far.|||I have a huge folder of these types of images.|||Aquarian It was just my guess, it doesn&#39;t need that much merit. Personally, I think Se is the hardest function to describe because it is so in the moment.|||Sorry, double post because of connectivity weirdness.|||I don&#39;t know if this has been posted before or if a thread about curses would be the best place but it&#39;ll do just fine. The important part is post #79, the giant wall of text. I think most of it was...|||If anything, imo, Ni would be how objects are interconnected. If I were to follow closely to your model: Introverted Intuition: Understanding how objects are connected Extraverted Intuition:...|||Sometimes you just don&#39;t see them :ninja: Seriously, I thought I was alone in a small town but I was surprised after training for a couple months.  You can easily learn and train by yourself, you...|||82063 Stuff by Andy Day, not only do I like it because it is the stuff of my passion but that new perspective of our surroundings that it brings. This is a great example of that. All those people...|||Sorry for the quality, my relative only gave me a physical copy, it&#39;s a picture of a picture. This is my INFP girlfriend of two years and me.|||If I am with my SO I almost need physical contact in some way.|||I pretty much have a guru dream that involves my SP wannabe passion. Around people I am close to I totally put on the gypsy king face, people are just so interesting. Hahaha can&#39;t stop laughing at ...|||I agree this this post very much, I just can&#39;t shake that vibe. To me it feels like you are an INTP who strongly identifies with INFJs. I think if you want a sound answer form yourself and others we...|||I do pretty well in emergencies, I do very well compared to normal conditions in my opinion. I feel like I become the ideal version of myself, for the most part. It&#39;s hard to describe but it&#39;s like...|||I have a very close INTJ friend. The Te Fe difference is acknowledged very well and I&#39;d say that both of our tertiary functions are well developed which helps a ton. He does not show it often but he...|||Being alone and/or doing something physical that I can naturally and reactively do without thinking or little thought.|||Pretty much this|||If I wear shoes or socks to bed and my feet are not on my bed I will wake up as if I was falling. 2/3 of the time this happens. Any other dreams that I remember(I don&#39;t remember most of my dreams...|||This one still gets me.  What I meant to say was Pass the salt but what I really said was You b****, you ruined my life|||I&#39;m sorry, but I find them so funny because I use them for good reason. They make people uncomfortable at first but then, slowly, make people more comfortable with the idea that people are different...|||XSFJ Mother, ISTJ father, and an XNFJ sister. Yep.|||I love dark jokes, especially racists/stereotypical jokes.&#39;
------------------------------------
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Preview of the data in the csv file, which has two columns: </span>
<span class="c1"># (1)type - label of the post (2)posts - the corresponding post content</span>
<span class="n">training_data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
  <div id="df-c70c6f70-380a-4cf0-9a96-d3d648ee572f">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>type</th>
      <th>posts</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>F</td>
      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>T</td>
      <td>'I'm finding the lack of me in these posts ver...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>T</td>
      <td>'Good one  _____   https://www.youtube.com/wat...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>T</td>
      <td>'Dear INTP,   I enjoyed our conversation the o...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>T</td>
      <td>'You're fired.|||That's another silly misconce...</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-c70c6f70-380a-4cf0-9a96-d3d648ee572f')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-c70c6f70-380a-4cf0-9a96-d3d648ee572f button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-c70c6f70-380a-4cf0-9a96-d3d648ee572f');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Extract the labels and posts and store into List</span>

<span class="c1"># Get the list of training data (posts)</span>
<span class="n">training_posts</span><span class="o">=</span><span class="n">training_data</span><span class="p">[</span><span class="s1">&#39;posts&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1"># Get the list of corresponding labels for the training data (posts)</span>
<span class="n">training_labels</span><span class="o">=</span><span class="n">training_data</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="c1"># Get the list of testing data (posts)</span>
<span class="n">testing_posts</span><span class="o">=</span><span class="n">testing_data</span><span class="p">[</span><span class="s1">&#39;posts&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1"># Get the list of corresponding labels for the testing data (posts)</span>
<span class="n">testing_labels</span><span class="o">=</span><span class="n">testing_data</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="url-removal">
<h2>1.1. URL Removal<a class="headerlink" href="#url-removal" title="Permalink to this headline">#</a></h2>
<p><em>related to the section 4.2</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Better naming</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">training_posts</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">testing_posts</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">training_labels</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">testing_labels</span>

<span class="c1"># REmove the URL for both training and testing set</span>
<span class="n">X_train_no_url</span> <span class="o">=</span> <span class="n">Prep</span><span class="o">.</span><span class="n">url_removal</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_no_url</span> <span class="o">=</span> <span class="n">Prep</span><span class="o">.</span><span class="n">url_removal</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="preprocess-data-e-g-stop-words-stemming">
<h2>1.2. Preprocess data (e.g. Stop words, Stemming)<a class="headerlink" href="#preprocess-data-e-g-stop-words-stemming" title="Permalink to this headline">#</a></h2>
<p><em>related to the section 4.2</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 2 mins+</span>
<span class="c1"># The best one</span>
<span class="n">X_train_best</span> <span class="o">=</span> <span class="n">Prep</span><span class="o">.</span><span class="n">pre_processing</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="c1"># 15mins+ -&gt; 4 mins after use only english stopwords -&gt; 2mins30s: after remove stemming and lemm -&gt; 1min : take firs 512 tokens</span>
<span class="n">X_test_best</span><span class="o">=</span> <span class="n">Prep</span><span class="o">.</span><span class="n">pre_processing</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> 

<span class="n">word_list</span><span class="p">,</span> <span class="n">vocab</span> <span class="o">=</span> <span class="n">Emb</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">X_train_best</span><span class="p">)</span>
<span class="c1"># Encode input </span>
<span class="n">X_train_encoded</span> <span class="o">=</span> <span class="n">Prep</span><span class="o">.</span><span class="n">encode_and_add_padding</span><span class="p">(</span><span class="n">X_train_best</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
<span class="n">X_test_encoded</span> <span class="o">=</span> <span class="n">Prep</span><span class="o">.</span><span class="n">encode_and_add_padding</span><span class="p">(</span><span class="n">X_test_best</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
<span class="n">y_train_encoded</span> <span class="o">=</span> <span class="n">Prep</span><span class="o">.</span><span class="n">encode_label</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_test_encoded</span> <span class="o">=</span> <span class="n">Prep</span><span class="o">.</span><span class="n">encode_label</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>

<span class="c1"># 💎Uncomment below to load the preprocess data from gdrive instead of real time processing since too long.</span>
<span class="c1"># h.download_best_model_gdrive()</span>
<span class="c1"># with open(&#39;X_train_number.json&#39;,&#39;r&#39;) as f:</span>
<span class="c1">#     X_train_processed_number = json.load(f)[&#39;data&#39;]</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="input-representation">
<h1>2 - Input Representation<a class="headerlink" href="#input-representation" title="Permalink to this headline">#</a></h1>
<section id="word-embedding-construction">
<h2>2.1. Word Embedding Construction<a class="headerlink" href="#word-embedding-construction" title="Permalink to this headline">#</a></h2>
<p><em>related to the section 4.1 and 4.3</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 2mins</span>
<span class="c1"># 💎 Preprocess data for word embeddings</span>
<span class="c1"># use both the training and testing datasets</span>
<span class="n">X_train_embed</span><span class="o">=</span> <span class="n">X_train_best</span> <span class="o">+</span> <span class="n">X_test_best</span>

<span class="c1"># 💎 Build the best self train model - FastText</span>
<span class="n">model_word2vec</span> <span class="o">=</span> <span class="n">Emb</span><span class="o">.</span><span class="n">make_self_trained_gensim_model</span><span class="p">(</span><span class="n">X_train_embed</span><span class="p">,</span> <span class="n">dimension_</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">window_</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># 5min+</span>

<span class="n">emb_table_word2vec</span><span class="p">,</span> <span class="n">emb_dim_word2vec</span> <span class="o">=</span> <span class="n">Emb</span><span class="o">.</span><span class="n">build_concat_embed_table</span><span class="p">(</span><span class="n">word_list</span><span class="p">,</span> <span class="p">[</span><span class="n">model_word2vec</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="pretrained-word-embedding">
<h2>2.2. Pretrained Word Embedding<a class="headerlink" href="#pretrained-word-embedding" title="Permalink to this headline">#</a></h2>
<p><em>related to the section 4.3</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 2mins+</span>
<span class="c1"># load pretrain twitter-25</span>
<span class="n">model_twitter</span> <span class="o">=</span> <span class="n">Emb</span><span class="o">.</span><span class="n">load_pre_train_gensim</span><span class="p">(</span><span class="n">which_</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Extract and apply the embedding</span>
<span class="c1"># Note it is applied as this embedding is concatenated in 2.3 which is then trained in 3.2 and evaluated in 4.3</span>
<span class="n">emb_table_twitter</span><span class="p">,</span> <span class="n">emb_dim_twitter</span> <span class="o">=</span> <span class="n">Emb</span><span class="o">.</span><span class="n">build_concat_embed_table</span><span class="p">(</span><span class="n">word_list</span><span class="p">,</span> <span class="p">[</span><span class="n">model_twitter</span><span class="p">])</span>

<span class="c1"># load pretrain wiki-100</span>
<span class="n">model_wiki</span> <span class="o">=</span> <span class="n">Emb</span><span class="o">.</span><span class="n">load_pre_train_gensim</span><span class="p">(</span><span class="n">which_</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Extract and apply the embedding </span>
<span class="n">emb_table_wiki</span><span class="p">,</span> <span class="n">emb_dim_wiki</span> <span class="o">=</span> <span class="n">Emb</span><span class="o">.</span><span class="n">build_concat_embed_table</span><span class="p">(</span><span class="n">word_list</span><span class="p">,</span> <span class="p">[</span><span class="n">model_wiki</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[==================================================] 100.0% 104.8/104.8MB downloaded
[==================================================] 100.0% 128.1/128.1MB downloaded
</pre></div>
</div>
</div>
</div>
</section>
<section id="input-concatenation">
<h2>2.3. Input Concatenation<a class="headerlink" href="#input-concatenation" title="Permalink to this headline">#</a></h2>
<p><em>related to the section 4.3</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Best Concatenation pretrain twitter &amp; wiki</span>
<span class="n">emb_table_concat</span><span class="p">,</span> <span class="n">emb_dim_concat</span> <span class="o">=</span> <span class="n">Emb</span><span class="o">.</span><span class="n">build_concat_embed_table</span><span class="p">(</span><span class="n">word_list</span><span class="p">,</span> <span class="p">[</span><span class="n">model_twitter</span><span class="p">,</span> <span class="n">model_wiki</span><span class="p">])</span> 

<span class="c1"># Below is the concatenation for self train + pretrained</span>
<span class="c1"># emb_table_concat, emb_dim_concat = Emb.build_concat_embed_table(word_list, [model_word2vec, model_twitter, model_wiki]) </span>

<span class="c1"># To address the OOV issue, the embedding is extracted the word exists in both the self train and pretrained</span>
<span class="c1"># Otherwise, it is pad with 0s.</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="model-implementation">
<h1>3 - Model Implementation<a class="headerlink" href="#model-implementation" title="Permalink to this headline">#</a></h1>
<section id="build-sequence-model-bi-directional-model">
<h2>3.1. Build Sequence Model (Bi-directional model)<a class="headerlink" href="#build-sequence-model-bi-directional-model" title="Permalink to this headline">#</a></h2>
<p><em>related to the section 4.4</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Bi_LSTM_Emb</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This Bi-LSTM model use the weights from the pretrained word embedding as shown in below: data.copy_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_class</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">,</span> <span class="n">emb_table</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Bi_LSTM_Emb</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">)</span>
        <span class="c1"># [IMPORTANT] Initialize the Embedding layer with the lookup table we created </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">emb_table</span><span class="p">))</span>
        <span class="c1"># Optional: set requires_grad = False to make this lookup table untrainable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">emb_dim</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">batch_first</span> <span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_class</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Get the embeded tensor</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>        
        <span class="c1"># we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.</span>
        <span class="c1"># details of the outputs from nn.LSTM can be found from: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html</span>
        <span class="c1"># c_n: containing the final cell state for each element in the sequence.</span>
        <span class="n">_</span><span class="p">,</span> <span class="p">(</span><span class="n">h_n</span><span class="p">,</span> <span class="n">c_n</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># concat the last hidden state from two direction</span>
        <span class="n">hidden_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">h_n</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,:],</span><span class="n">h_n</span><span class="p">[</span><span class="mi">1</span><span class="p">,:,:]),</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">hidden_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-sequence-model-bi-directional-model">
<h2>3.2. Train Sequence Model (Bi-directional model)<a class="headerlink" href="#train-sequence-model-bi-directional-model" title="Permalink to this headline">#</a></h2>
<p><em>related to the section 4.4</em></p>
<p>Note that it will not be marked if you do not display the Training Loss and the Number of Epochs in the Assignment 1 ipynb.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define best sequence model params</span>
<span class="n">PARAMS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;vocab_size&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_list</span><span class="p">),</span>
    <span class="s1">&#39;n_class&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s1">&#39;n_hidden&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="c1"># learning rate </span>
    <span class="s1">&#39;n_epoch&#39;</span><span class="p">:</span> <span class="mi">250</span>
<span class="p">}</span>

<span class="n">model_eval</span><span class="p">,</span> <span class="n">val_history</span> <span class="o">=</span> <span class="n">Train</span><span class="o">.</span><span class="n">train_sequence_model</span><span class="p">(</span><span class="n">X_train_encoded</span><span class="p">,</span> <span class="n">y_train_encoded</span><span class="p">,</span> <span class="n">PARAMS</span><span class="p">,</span> <span class="n">emb_dim_concat</span><span class="p">,</span> <span class="n">emb_table_concat</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="p">[</span><span class="n">X_test_encoded</span><span class="p">,</span> <span class="n">y_test_encoded</span><span class="p">])</span>
<span class="c1"># dropout give 0.6 poor result</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1, loss: 0.72880, train_acc: 0.45
Epoch: 2, loss: 0.68969, train_acc: 0.55
Epoch: 3, loss: 0.71804, train_acc: 0.42
Epoch: 4, loss: 0.69931, train_acc: 0.39
Epoch: 5, loss: 0.68372, train_acc: 0.58
Epoch: 6, loss: 0.69651, train_acc: 0.50
Epoch: 7, loss: 0.67035, train_acc: 0.67
Epoch: 8, loss: 0.73025, train_acc: 0.42
Epoch: 9, loss: 0.70207, train_acc: 0.50
Epoch: 10, loss: 0.70532, train_acc: 0.50
Epoch: 11, loss: 0.65637, train_acc: 0.66
Epoch: 12, loss: 0.69071, train_acc: 0.58
Epoch: 13, loss: 0.64800, train_acc: 0.69
Epoch: 14, loss: 0.69779, train_acc: 0.55
Epoch: 15, loss: 0.71276, train_acc: 0.52
Epoch: 16, loss: 0.66253, train_acc: 0.62
Epoch: 17, loss: 0.67896, train_acc: 0.62
Epoch: 18, loss: 0.69338, train_acc: 0.52
Epoch: 19, loss: 0.66535, train_acc: 0.62
Epoch: 20, loss: 0.70982, train_acc: 0.45
Epoch: 21, loss: 0.71053, train_acc: 0.52
Epoch: 22, loss: 0.70706, train_acc: 0.47
Epoch: 23, loss: 0.67143, train_acc: 0.59
Epoch: 24, loss: 0.65034, train_acc: 0.69
Epoch: 25, loss: 0.67799, train_acc: 0.56
Epoch: 26, loss: 0.72632, train_acc: 0.41
Epoch: 27, loss: 0.69184, train_acc: 0.53
Epoch: 28, loss: 0.68409, train_acc: 0.56
Epoch: 29, loss: 0.70221, train_acc: 0.45
Epoch: 30, loss: 0.67709, train_acc: 0.64
Epoch: 31, loss: 0.69472, train_acc: 0.53
Epoch: 32, loss: 0.68859, train_acc: 0.53
Epoch: 33, loss: 0.68916, train_acc: 0.50
Epoch: 34, loss: 0.68852, train_acc: 0.58
Epoch: 35, loss: 0.68335, train_acc: 0.61
Epoch: 36, loss: 0.68438, train_acc: 0.59
Epoch: 37, loss: 0.68491, train_acc: 0.58
Epoch: 38, loss: 0.68740, train_acc: 0.58
Epoch: 39, loss: 0.68970, train_acc: 0.53
Epoch: 40, loss: 0.69682, train_acc: 0.45
Epoch: 41, loss: 0.68481, train_acc: 0.61
Epoch: 42, loss: 0.68089, train_acc: 0.61
Epoch: 43, loss: 0.69017, train_acc: 0.59
Epoch: 44, loss: 0.68831, train_acc: 0.53
Epoch: 45, loss: 0.68715, train_acc: 0.45
Epoch: 46, loss: 0.67165, train_acc: 0.67
Epoch: 47, loss: 0.67805, train_acc: 0.59
Epoch: 48, loss: 0.69558, train_acc: 0.50
Epoch: 49, loss: 0.67744, train_acc: 0.56
Epoch: 50, loss: 0.68508, train_acc: 0.53
Epoch: 51, loss: 0.69412, train_acc: 0.45
Epoch: 52, loss: 0.68798, train_acc: 0.53
Epoch: 53, loss: 0.66841, train_acc: 0.64
Epoch: 54, loss: 0.68946, train_acc: 0.52
Epoch: 55, loss: 0.69977, train_acc: 0.47
Epoch: 56, loss: 0.72027, train_acc: 0.47
Epoch: 57, loss: 0.68787, train_acc: 0.52
Epoch: 58, loss: 0.68141, train_acc: 0.55
Epoch: 59, loss: 0.68993, train_acc: 0.58
Epoch: 60, loss: 0.66717, train_acc: 0.62
Epoch: 61, loss: 0.68970, train_acc: 0.45
Epoch: 62, loss: 0.69212, train_acc: 0.53
Epoch: 63, loss: 0.67888, train_acc: 0.55
Epoch: 64, loss: 0.68026, train_acc: 0.52
Epoch: 65, loss: 0.66219, train_acc: 0.72
Epoch: 66, loss: 0.70327, train_acc: 0.39
Epoch: 67, loss: 0.67479, train_acc: 0.59
Epoch: 68, loss: 0.68111, train_acc: 0.59
Epoch: 69, loss: 0.68708, train_acc: 0.56
Epoch: 70, loss: 0.68283, train_acc: 0.59
Epoch: 71, loss: 0.66753, train_acc: 0.64
Epoch: 72, loss: 0.69108, train_acc: 0.53
Epoch: 73, loss: 0.67624, train_acc: 0.62
Epoch: 74, loss: 0.67380, train_acc: 0.59
Epoch: 75, loss: 0.68352, train_acc: 0.56
Epoch: 76, loss: 0.69304, train_acc: 0.52
Epoch: 77, loss: 0.67913, train_acc: 0.58
Epoch: 78, loss: 0.70486, train_acc: 0.39
Epoch: 79, loss: 0.68238, train_acc: 0.58
Epoch: 80, loss: 0.67106, train_acc: 0.62
Epoch: 81, loss: 0.66899, train_acc: 0.64
Epoch: 82, loss: 0.67540, train_acc: 0.61
Epoch: 83, loss: 0.64932, train_acc: 0.67
Epoch: 84, loss: 0.67540, train_acc: 0.59
Epoch: 85, loss: 0.68331, train_acc: 0.55
Epoch: 86, loss: 0.68140, train_acc: 0.59
Epoch: 87, loss: 0.65818, train_acc: 0.62
Epoch: 88, loss: 0.65803, train_acc: 0.61
Epoch: 89, loss: 0.68274, train_acc: 0.53
Epoch: 90, loss: 0.65861, train_acc: 0.56
Epoch: 91, loss: 0.63702, train_acc: 0.64
Epoch: 92, loss: 0.65337, train_acc: 0.62
Epoch: 93, loss: 0.68744, train_acc: 0.50
Epoch: 94, loss: 0.66810, train_acc: 0.61
Epoch: 95, loss: 0.67296, train_acc: 0.56
Epoch: 96, loss: 0.66790, train_acc: 0.56
Epoch: 97, loss: 0.63115, train_acc: 0.66
Epoch: 98, loss: 0.67776, train_acc: 0.53
Epoch: 99, loss: 0.66676, train_acc: 0.56
Epoch: 100, loss: 0.66058, train_acc: 0.58
Epoch: 101, loss: 0.65182, train_acc: 0.61
Epoch: 102, loss: 0.66751, train_acc: 0.56
Epoch: 103, loss: 0.64848, train_acc: 0.61
Epoch: 104, loss: 0.65897, train_acc: 0.58
Epoch: 105, loss: 0.69033, train_acc: 0.56
Epoch: 106, loss: 0.66561, train_acc: 0.58
Epoch: 107, loss: 0.68354, train_acc: 0.56
Epoch: 108, loss: 0.69341, train_acc: 0.55
Epoch: 109, loss: 0.65446, train_acc: 0.61
Epoch: 110, loss: 0.65870, train_acc: 0.64
Epoch: 111, loss: 0.68442, train_acc: 0.53
Epoch: 112, loss: 0.63514, train_acc: 0.62
Epoch: 113, loss: 0.67721, train_acc: 0.52
Epoch: 114, loss: 0.69076, train_acc: 0.55
Epoch: 115, loss: 0.69089, train_acc: 0.59
Epoch: 116, loss: 0.63042, train_acc: 0.64
Epoch: 117, loss: 0.62311, train_acc: 0.67
Epoch: 118, loss: 0.67252, train_acc: 0.58
Epoch: 119, loss: 0.67428, train_acc: 0.59
Epoch: 120, loss: 0.67606, train_acc: 0.53
Epoch: 121, loss: 0.70212, train_acc: 0.55
Epoch: 122, loss: 0.64510, train_acc: 0.69
Epoch: 123, loss: 0.70854, train_acc: 0.58
Epoch: 124, loss: 0.71131, train_acc: 0.50
Epoch: 125, loss: 0.63990, train_acc: 0.67
Epoch: 126, loss: 0.65303, train_acc: 0.61
Epoch: 127, loss: 0.64462, train_acc: 0.56
Epoch: 128, loss: 0.63462, train_acc: 0.73
Epoch: 129, loss: 0.66310, train_acc: 0.56
Epoch: 130, loss: 0.60972, train_acc: 0.75
Epoch: 131, loss: 0.66658, train_acc: 0.58
Epoch: 132, loss: 0.65860, train_acc: 0.62
Epoch: 133, loss: 0.64643, train_acc: 0.62
Epoch: 134, loss: 0.61493, train_acc: 0.69
Epoch: 135, loss: 0.65780, train_acc: 0.61
Epoch: 136, loss: 0.62209, train_acc: 0.66
Epoch: 137, loss: 0.64578, train_acc: 0.66
Epoch: 138, loss: 0.63392, train_acc: 0.73
Epoch: 139, loss: 0.60288, train_acc: 0.67
Epoch: 140, loss: 0.62510, train_acc: 0.61
Epoch: 141, loss: 0.63981, train_acc: 0.62
Epoch: 142, loss: 0.65158, train_acc: 0.59
Epoch: 143, loss: 0.60841, train_acc: 0.67
Epoch: 144, loss: 0.60714, train_acc: 0.70
Epoch: 145, loss: 0.59772, train_acc: 0.70
Epoch: 146, loss: 0.56079, train_acc: 0.69
Epoch: 147, loss: 0.56101, train_acc: 0.69
Epoch: 148, loss: 0.69210, train_acc: 0.53
Epoch: 149, loss: 0.55292, train_acc: 0.69
Epoch: 150, loss: 0.72478, train_acc: 0.52
Epoch: 151, loss: 0.62389, train_acc: 0.67
Epoch: 152, loss: 0.64197, train_acc: 0.56
Epoch: 153, loss: 0.64345, train_acc: 0.64
Epoch: 154, loss: 0.63635, train_acc: 0.61
Epoch: 155, loss: 0.67547, train_acc: 0.64
Epoch: 156, loss: 0.61874, train_acc: 0.69
Epoch: 157, loss: 0.62959, train_acc: 0.66
Epoch: 158, loss: 0.65143, train_acc: 0.72
Epoch: 159, loss: 0.62030, train_acc: 0.67
Epoch: 160, loss: 0.63204, train_acc: 0.70
Epoch: 161, loss: 0.67315, train_acc: 0.56
Epoch: 162, loss: 0.69654, train_acc: 0.52
Epoch: 163, loss: 0.66523, train_acc: 0.58
Epoch: 164, loss: 0.62529, train_acc: 0.59
Epoch: 165, loss: 0.66593, train_acc: 0.52
Epoch: 166, loss: 0.61960, train_acc: 0.67
Epoch: 167, loss: 0.66273, train_acc: 0.59
Epoch: 168, loss: 0.63960, train_acc: 0.67
Epoch: 169, loss: 0.59990, train_acc: 0.73
Epoch: 170, loss: 0.67795, train_acc: 0.56
Epoch: 171, loss: 0.65175, train_acc: 0.47
Epoch: 172, loss: 0.65383, train_acc: 0.58
Epoch: 173, loss: 0.63681, train_acc: 0.72
Epoch: 174, loss: 0.64704, train_acc: 0.69
Epoch: 175, loss: 0.61254, train_acc: 0.64
Epoch: 176, loss: 0.64633, train_acc: 0.61
Epoch: 177, loss: 0.64688, train_acc: 0.61
Epoch: 178, loss: 0.63242, train_acc: 0.64
Epoch: 179, loss: 0.62932, train_acc: 0.66
Epoch: 180, loss: 0.59415, train_acc: 0.75
Epoch: 181, loss: 0.72029, train_acc: 0.55
Epoch: 182, loss: 0.59124, train_acc: 0.72
Epoch: 183, loss: 0.55698, train_acc: 0.73
Epoch: 184, loss: 0.62658, train_acc: 0.66
Epoch: 185, loss: 0.55122, train_acc: 0.80
Epoch: 186, loss: 0.63266, train_acc: 0.67
Epoch: 187, loss: 0.64340, train_acc: 0.64
Epoch: 188, loss: 0.62189, train_acc: 0.69
Epoch: 189, loss: 0.68467, train_acc: 0.59
Epoch: 190, loss: 0.60776, train_acc: 0.67
Epoch: 191, loss: 0.63616, train_acc: 0.62
Epoch: 192, loss: 0.65918, train_acc: 0.62
Epoch: 193, loss: 0.64863, train_acc: 0.66
Epoch: 194, loss: 0.65934, train_acc: 0.62
Epoch: 195, loss: 0.60676, train_acc: 0.62
Epoch: 196, loss: 0.71262, train_acc: 0.55
Epoch: 197, loss: 0.71657, train_acc: 0.52
Epoch: 198, loss: 0.67196, train_acc: 0.58
Epoch: 199, loss: 0.67525, train_acc: 0.56
Epoch: 200, loss: 0.68847, train_acc: 0.55
Epoch: 201, loss: 0.61188, train_acc: 0.69
Epoch: 202, loss: 0.61397, train_acc: 0.67
Epoch: 203, loss: 0.62681, train_acc: 0.59
Epoch: 204, loss: 0.72129, train_acc: 0.56
Epoch: 205, loss: 0.72388, train_acc: 0.56
Epoch: 206, loss: 0.56958, train_acc: 0.70
Epoch: 207, loss: 0.65051, train_acc: 0.64
Epoch: 208, loss: 0.66935, train_acc: 0.62
Epoch: 209, loss: 0.67628, train_acc: 0.58
Epoch: 210, loss: 0.64608, train_acc: 0.69
Epoch: 211, loss: 0.62052, train_acc: 0.64
Epoch: 212, loss: 0.68947, train_acc: 0.50
Epoch: 213, loss: 0.68163, train_acc: 0.58
Epoch: 214, loss: 0.65261, train_acc: 0.67
Epoch: 215, loss: 0.68551, train_acc: 0.58
Epoch: 216, loss: 0.61836, train_acc: 0.72
Epoch: 217, loss: 0.64999, train_acc: 0.67
Epoch: 218, loss: 0.63876, train_acc: 0.69
Epoch: 219, loss: 0.63720, train_acc: 0.70
Epoch: 220, loss: 0.68033, train_acc: 0.58
Epoch: 221, loss: 0.64662, train_acc: 0.66
Epoch: 222, loss: 0.66086, train_acc: 0.58
Epoch: 223, loss: 0.63562, train_acc: 0.66
Epoch: 224, loss: 0.64000, train_acc: 0.64
Epoch: 225, loss: 0.63015, train_acc: 0.58
Epoch: 226, loss: 0.65848, train_acc: 0.55
Epoch: 227, loss: 0.62636, train_acc: 0.66
Epoch: 228, loss: 0.61165, train_acc: 0.66
Epoch: 229, loss: 0.66929, train_acc: 0.59
Epoch: 230, loss: 0.63789, train_acc: 0.69
Epoch: 231, loss: 0.65141, train_acc: 0.56
Epoch: 232, loss: 0.60014, train_acc: 0.67
Epoch: 233, loss: 0.61863, train_acc: 0.66
Epoch: 234, loss: 0.65078, train_acc: 0.59
Epoch: 235, loss: 0.63005, train_acc: 0.69
Epoch: 236, loss: 0.62589, train_acc: 0.64
Epoch: 237, loss: 0.61588, train_acc: 0.59
Epoch: 238, loss: 0.64900, train_acc: 0.67
Epoch: 239, loss: 0.58109, train_acc: 0.70
Epoch: 240, loss: 0.58100, train_acc: 0.72
Epoch: 241, loss: 0.61628, train_acc: 0.69
Epoch: 242, loss: 0.63850, train_acc: 0.64
Epoch: 243, loss: 0.57982, train_acc: 0.70
Epoch: 244, loss: 0.72113, train_acc: 0.55
Epoch: 245, loss: 0.62411, train_acc: 0.69
Epoch: 246, loss: 0.70003, train_acc: 0.58
Epoch: 247, loss: 0.68028, train_acc: 0.61
Epoch: 248, loss: 0.58521, train_acc: 0.72
Epoch: 249, loss: 0.53862, train_acc: 0.78
Epoch: 250, loss: 0.65668, train_acc: 0.64
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="evaluation">
<h1>4 - Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">#</a></h1>
<section id="word-embedding-evaluation">
<h2>4.1. Word Embedding Evaluation<a class="headerlink" href="#word-embedding-evaluation" title="Permalink to this headline">#</a></h2>
<p>You are to apply Semantic-Syntactic word relationship tests for the trained word embeddings and visualise the result of Semantic-Syntactic word relationship tests.
Note that it will not be marked if you do not display it in the ipynb file.</p>
<p>(<em>Please show your empirical evidence and justification</em>)</p>
<p>🎶 <strong>Table</strong></p>
<p>Different feature <strong>dimension</strong> :</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Dim</p></th>
<th class="head"><p>Window Size</p></th>
<th class="head"><p>Sem</p></th>
<th class="head"><p>Syn</p></th>
<th class="head"><p>Tot.</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>100</p></td>
<td><p>5</p></td>
<td><p>12.75%  (38/298)</p></td>
<td><p>48.41%  (990/2045)</p></td>
<td><p>43.88%  (1028/2343)</p></td>
</tr>
<tr class="row-odd"><td><p>150</p></td>
<td><p>5</p></td>
<td><p>12.42%  (37/298)</p></td>
<td><p>50.86%  (1040/2045)</p></td>
<td><p>45.97%  (1077/2343)</p></td>
</tr>
<tr class="row-even"><td><p>200</p></td>
<td><p>5</p></td>
<td><p>8.72%  (26/298)</p></td>
<td><p>51.39%  (1051/2045)</p></td>
<td><p>45.97%  (1077/2343)</p></td>
</tr>
</tbody>
</table>
<hr class="docutils" />
<p>Different <strong>window size</strong>:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Dim</p></th>
<th class="head"><p>Window Size</p></th>
<th class="head"><p>Sem</p></th>
<th class="head"><p>Syn</p></th>
<th class="head"><p>Tot.</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>150</p></td>
<td><p>3</p></td>
<td><p>10.74%  (32/298)</p></td>
<td><p>55.31%  (1131/2045)</p></td>
<td><p>49.64%  (1163/2343)</p></td>
</tr>
<tr class="row-odd"><td><p>150</p></td>
<td><p>5</p></td>
<td><p>12.42%  (37/298)</p></td>
<td><p>50.86%  (1040/2045)</p></td>
<td><p>45.97%  (1077/2343)</p></td>
</tr>
<tr class="row-even"><td><p>150</p></td>
<td><p>7</p></td>
<td><p>14.77%  (44/298)</p></td>
<td><p>47.33%  (968/2045)</p></td>
<td><p>43.19%  (1012/2343)</p></td>
</tr>
<tr class="row-odd"><td><p>150</p></td>
<td><p>10</p></td>
<td><p>15.44%  (46/298)</p></td>
<td><p>42.25%  (864/2045)</p></td>
<td><p>38.84%  (910/2343)</p></td>
</tr>
</tbody>
</table>
<p>🎈 Model used: FastText &amp; skipgram - self train gensim</p>
<p>🍔 <strong>1. Observation &amp; Explain the pattern</strong></p>
<p>💎 <strong>1.1 Window size</strong></p>
<p>As seem from above table and below chart:</p>
<ul class="simple">
<li><p>(1) the semantic accuracy goes up as the window size increases while</p></li>
<li><p>(2) the syntactic and overall accuracy goes down.</p></li>
</ul>
<p>(1) is because:</p>
<ul class="simple">
<li><p><strong>Context</strong> of a word is defined by its <strong>surrounding</strong> words</p></li>
<li><p>As the window size goes large, more words are used to be trained for the model to understand the context of that word. Thus, the larger the window size, the higher the semantic accruacy.</p></li>
</ul>
<p>(2) is because:</p>
<ul class="simple">
<li><p>With the increase window size and thus the context words, the model needs to then focus on too many grammar/syntactic meaning at a time which confuse the model and thus lower the syntactic accuracy. Such decrease in syntactic is higher than the increase in semantic one and thus the overal accuracy goes down.</p></li>
</ul>
<p>Below <strong>references</strong> basically means <strong>higher window size, better semantic understnading</strong> for the model.</p>
<blockquote>
<div><p>A window size of 5 is commonly used to capture broad
<strong>topical</strong> content, whereas smaller windows contain
more focused <strong>information</strong> about the target word - Levy, O., &amp; Goldberg, Y. (2014, June)</p>
</div></blockquote>
<blockquote>
<div><p>To <strong>maximize the accuracy</strong> on the phrase analogy task, we increased the amount of the training data by using a dataset with about 33 billion words. We used the hierarchical softmax, dimensionality
of 1000, and the <strong>entire sentence for the context</strong>
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013)</p>
</div></blockquote>
<p>💎 <strong>1.2 Dimension Size</strong></p>
<p>The higher the dimension size, (1) the higher the overal and syntactic accuracy, meanwhile, (2) the lower the semantic arruacy.</p>
<p>(1) is because:</p>
<ul class="simple">
<li><p>There are more feature dimension to represent a word and so the syntactic/grammar understanding will be greater. The overall accuracy is higher because the the increase in syntactic accuacy is higher than the reduce in semantic.</p></li>
</ul>
<p>(2) is because:</p>
<ul class="simple">
<li><p>As the dimension get larger, the model is less capable to find the connection between different words of such much larger feature space. And thus poorer at understanding the context/semantic.</p></li>
</ul>
<p>🍔 <strong>Justify my decision</strong></p>
<p>My final choice for the model is (1) dimension size = 150 and (2) window size = 3.</p>
<p>(1) is because:</p>
<ul class="simple">
<li><p>Dimension size of 150 and 200 has the highest overall accuracy but size 150 is much smaller than 200 and it is less likely to overfit and faster to train.</p></li>
</ul>
<blockquote>
<div><p>“Given two models with the same error on the whole instance space X, choose the simpler one” - Occam’s Razor.</p>
</div></blockquote>
<p>(2) is because:</p>
<ul class="simple">
<li><p>Window size of 3 has the highest accuracy amongs other choice and that the samller the window size, the faster to train and less resource (e.g. ram) required.</p></li>
</ul>
<p><strong>Problem</strong></p>
<p>Note that the current approach fixed all other variables and varise one could miss the correlation between different params, sicne the combo of different params is not tested e.g. via gridsearch, the reference is thus used to ensure the observation is correct.</p>
<p><strong>Reference</strong>:</p>
<p>[1]: Levy, O., &amp; Goldberg, Y. (2014, June). Dependency-based word embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (pp. 302-308).</p>
<p>[2]: Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use self train gensim word2vec model as instructed, this word2vec one is the best</span>

<span class="c1"># Save gensim weight</span>
<span class="n">path</span> <span class="o">=</span> <span class="s1">&#39;model_word2vec_eval.txt&#39;</span>
<span class="n">model_word2vec</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Performace instrinsic evalution using standford provided ones</span>
<span class="n">Eval</span><span class="o">.</span><span class="n">instrinsic_eval</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="c1"># Recorded data with plot for different window size and dimension comparison</span>
<span class="n">plot_multi_lines</span><span class="p">(</span><span class="n">window_size_list</span><span class="p">,</span> <span class="s1">&#39;Window Size&#39;</span><span class="p">,</span> <span class="n">w_semantic_list</span><span class="p">,</span> <span class="n">w_syntactic_list</span><span class="p">,</span> <span class="n">w_overal_list</span><span class="p">)</span>
<span class="n">plot_multi_lines</span><span class="p">(</span><span class="n">dimension_list</span><span class="p">,</span> <span class="s1">&#39;Dimension Size&#39;</span><span class="p">,</span> <span class="n">d_semantic_list</span><span class="p">,</span> <span class="n">d_syntactic_list</span><span class="p">,</span> <span class="n">d_overal_list</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Vocab size:  18464
capital-common-countries.txt:
ACCURACY TOP1: 1.39% (1/72)
capital-world.txt:
ACCURACY TOP1: 0.00% (0/39)
currency.txt:
ACCURACY TOP1: 0.00% (0/10)
city-in-state.txt:
ACCURACY TOP1: 1.23% (1/81)
family.txt:
ACCURACY TOP1: 24.51% (75/306)
gram1-adjective-to-adverb.txt:
ACCURACY TOP1: 73.03% (593/812)
gram2-opposite.txt:
ACCURACY TOP1: 84.98% (430/506)
gram3-comparative.txt:
ACCURACY TOP1: 67.30% (848/1260)
gram4-superlative.txt:
ACCURACY TOP1: 82.34% (578/702)
gram5-present-participle.txt:
ACCURACY TOP1: 74.87% (566/756)
gram6-nationality-adjective.txt:
ACCURACY TOP1: 34.24% (252/736)
gram7-past-tense.txt:
ACCURACY TOP1: 25.31% (284/1122)
gram8-plural.txt:
ACCURACY TOP1: 54.07% (571/1056)
gram9-plural-verbs.txt:
ACCURACY TOP1: 73.52% (372/506)
Questions seen/total: 40.75% (7964/19544)
Semantic accuracy: 15.16%  (77/508)
Syntactic accuracy: 60.27%  (4494/7456)
Total accuracy: 57.40%  (4571/7964)
</pre></div>
</div>
<img alt="../../../_images/a1_27_1.png" src="../../../_images/a1_27_1.png" />
<img alt="../../../_images/a1_27_2.png" src="../../../_images/a1_27_2.png" />
</div>
</div>
</section>
<section id="performance-evaluation-with-data-processing-techiques">
<h2>4.2. Performance Evaluation with Data Processing Techiques<a class="headerlink" href="#performance-evaluation-with-data-processing-techiques" title="Permalink to this headline">#</a></h2>
<p>You are required to evaluate with the testing dataset and provide the table with f1 of test set.
Note that it will not be marked if you do not display it in the ipynb file.</p>
<p>(<em>Please show your empirical evidence and justification</em>)</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>F1</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Bi-LSTM With URL</p></td>
<td><p>0.3771, precision:  0 for class 1, division by 0 issue</p></td>
</tr>
<tr class="row-odd"><td><p>Bi-LSTM Without URL</p></td>
<td><p>0.3792, precision:  0.5 for class 1</p></td>
</tr>
<tr class="row-even"><td><p>Bi-LSTM Without url, puncation, stopwords, Lemmatisation, lowercase,stemming, number</p></td>
<td><p>0.440213</p></td>
</tr>
<tr class="row-odd"><td><p>Bi-LSTM Without url, puncation, stopwords, Lemmatisation, lowercase,stemming</p></td>
<td><p>0.489996</p></td>
</tr>
<tr class="row-even"><td><p>Bi-LSTM Without url, puncation, stopwords(english), max len=128 (best model)</p></td>
<td><p>0.52864</p></td>
</tr>
</tbody>
</table>
<p><strong>Note that</strong> the last row is trained with 250 epoch while others are 10.</p>
<p>🍔 <strong>Explain the pattern</strong></p>
<p>Based on the above table, (1) dataset without url is slightly better while (2) the preprocessing combination with the number is better that without it.</p>
<p>(1) is because:</p>
<ul class="simple">
<li><p>It could be that url is not a valuable information to determine whether a person is a thinker or feeler and that given it is a small dataset, the url may not be processed by the model effectively to highlights its importance.</p></li>
</ul>
<p>(2) is because:</p>
<ul class="simple">
<li><p>This is explainable as number could be a key factor of the personality type thinker whereas a feeler may use more adjective instead of number to address their comments.</p></li>
</ul>
<p>🍔 <strong>Justify my decision</strong></p>
<p>For the final decision on pre processing, I choose the one <strong>Without url, puncation, stopwords(english)</strong> as it gives the highest f1 score. It is because:</p>
<ul class="simple">
<li><p><strong>Lemmatisation and stemming not used</strong> not used because it is important to make the vocab as close as possible to the pretrain embedding to maximise the use of it. Both pretrained used are glove and the preprocessing for glove do not involved Lemmatisation and stemming.</p></li>
<li><p><strong>Remove URL</strong> because the result difference is minimal with or without URL and URL could introduce a big over head in creatign the vocab and the later training process.</p></li>
<li><p><strong>Remove english stopwords</strong> because it does not help decide personality.</p></li>
</ul>
<p>Nan / none value</p>
<ul class="simple">
<li><p>Min len in training is 57 so nothing is done here.</p></li>
</ul>
<p>Length</p>
<ul class="simple">
<li><p>Note that the length of sentence is cut off at 512 for training the word embedding and 128 for the model because the embedding need large context words to ensure a better semantic understanding and 128 because I want to train the result fast to make more comparison.</p></li>
</ul>
<p>Some improvement could be:</p>
<ul class="simple">
<li><p>instead of remove the url, count the number of it and add it as a feature.</p></li>
<li><p>Before lower case, count number of upper case and add it to features.</p></li>
<li><p>add occurance of punction clusters as feature since a feeler may use more punctions</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Best Preprocessing combinations </span>
<span class="c1"># ONE test case</span>
<span class="n">p2</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;vocab_size&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_list</span><span class="p">),</span>
    <span class="s1">&#39;n_class&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s1">&#39;n_hidden&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="c1"># learning rate </span>
    <span class="s1">&#39;n_epoch&#39;</span><span class="p">:</span> <span class="mi">20</span>
<span class="p">}</span>
<span class="n">model_preprocessed</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Train</span><span class="o">.</span><span class="n">train_sequence_model</span><span class="p">(</span><span class="n">X_train_encoded</span><span class="p">,</span> <span class="n">y_train_encoded</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="n">emb_dim_concat</span><span class="p">,</span> <span class="n">emb_table_concat</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="p">[</span><span class="n">X_test_encoded</span><span class="p">,</span> <span class="n">y_test_encoded</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;f1 score: &#39;</span><span class="p">,</span> <span class="n">Eval</span><span class="o">.</span><span class="n">eval_f1_score</span><span class="p">(</span><span class="n">model_preprocessed</span><span class="p">,</span> <span class="n">X_test_encoded</span><span class="p">,</span> <span class="n">y_test_encoded</span><span class="p">))</span>

<span class="c1"># Because it is a control and treatment group, the model used here is not the best one which is ok based on Ed </span>


<span class="c1"># URL section </span>
<span class="c1"># # Encode input </span>
<span class="c1"># X_train_no_url_encoded = Prep.encode_and_add_padding(X_train_no_url, vocab)</span>
<span class="c1"># X_test_no_url_encoded = Prep.encode_and_add_padding(X_test_no_url, vocab)</span>

<span class="c1"># model_no_url = Train.train_sequence_model(X_train_no_url_encoded, y_train_encoded, PARAMS, emb_dim_concat, emb_table_concat)</span>
<span class="c1"># print()</span>
<span class="c1"># Eval.eval_f1_score(model_no_url, X_test_no_url_encoded, y_test_encoded, is_report=True)</span>
<span class="c1"># model_preprocessed = Train.train_sequence_model(X_train_encoded, y_train_encoded, PARAMS, emb_dim_concat, emb_table_concat)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1, loss: 0.70424, train_acc: 0.44
Epoch: 2, loss: 0.69765, train_acc: 0.50
Epoch: 3, loss: 0.68138, train_acc: 0.56
Epoch: 4, loss: 0.68906, train_acc: 0.55
Epoch: 5, loss: 0.69850, train_acc: 0.44
Epoch: 6, loss: 0.69582, train_acc: 0.42
Epoch: 7, loss: 0.67972, train_acc: 0.58
Epoch: 8, loss: 0.69424, train_acc: 0.58
Epoch: 9, loss: 0.68024, train_acc: 0.58
Epoch: 10, loss: 0.68372, train_acc: 0.58
Epoch: 11, loss: 0.65952, train_acc: 0.70
Epoch: 12, loss: 0.67881, train_acc: 0.58
Epoch: 13, loss: 0.72361, train_acc: 0.42
Epoch: 14, loss: 0.64051, train_acc: 0.67
Epoch: 15, loss: 0.69414, train_acc: 0.59
Epoch: 16, loss: 0.69932, train_acc: 0.56
Epoch: 17, loss: 0.69692, train_acc: 0.55
Epoch: 18, loss: 0.68037, train_acc: 0.58
Epoch: 19, loss: 0.70640, train_acc: 0.48
Epoch: 20, loss: 0.69266, train_acc: 0.56
f1 score:  0.3966393075271114
</pre></div>
</div>
</div>
</div>
</section>
<section id="performance-evaluation-with-different-input">
<h2>4.3. Performance Evaluation with Different Input<a class="headerlink" href="#performance-evaluation-with-different-input" title="Permalink to this headline">#</a></h2>
<p>You are required to evaluate with the testing dataset and provide the table with f1 of test set.
Note that it will not be marked if you do not display it in the ipynb file.</p>
<p>(<em>Please show your empirical evidence and justification</em>)</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>F1</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Bi-LSTM with Word2vec (SG)</p></td>
<td><p>0.4446</p></td>
</tr>
<tr class="row-odd"><td><p>Bi-LSTM with fasttext (SG)</p></td>
<td><p>0.3771</p></td>
</tr>
<tr class="row-even"><td><p>Bi-LSTM with glove-twitter-25</p></td>
<td><p>0.3933</p></td>
</tr>
<tr class="row-odd"><td><p>Bi-LSTM with glove-wiki-gigaword-100</p></td>
<td><p>0.3823</p></td>
</tr>
<tr class="row-even"><td><p>Bi-LSTM with + glove-twitter-25 +  glove-wiki-gigaword-100</p></td>
<td><p>0.4790</p></td>
</tr>
<tr class="row-odd"><td><p>Bi-LSTM with Word2vec (SG) + fasttext (SG) + glove-twitter-25 + glove-wiki-gigaword-100</p></td>
<td><p>0.4330</p></td>
</tr>
</tbody>
</table>
<p>Note that above is tested with 10 epoch.</p>
<p>🎶 <strong>1 Explain the pattern</strong></p>
<p>🦉 <strong>1.1 Observatoin</strong></p>
<p>Based on the above table and the model parameters e.g. 10 epoch only,</p>
<ul class="simple">
<li><p>(1) For <em><strong>self training</strong></em> word embedding, word2vec seems performace better than fasttext by 0.07</p></li>
<li><p>(2) For <em><strong>pretrain</strong></em> embedding, the twitter pretrain embedding better than wiki one by 0.01</p></li>
<li><p>(3) For <em><strong>concatentation</strong></em>, the embedding concatenation of twitter and wiki is better than concatenation with all the self train and pretrain embedding. (0.05+) It is also the best embedding model among other choices.</p></li>
<li><p>(4) <strong>Compare</strong> self train and pretrain, Word2vec self train is better than any other single word embedding model. Other three models perform similarly.</p></li>
<li><p>(5) In general, it seems like <strong>the more</strong> different word embeddings, <strong>the</strong> <strong>better</strong> result.</p></li>
</ul>
<p>🐨 <strong>1.2 Explanation</strong></p>
<p>(1), (4) is because:</p>
<ul class="simple">
<li><p>It could be since this is a <strong>small dataset</strong>, fasttext n-gram approach and the glove may need more data to be effective to capture the fine representation whereas word2vec is simpler and may not need big data to capture the vector space properly.</p></li>
</ul>
<p>(2) is because:</p>
<ul class="simple">
<li><p>Twitter may contain more <strong>emotional words</strong> which is helpful in determining the feeler and thus the thinker. Wiki is more <strong>formal</strong> and so not as effective.</p></li>
</ul>
<p>(3), (5) is because:</p>
<ul class="simple">
<li><p>It is likely that the diversity is more important the quantity for embedding concatenation.This could be why the pretrain concatenation is better than the self train one as pretrain one is from a large corpus with large size and thus diversity.</p></li>
</ul>
<p>🎶 <strong>2 Justify my decision</strong></p>
<p>I choose <strong>Bi-LSTM with + glove-twitter-25 +  glove-wiki-gigaword-100</strong>.</p>
<ul class="simple">
<li><p>Firstly, it gives the highest f1 score compared with others.</p></li>
<li><p>Secondly, there two mebedding are pretrained with large corpus, the model is thus likely to generate better than other options.<br />
These two Glove pretrained embedding are considered over other gensim pretrained provided because they are the smallest available which makes the trainign process faster and that the dataset is small so no need for large embedding.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load best model for evalution</span>
<span class="c1"># One test case</span>
<span class="n">p2</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;vocab_size&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_list</span><span class="p">),</span>
    <span class="s1">&#39;n_class&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s1">&#39;n_hidden&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="c1"># learning rate </span>
    <span class="s1">&#39;n_epoch&#39;</span><span class="p">:</span> <span class="mi">20</span>
<span class="p">}</span>
<span class="n">model_input</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Train</span><span class="o">.</span><span class="n">train_sequence_model</span><span class="p">(</span><span class="n">X_train_encoded</span><span class="p">,</span> <span class="n">y_train_encoded</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="n">emb_dim_concat</span><span class="p">,</span> <span class="n">emb_table_concat</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="p">[</span><span class="n">X_test_encoded</span><span class="p">,</span> <span class="n">y_test_encoded</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;f1 score: &#39;</span><span class="p">,</span> <span class="n">Eval</span><span class="o">.</span><span class="n">eval_f1_score</span><span class="p">(</span><span class="n">model_input</span><span class="p">,</span> <span class="n">X_test_encoded</span><span class="p">,</span> <span class="n">y_test_encoded</span><span class="p">))</span>


<span class="c1"># Other ones</span>
<span class="c1"># model_word2vec_eval = Train.train_sequence_model(X_train_encoded, y_train_encoded, PARAMS, emb_dim_word2vec, emb_table_word2vec)</span>
<span class="c1"># model_fastext_eval = Train.train_sequence_model(X_train_encoded, y_train_encoded, PARAMS, emb_dim_fastext, emb_table_fastext)</span>
<span class="c1"># model_twitter_eval = Train.train_sequence_model(X_train_encoded, y_train_encoded, PARAMS, emb_dim_twitter, emb_table_twitter)</span>
<span class="c1"># model_wiki_eval = Train.train_sequence_model(X_train_encoded, y_train_encoded, PARAMS, emb_dim_wiki, emb_table_wiki)</span>
<span class="c1"># etc..</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1, loss: 0.70374, train_acc: 0.45
Epoch: 2, loss: 0.69720, train_acc: 0.44
Epoch: 3, loss: 0.71608, train_acc: 0.36
Epoch: 4, loss: 0.72377, train_acc: 0.34
Epoch: 5, loss: 0.69968, train_acc: 0.45
Epoch: 6, loss: 0.68566, train_acc: 0.58
Epoch: 7, loss: 0.68755, train_acc: 0.59
Epoch: 8, loss: 0.69987, train_acc: 0.39
Epoch: 9, loss: 0.68075, train_acc: 0.58
Epoch: 10, loss: 0.68097, train_acc: 0.59
Epoch: 11, loss: 0.69924, train_acc: 0.45
Epoch: 12, loss: 0.68422, train_acc: 0.58
Epoch: 13, loss: 0.69794, train_acc: 0.52
Epoch: 14, loss: 0.69408, train_acc: 0.58
Epoch: 15, loss: 0.67980, train_acc: 0.58
Epoch: 16, loss: 0.70852, train_acc: 0.47
Epoch: 17, loss: 0.68791, train_acc: 0.55
Epoch: 18, loss: 0.70869, train_acc: 0.48
Epoch: 19, loss: 0.70748, train_acc: 0.53
Epoch: 20, loss: 0.67323, train_acc: 0.62
f1 score:  0.3940615472208672
</pre></div>
</div>
</div>
</div>
</section>
<section id="performance-evaluation-with-different-sequence-models">
<h2>4.4. Performance Evaluation with Different Sequence Models<a class="headerlink" href="#performance-evaluation-with-different-sequence-models" title="Permalink to this headline">#</a></h2>
<p>You are required to evaluate with the testing dataset and provide the table with f1 of test set.
Note that it will not be marked if you do not display it in the ipynb file.</p>
<p>(<em>Please show your empirical evidence and justification</em>)</p>
<p>All below model has the word mebedding with: <code class="docutils literal notranslate"><span class="pre">glove-twitter-25</span> <span class="pre">+</span>&#160; <span class="pre">glove-wiki-gigaword-100</span></code>, 10 epoches (except for the 150 one).</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>F1</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Bi-RNN</p></td>
<td><p>0.5174</p></td>
</tr>
<tr class="row-odd"><td><p>Bi-LSTM</p></td>
<td><p>0.4790</p></td>
</tr>
<tr class="row-even"><td><p>Bi-GRU</p></td>
<td><p>0.3874</p></td>
</tr>
<tr class="row-odd"><td><p>Bi-LSTM (150 epoch)</p></td>
<td><p>0.65</p></td>
</tr>
</tbody>
</table>
<p>🎶 <strong>1 Explain the pattern</strong></p>
<p>🦉 <strong>1.1 Observatoin</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">Bi-RNN</span> <span class="pre">&gt;</span> <span class="pre">Bi-LSTM</span> <span class="pre">&gt;</span> <span class="pre">Bi-GRU</span></code> : For f1 score from highest to lowerst.</p>
<p>🐨 <strong>1.2 Explanation</strong></p>
<p>Given this is a relatively small dataset, the vanishing / exploding graident for RNN may not occur here. Besides, RNN is a relatively simpler model which is perhaps more suitable for this dataset. LSTM and GRU may be a bit more complex for this small dataset.
Therefore, RNN outperformace the others. GRU performs the poorest, it may because it needs some more fine-tuning which is not done or blow justify the reason.</p>
<blockquote>
<div><p>“The GRU outperformed the LSTM on all tasks with the exception of language modelling” - [1].</p>
</div></blockquote>
<p>[1] An Empirical Exploration of Recurrent Network Architectures, Google (Ilya, Wojciech, Rafal): <a class="reference external" href="http://proceedings.mlr.press/v37/jozefowicz15.pdf">http://proceedings.mlr.press/v37/jozefowicz15.pdf</a></p>
<p>🎈 <strong>2 Justify my decision</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">LSTM</span> <span class="pre">+</span> <span class="pre">glove-twitter-25</span> <span class="pre">+</span>&#160; <span class="pre">glove-wiki-gigaword-100</span></code> is used for the final model choice.<br />
It is because LSTM can handle longer sequence better than RNN and that its performance is not far from RNN given that the epoch used here is <code class="docutils literal notranslate"><span class="pre">10</span></code> only and no parameters tuning is added yet. GRU is not considered given its low f1 score.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># One test case</span>
<span class="n">p2</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;vocab_size&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_list</span><span class="p">),</span>
    <span class="s1">&#39;n_class&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s1">&#39;n_hidden&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="c1"># learning rate </span>
    <span class="s1">&#39;n_epoch&#39;</span><span class="p">:</span> <span class="mi">20</span>
<span class="p">}</span>
<span class="n">model_seq</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Train</span><span class="o">.</span><span class="n">train_sequence_model</span><span class="p">(</span><span class="n">X_train_encoded</span><span class="p">,</span> <span class="n">y_train_encoded</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="n">emb_dim_concat</span><span class="p">,</span> <span class="n">emb_table_concat</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="p">[</span><span class="n">X_test_encoded</span><span class="p">,</span> <span class="n">y_test_encoded</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;f1 score: &#39;</span><span class="p">,</span> <span class="n">Eval</span><span class="o">.</span><span class="n">eval_f1_score</span><span class="p">(</span><span class="n">model_seq</span><span class="p">,</span> <span class="n">X_test_encoded</span><span class="p">,</span> <span class="n">y_test_encoded</span><span class="p">))</span>


<span class="c1"># IMPORTANT: Below result is based on the best model, the table result is determined by a non-best one.</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">load_best_model</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;best f1 score: &#39;</span><span class="p">,</span> <span class="n">Eval</span><span class="o">.</span><span class="n">eval_f1_score</span><span class="p">(</span><span class="n">best_model</span><span class="p">,</span> <span class="n">X_test_encoded</span><span class="p">,</span> <span class="n">y_test_encoded</span><span class="p">))</span>

<span class="c1"># Below is score for the other 2 models, RNN &amp; GRU</span>
<span class="c1"># GRU</span>
<span class="c1"># model_gru_eval = Train.train_sequence_model(X_train_encoded, y_train_encoded, PARAMS, emb_dim_concat, emb_table_concat, which_=2)</span>
<span class="c1"># Eval.eval_f1_score(model_gru_eval, X_test_encoded, y_test_encoded, is_report=True)</span>
<span class="c1"># RNN</span>
<span class="c1"># model_rnn_eval = Train.train_sequence_model(X_train_encoded, y_train_encoded, PARAMS, emb_dim_concat, emb_table_concat, which_=3)</span>
<span class="c1"># Eval.eval_f1_score(model_rnn_eval, X_test_encoded, y_test_encoded, is_report=True)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 1, loss: 0.69798, train_acc: 0.45
Epoch: 2, loss: 0.71091, train_acc: 0.44
Epoch: 3, loss: 0.70474, train_acc: 0.47
Epoch: 4, loss: 0.69291, train_acc: 0.52
Epoch: 5, loss: 0.69493, train_acc: 0.53
Epoch: 6, loss: 0.69029, train_acc: 0.56
Epoch: 7, loss: 0.68786, train_acc: 0.61
Epoch: 8, loss: 0.69325, train_acc: 0.53
Epoch: 9, loss: 0.70703, train_acc: 0.45
Epoch: 10, loss: 0.70170, train_acc: 0.47
Epoch: 11, loss: 0.68408, train_acc: 0.62
Epoch: 12, loss: 0.68418, train_acc: 0.53
Epoch: 13, loss: 0.66303, train_acc: 0.67
Epoch: 14, loss: 0.70325, train_acc: 0.48
Epoch: 15, loss: 0.68086, train_acc: 0.56
Epoch: 16, loss: 0.72416, train_acc: 0.44
Epoch: 17, loss: 0.70445, train_acc: 0.47
Epoch: 18, loss: 0.69098, train_acc: 0.50
Epoch: 19, loss: 0.68671, train_acc: 0.55
Epoch: 20, loss: 0.67623, train_acc: 0.59
f1 score:  0.44357715691040633
best f1 score:  0.5559555298167024
</pre></div>
</div>
</div>
</div>
</section>
<section id="hyperparameter-testing">
<h2>4.5. HyperParameter Testing<a class="headerlink" href="#hyperparameter-testing" title="Permalink to this headline">#</a></h2>
<p><em>You are required to draw a graph(y-axis: f1, x-axis: epoch) for test set and explain the optimal number of epochs based on the learning rate you have already chosen.</em> Note that it will not be marked if you do not display it in the ipynb file.</p>
<p>(<em>Please show your empirical evidence and justification</em>)</p>
<p><strong>README</strong>:</p>
<ul class="simple">
<li><p>Below explanation is <strong>based on the first two charts</strong>.</p></li>
<li><p>Note that the first 2 chart is the result without using batch size.</p></li>
<li><p>The last three using the batch size of 64 (added late).</p></li>
</ul>
<p>🎶 <strong>1 Explain the pattern</strong></p>
<p>🦉 <strong>1.1 Observation</strong></p>
<p>The model learns (improves it f1 socre) quickly during 0 ~ 40 epoch. Then the learning starts to slow down during 41~100. And after 90th epoch, the model starts to converge and little improvement is observed afte 100th epoch. <strong>(1)</strong></p>
<p>The learning process for learing rate (lr) of 1e-3 starts <strong>converging</strong> after 100 epoch with f1 score around 0.63 whereas lr of 1e-4 is <strong>still learning</strong> after 150 epoch with f1 score of 0.52 at the 149th epoch.<br />
<strong>(2)</strong> Given epoch of 150, model with bigger lr converges faster.</p>
<p>🐨 <strong>1.2 Explanation</strong></p>
<p>(1) is because:</p>
<ul class="simple">
<li><p>At the <strong>early</strong> stage, the model quickly forms its representation to understand the relationship between the sequence input and the target type and therefore f1 score improves quickly. During the <strong>middle</strong> stage, such representation is twisted and adjusted with less significant changes on the parameters. At the <strong>late</strong> stage, the represenatation could not be improved due to the <strong>limitation</strong> of:<br />
💎 small sample size / the low complexity of the model / less fine tuned parameters / the inpropriate preprocessing / the less diverse word embedding.</p></li>
</ul>
<p>(2) is because:</p>
<ul class="simple">
<li><p>A bigger lr means a <strong>greater update</strong> in the graident which then changes the weights faster after every epoch of training and thus it will learn &amp; converge faster.</p></li>
</ul>
<p>🎈 <strong>2 Justify my decision</strong></p>
<p>The <strong>optimal epoch</strong> chosen is <strong>150</strong>. It is because the later ones seems to be oscillating with little improve in the validation (on testing set). I think further training may lead to overfitting issue that make the model less generalise. And so early stopping at 100 epoch is applied.</p>
<p><strong>Lr = 1e-3 is chosen</strong>. This is because this lr helps the model to achieve a reasonable f1 score given the small dataset and the simple model. The smaller learning rate is not considered as it converges too slowly. As seen from below, it is at 0.52 f1 score after 150 epoch compared to 0.65. And the smaller one, if <strong>continues this trend</strong> of learning (epoch 130 ~ 150 improve by 0.02), would take about (0.65 - 0.52) / 0.02 * (150-130) = 130 epoch more to reach 0.65 score though this is likely to be a bad estimation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 💎💎💎 Uncommented Below if use real time data  💎💎💎</span>
<span class="c1"># plot_val_history(val_history, title_=&#39;Epoch vs F1 score: Bi-LSTM Learnign rate = 1e-3&#39;)</span>

<span class="n">plot_val_history</span><span class="p">(</span><span class="n">lstm_ep_1e_3</span><span class="p">,</span> <span class="n">title_</span><span class="o">=</span><span class="s1">&#39;No batch but all training set with epoch 150: Epoch vs F1 score: Bi-LSTM Learnign rate = 1e-3&#39;</span><span class="p">)</span>
<span class="n">plot_val_history</span><span class="p">(</span><span class="n">lstm_ep_1e_4</span><span class="p">,</span> <span class="n">title_</span><span class="o">=</span><span class="s1">&#39;No batch but all training set with epoch 150: : Epoch vs F1 score: Bi-LSTM Learnign rate = 1e-4&#39;</span><span class="p">)</span>
<span class="n">plot_val_history</span><span class="p">(</span><span class="n">plot_res_final</span><span class="p">,</span> <span class="n">title_</span><span class="o">=</span><span class="s1">&#39;Batch 64 with epoch 250: Epoch vs F1 score: Bi-LSTM Learnign rate = 1e-3&#39;</span><span class="p">)</span>
<span class="n">plot_val_history</span><span class="p">(</span><span class="n">lstm_1e_3_batch_64</span><span class="p">,</span> <span class="n">title_</span><span class="o">=</span><span class="s1">&#39;Batch 64 with epoch 500: Epoch vs F1 score: Bi-LSTM Learnign rate = 1e-3&#39;</span><span class="p">)</span>
<span class="n">plot_val_history</span><span class="p">(</span><span class="n">lstm_1e_4_batch_64</span><span class="p">,</span> <span class="n">title_</span><span class="o">=</span><span class="s1">&#39;Batch 64with epoch 500: Epoch vs F1 score: Bi-LSTM Learnign rate = 1e-4&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/a1_39_0.png" src="../../../_images/a1_39_0.png" />
<img alt="../../../_images/a1_39_1.png" src="../../../_images/a1_39_1.png" />
<img alt="../../../_images/a1_39_2.png" src="../../../_images/a1_39_2.png" />
<img alt="../../../_images/a1_39_3.png" src="../../../_images/a1_39_3.png" />
<img alt="../../../_images/a1_39_4.png" src="../../../_images/a1_39_4.png" />
</div>
</div>
<p>#5 - Test your model via Colab Form Fields User Interface</p>
<p>You are required to design a user interface so that user can input a textual sentence via the colab form fields user interface to get the personality type classification result from your trained model. <em>You can just modify based on the following Colab Form Fields template</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#@title Personality Type Prediction

text = &#39;I am thinking logically&#39;  #@param {type: &quot;string&quot;}

from pathlib import Path
import re
import numpy as np
import os
import json
from google.colab import drive

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords as sw
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

import nltk
nltk.download(&#39;punkt&#39;, quiet=True)
nltk.download(&#39;stopwords&#39;, quiet=True)
nltk.download(&#39;wordnet&#39;, quiet=True)

from gensim.models import Word2Vec # TODO: Glove
from gensim.models import FastText

import gensim.downloader as api

from typing import * # type hint

import warnings
warnings.filterwarnings(&quot;ignore&quot;, category=DeprecationWarning) 

%matplotlib inline 
# from IPython.core.interactiveshell import InteractiveShell
# from IPython import get_ipython
# get_ipython().ast_node_interactivity = &#39;all&#39;

CONFIG = {
    &#39;seed&#39;: 23
}

def set_seed(seed=42):
    &#39;&#39;&#39;Sets seed so result unchanged - reproducibility&#39;&#39;&#39;
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    # When running on the CuDNN backend, two further options must be set
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    # Set a fixed value for the hash seed
    os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)
    
set_seed(CONFIG[&#39;seed&#39;])
device = torch.device(&#39;cpu&#39;) # torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

#################################################################
sww = set(sw.words(&#39;english&#39;)) # 179 vs 7110 if without specify english
#################################################################
class Bi_LSTM_Emb(nn.Module):
    def __init__(self, vocab_size_, n_hidden_, n_class_, emb_dim_, emb_table_):
        super(Bi_LSTM_Emb, self).__init__()
        self.emb = nn.Embedding(vocab_size_, emb_dim_)
        # [IMPORTANT] Initialize the Embedding layer with the lookup table we created 
        self.emb.weight.data.copy_(torch.from_numpy(emb_table_))
        # Optional: set requires_grad = False to make this lookup table untrainable
        self.emb.weight.requires_grad = False

        self.lstm = nn.LSTM(emb_dim_, n_hidden_, batch_first =True, bidirectional=True)
        self.linear = nn.Linear(n_hidden_*2, n_class_)

    def forward(self, x):
        # Get the embeded tensor
        x = self.emb(x)        
        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.
        # details of the outputs from nn.LSTM can be found from: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html
        # c_n: containing the final cell state for each element in the sequence.
        _, (h_n, c_n) = self.lstm(x)
        # concat the last hidden state from two direction
        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)
        z = self.linear(hidden_out)
        return z

class Preprocessing:
    def __init__(self):
        self.lEnc = LabelEncoder()
    #################################### 1.1 URL Removal #############################
    &quot;&quot;&quot;
    You are asked to remove the URL from the post. 
    You are asked to compare, by experimental results (in Section 4.2), 
    when you remove the URL from the post versus keeping the URL. 
    Which will you use? (Section 4.2., Justify your decision)
    &quot;&quot;&quot;
    def template4comment(var_: object) -&gt; object:
        &quot;&quot;&quot;summary
        Args:
            var(type): comment
        Returns:
            var(type): comment
        &quot;&quot;&quot;
    def url_removal(self, posts_:List[str]) -&gt; List[str]:
        &quot;&quot;&quot;remove url from given post
        1. use ||| to split different posts by the same user
        2. use re to remove url with http pattern
        Args:
            post_(str): one instance of the training set
        Returns:
            str: with url removed
        &quot;&quot;&quot;
        res = []
        for x in posts_:
            # x = x.replace(&#39;|||&#39;, &#39; &#39;)
            x = re.sub(r&#39;http\S+&#39;, &#39;&#39;, x) #TODO: explain
            res.append(x)
        return res

    #################################### 1.2 Preprocess data: for the trained model #############################
    &quot;&quot;&quot;
    you are asked to pre-process the training set by integrating several 
    tept pre-processing techniques [Lab5] (e.g. tokenisation, removing numbers,
    converting to lowercase, removing stop words, stemming, etc.). 
    You should test and justify the reason why you apply the specific 
    preprocessing techniques based on the test result 
    in section 4.2 (Section 4.2., Justify your decision)
    &quot;&quot;&quot;
    &quot;&quot;&quot;
    #Note:
    Order of pre-processing matters, i.e. 
    stop before stemming otherwise word chanegd and cannot be 
    &quot;&quot;&quot;
    def pre_processing(self, posts_:List[str], is_urls=False, is_stopwords=False, is_number=False, is_encoded=False) -&gt; List[List[str]]:
        &quot;&quot;&quot;pre-process the training set
        Args:
            posts_(List[str]): comment
        Returns:
            var( List[List[str]] ): a outer list contains a list of word pre-processed tokens, where each inner list represents a post
        &quot;&quot;&quot;
        res = []
        for x in posts_:
            # separate different posts from the same user
            x = x.replace(&#39;|||&#39;, &#39; &#39;)

            if not is_urls:
                x = re.sub(r&#39;http\S+&#39;, &#39;&#39;, x) 

            # remove puncation
            x = re.sub(r&#39;[^\w\s]&#39;,&#39;&#39;, x) 
            ############################################################## before tokenisation
            # word tokenisation
            x = word_tokenize(x)

            # remove puncation
            x = [re.sub(r&#39;[^\w\s]&#39;,&#39;&#39;, w) for w in x]

            # remove stop words
            if not is_stopwords:
                
                x = [w for w in x if not w in sww]

            # remove number, note this will not remove string contain integer  
            if not is_number:
                x = [w for w in x if not w.isdigit()]
            ############################################################## lab5

            # Lemmatisation 
            lemmatizer = WordNetLemmatizer()
            x = [lemmatizer.lemmatize(w) for w in x]
            ############################################################## lab5
            # convert the tokens into lowercase
            x = [t.lower() for t in x]
            
            # stemming
            stemmer = PorterStemmer()
            x = [stemmer.stem(w) for w in x]

            # remove empty word
            x = [w for w in x if w]

            # assignment
            res.append(x)

        if is_encoded:
            _, vocab_ = self.build_vocab(res)
            return self.encode_and_add_padding(res, vocab_)
        return res
    #################################### 2.1 Word Embedding Construction #############################

    ############################### 2.1 (1): Preprocess data for word embeddings  #########################
        
    def encode_label(self, labels:List[str]) -&gt; List[str]:
        &quot;&quot;&quot;encode the label from form of str to integer
        Args:
            labels(List[str]): a list of string: either F - Feeler or T - Thinker
        Returns:
            (List[str]): a list of integer labels instead of str labels
        &quot;&quot;&quot;
        
        # encode the labels 
        # Hint: Try to understand the difference between fit_transform and transform
        return self.lEnc.fit_transform(labels)

    #################### Prepare data ####################
    def encode_and_add_padding(self, sentences:List[str], word_index:dict) -&gt; List[List[int]]:
        &quot;&quot;&quot;
        Convert the sentences to the word index that aligns with the lookup table
        Args:
            sentences(List[str]): pre-processed posts
            word_index(dict): mapping between unique word to integer
        Returns:
            sent_encoded(List[List[int]]): a list of list containing integer that correspond to each word in a sentece
        &quot;&quot;&quot;
        
        len_list = [len(s) for s in sentences]
        max_seq_length = 256 #max(len_list) #max length for all input sequence / post
        
        sent_encoded = []

        for sent in sentences:
            # if in then add idx, else integer for the unknown token
            temp_encoded = [word_index[word] if word in word_index else word_index[&#39;[UNKNOWN]&#39;] for word in sent] 
            # if len is less than pre-defined, add pad token until same len
            if len(temp_encoded) &lt; max_seq_length:
                temp_encoded += [word_index[&#39;[PAD]&#39;]] * (max_seq_length - len(temp_encoded)) 
            # exceed. then cutoff
            if len(temp_encoded) &gt; max_seq_length:
                temp_encoded = temp_encoded[:max_seq_length]
            sent_encoded.append(temp_encoded)
        return sent_encoded

Prep_ = Preprocessing()

class Word_embedding:
    def __init__(self):
        pass
    
    ############################### 2.1 (2): Build training model for word embeddings  #########################  
  
    &quot;&quot;&quot;
    Build training model for word embeddings: You are to build a training model for word embeddings. 
    You are required to articulate the hyperparameters [Lab2] you choose (dimension of embeddings and window size) 
    in Section 4.1. Note that any word embeddings model [Lab2] (e.g. word2vec-CBOW, word2vec-Skip gram, fasttext, glove) 
    can be applied. (Section 4.1. and Section 4.3., Justify your decision)
    &quot;&quot;&quot;
    def make_self_trained_gensim_model(self, posts_, dimension_=25, window_=3, which_=1) -&gt; Type[Word2Vec]:
        &quot;&quot;&quot;contruct gensim word2vec model
        Args:
            posts_(List[str]): a list of string: either F - Feeler or T - Thinker
        Returns:
            (gensim.model.Word2Vec): gensim API to create word2vec model
        &quot;&quot;&quot;
        #TODO: params
        models = {
            1: FastText,
            2: Word2Vec
        }
        return models[which_](sentences=posts_, size=dimension_, window=window_, min_count=5, workers=4, sg=1)

    ############################################### 2.2  Pretrained Word Embedding ##################################################
    &quot;&quot;&quot;
    You are asked to extract and apply the pretrained word embedding. 
    Gensim provides several pretrained word embeddings, you can find those in the gensim github. 
    You can select the pretrained word embedding that would be useful for the assignment 1 task, 
    personality type classification.(Section 4.3., Justify your decision)
    &quot;&quot;&quot;

    ######### load gensim pretrain model #########
    &quot;&quot;&quot;
    Pre trained model based on link here: https://github.com/RaRe-Technologies/gensim-data#models
    &quot;&quot;&quot;
    def load_pre_train_gensim(self, which_=1) -&gt; object:
        # load the pretrained embedding
        models = {
            1:&#39;glove-twitter-25&#39;,
            2: &#39;glove-wiki-gigaword-100&#39;
        }

        return api.load(models[which_]) # NOTE: Download an embedding other than glove-twitter-255

    ######### build_vocab #########
    def build_vocab(self, docs_:List[List[str]]) -&gt; dict:
        &quot;&quot;&quot; build vocabulary based on given dataset
        Args:
            docs_: list of documents that contains string of sentence post
        Returns:
            vocab with key as unique word, value as its mapping integer
        &quot;&quot;&quot;
        word_set = set() 
        for sent in docs_:
            for word in sent:
                word_set.add(word)
        word_set.add(&#39;[PAD]&#39;)
        word_set.add(&#39;[UNKNOWN]&#39;)

        word_list = list(word_set) 
        word_list.sort() # just to make sure order persistent

        word_index = {}
        ind = 0
        for word in word_list:
            word_index[word] = ind
            ind += 1
        return word_list, word_index

    ############################################### 2.3 Input Concatenation ##################################################
    def build_concat_embed_table(self, word_list_: list, word_emb_models:List[object]) -&gt; np.array:
        &quot;&quot;&quot; Build the embedding table from the pre-train model.
        Extract those word embedding that exists in the vocab of the current dataset
        Args:
            word_list_(list): a list of unique word in the corpus
        Returns:
            word_emb_models(List[gensim.model]): a list of gensim models
        &quot;&quot;&quot;
        emb_dim = 0
        for m in word_emb_models:
            emb_dim += m.vector_size

        emb_table = []
        
        for i, word in enumerate(word_list_):
            is_existed = False # initially, assume word does not exist in any embedding 
            concat = []
            if all(word in w_emb for w_emb in word_emb_models): # must all in to ensure shape consistent 
                for w_emb in word_emb_models: # concat all
                    if concat == []: # when empty initalise
                        concat = w_emb[word]
                    else:
                        concat = np.concatenate((concat, w_emb[word]), 0)
                emb_table.append(concat)
            else:
                # The pretrained glove twitter does not contain the embeddings for the [PAD] and [UNKNOWN] tokens we defined
                # Here, we just use all 0 for both [PAD] and [UNKNOWN] tokens for simplicity
                #TODO
                emb_table.append(np.array([0]*emb_dim))

        return np.array(emb_table), emb_dim
Emb_ = Word_embedding()


############################################### 3.2. Train Sequence Model (Bi-directional model) ###############################################
class Train_model:
    def __init__(self):
        pass
    def train_sequence_model(self, X_train_, y_train_, params:dict, emb_dim, emb_table, which_=1, test_data=[]):
        &quot;&quot;&quot;train the sequence model&quot;&quot;&quot;
        assert type(which_) == int, &#39;type(which_) == int&#39;
        assert type(test_data) == list, &#39;type(test_data) == list&#39;
        assert type(emb_dim) == int, &#39;type(emb_dim) == int&#39;

        p = params
        vocab_size, n_class, n_hidden, lr_, n_epoch = p[&#39;vocab_size&#39;], p[&#39;n_class&#39;], p[&#39;n_hidden&#39;], p[&#39;lr&#39;], p[&#39;n_epoch&#39;]

        model = None
        # Move the model to GPU
        if which_ == 1:
            model = Bi_LSTM_Emb(vocab_size, n_hidden, n_class, emb_dim, emb_table).to(device)
        elif which_ == 2:
            model = Bi_GRU(vocab_size, n_hidden, n_class, emb_dim, emb_table).to(device)
        elif which_ ==3:
            model = Bi_RNN(vocab_size, n_hidden, n_class, emb_dim, emb_table).to(device)
        # Loss function and optimizer
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=lr_)

        # Preparing input
        input_torch = torch.from_numpy(np.array(X_train_)).to(device)
        targe_torch = torch.from_numpy(np.array(y_train_)).view(-1).to(device)
        
        plot = []

        for epoch in range(n_epoch):  
            
            # Set the flag to training
            model.train()
            
            # forward + backward + optimize
            optimizer.zero_grad()
            outputs = model(input_torch) 
            loss = criterion(outputs, targe_torch)
            loss.backward()
            optimizer.step()
            
            predicted = torch.argmax(outputs, -1)
            acc = -1
            
            acc= accuracy_score(predicted.cpu().numpy(),targe_torch.cpu().numpy())
            print(&#39;Epoch: %d, loss: %.5f, train_acc: %.2f&#39; %(epoch + 1, loss.item(), acc))
            eval_tmp = Instrinsic_eval()
            if test_data != [] and (epoch % 5 == 0 or epoch == n_epoch - 1):
                f1 = eval_tmp.eval_f1_score(model, test_data[0], test_data[1])
                plot.append([epoch, f1])
        if test_data != []:
            return model, plot
        return model
        
Train_ = Train_model()

########################################################################################################################
############################################### Full run logic #########################################################
########################################################################################################################

def run_pipeline():
    
    X_train_ori = training_posts
    y_train_ori = training_labels

    X_train_ = Prep_.pre_processing(X_train_ori)
    #TODO: donwload
    X_train_encoded_ = Prep_.encode_and_add_padding(X_train_, vocab)
    y_train_encoded_ = Prep_.encode_label(y_train_ori)

    word_list_, vocab_ = Emb_.build_vocab(X_train_)

    PARAMS_ = {
            &#39;vocab_size&#39;: 69071, # 69071, # len(word_list)
            &#39;n_class&#39;: 2,
            &#39;n_hidden&#39;: 32,
            &#39;lr&#39;: 1e-3, # learning rate 
            &#39;n_epoch&#39;: 150
        }

    model_twitter_ = Emb_.load_pre_train_gensim(which_=1)
    model_gigaword_ = Emb_.load_pre_train_gensim(which_=2)

    emb_table_concat_, emb_dim_concat_ = Emb_.build_concat_embed_table(word_list_, [model_twitter_, model_gigaword_]) 

    return Train_.train_sequence_model(X_train_encoded_, y_train_encoded_, PARAMS_, emb_dim_concat_, emb_table_concat_) 

########################################################################################################################
############################################### Load downloaded torch model ############################################
########################################################################################################################

def load_local_torch_model(model_, path_:str):
    
    model_.load_state_dict(torch.load(path_))
    return model_

def load_local_numpy_txt(path_:str):
        
    return np.loadtxt(path_, dtype=np.float64)

def download_best_model_gdrive():
    # Code to download file into Colaboratory:
    !pip install -U -q PyDrive
    from pydrive.auth import GoogleAuth
    from pydrive.drive import GoogleDrive
    from google.colab import auth
    from oauth2client.client import GoogleCredentials
    # Authenticate and create the PyDrive client.
    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)


    id = &#39;16g474hdNsaNx0_SnoKuqj2BuwSEGdnbt&#39;
    downloaded = drive.CreateFile({&#39;id&#39;:id}) 
    downloaded.GetContentFile(&#39;training_data.csv&#39;)  

    id = &#39;1-7hj0sF3Rc5G6POKdkpbDXm_Q6BWFDPU&#39;
    downloaded = drive.CreateFile({&#39;id&#39;:id}) 
    downloaded.GetContentFile(&#39;testing_data.csv&#39;)  

    import pandas as pd
    training_data = pd.read_csv(&quot;/content/training_data.csv&quot;)
    testing_data = pd.read_csv(&quot;/content/testing_data.csv&quot;)


    # Extract the labels and posts and store into List

    # Get the list of training data (posts)
    training_posts=training_data[&#39;posts&#39;].tolist()
    # Get the list of corresponding labels for the training data (posts)
    training_labels=training_data[&#39;type&#39;].tolist()

    # Get the list of testing data (posts)
    testing_posts=testing_data[&#39;posts&#39;].tolist()
    # Get the list of corresponding labels for the testing data (posts)
    testing_labels=testing_data[&#39;type&#39;].tolist()

    ############################################################################################################

    id = &#39;15w8MO5FjQAxt9chIOEWEBWiXktA5xOjz&#39;
    downloaded = drive.CreateFile({&#39;id&#39;:id}) 
    downloaded.GetContentFile(best_model_path) 

    id = &#39;1-CKuXmWSb75rejxAWpaHZE7PWQ_YuoiZ&#39;
    downloaded = drive.CreateFile({&#39;id&#39;:id}) 
    downloaded.GetContentFile(best_embed_path)  

    id = &#39;1LDzY7S-Mqbm4xGWdVNvS4BaBS1QZ8waz&#39;
    downloaded = drive.CreateFile({&#39;id&#39;:id}) 
    downloaded.GetContentFile(vocab_path)  
    
def make_prediction(input_):
     
    model_file = Path(best_model_path)
    embed_file = Path(best_embed_path)
    vocab_file = Path(vocab_path)
    
    PARAMS_ = {
        &#39;vocab_size&#39;: 69071, # len(word_list)
        &#39;n_class&#39;: 2,
        &#39;n_hidden&#39;: 32,
        &#39;lr&#39;: 1e-3, # learning rate 
        &#39;n_epoch&#39;: 150
    }
    if not all([model_file.is_file(), embed_file.is_file(), vocab_file.is_file()]): # check if file exists
        
        # download the saved word embedding for the model and the model weight in order to reuse the best model
        download_best_model_gdrive()

    # Load data from json file
    with open(vocab_path,&#39;r&#39;) as f:
        vocab_ = json.load(f)[&#39;data&#39;]
    emb_dim_concat_ = 125 # 25 (twitter) + 100 (wiki)
    emb_table_concat_ = load_local_numpy_txt(best_embed_path)
    model_ = Bi_LSTM_Emb(PARAMS_[&#39;vocab_size&#39;], PARAMS_[&#39;n_hidden&#39;], PARAMS_[&#39;n_class&#39;], emb_dim_concat_, emb_table_concat_)
    model_ = load_local_torch_model(model_, best_model_path)
        
    # 💎 Uncomment below to run the full pipeline from preprocessing to training
    # model_ = run_pipeline()
    
    text_processed = Prep_.pre_processing([input_])
    text_encoded = Prep_.encode_and_add_padding(text_processed, vocab_)

    input_torch = torch.from_numpy(np.array(text_encoded)).to(device)
    outputs = model_(input_torch) 
    predicted = torch.argmax(outputs, -1)
    return predicted.cpu().numpy()

best_model_path = &#39;best_model.pth&#39;
best_embed_path = &#39;best_embed.txt&#39;
vocab_path = &#39;vocab.json&#39;

res1 = make_prediction(text)[0]
res2 = &#39;F&#39; if res1 == 0 else &#39;T&#39;

print(&quot;Predicted Personality Type: &quot;, res2) # transform back from integer to its orginal label
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predicted Personality Type:  T
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="object-oriented-programming-codes-here">
<h1>Object Oriented Programming codes here<a class="headerlink" href="#object-oriented-programming-codes-here" title="Permalink to this headline">#</a></h1>
<p><em>You can use multiple code snippets. Just add more if needed</em></p>
<section id="import-set-seed">
<h2>Import &amp; Set seed<a class="headerlink" href="#import-set-seed" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>import re
import numpy as np
import os
import json
from google.colab import drive
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords as sw
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

import nltk
nltk.download(&#39;punkt&#39;, quiet=True)
nltk.download(&#39;stopwords&#39;, quiet=True)
nltk.download(&#39;wordnet&#39;, quiet=True)

from gensim.models import Word2Vec # TODO: Glove
from gensim.models import FastText

import gensim.downloader as api

from typing import * # type hint

import warnings
warnings.filterwarnings(&quot;ignore&quot;, category=DeprecationWarning) 

%matplotlib inline 
# from IPython.core.interactiveshell import InteractiveShell
# from IPython import get_ipython
# get_ipython().ast_node_interactivity = &#39;all&#39;

CONFIG = {
    &#39;seed&#39;: 23
}

def set_seed(seed=42):
    &#39;&#39;&#39;Sets seed so result unchanged - reproducibility&#39;&#39;&#39;
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    # When running on the CuDNN backend, two further options must be set
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    # Set a fixed value for the hash seed
    os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)
    
set_seed(CONFIG[&#39;seed&#39;])
device = torch.device(&#39;cpu&#39;) #torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

!git clone --quiet https://github.com/stanfordnlp/GloVe.git # for the instrinic evalution

sww = set(sw.words(&#39;english&#39;)) # 179 vs 7110 if without specify english
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fatal: destination path &#39;GloVe&#39; already exists and is not an empty directory.
</pre></div>
</div>
</div>
</div>
</section>
<section id="class-sequence-model">
<h2>class - sequence model<a class="headerlink" href="#class-sequence-model" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">############################################### 3.1 Build Sequence Model (Bi-directional model) ##################################################</span>

<span class="c1">#################### Bi-GRU ####################</span>
<span class="k">class</span> <span class="nc">Bi_GRU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_class</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">,</span> <span class="n">emb_table</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Bi_GRU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">)</span>
        <span class="c1"># [IMPORTANT] Initialize the Embedding layer with the lookup table we created </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">emb_table</span><span class="p">))</span>
        <span class="c1"># Optional: set requires_grad = False to make this lookup table untrainable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">emb_dim</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">batch_first</span> <span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_class</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Get the embeded tensor</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>        
        <span class="c1"># we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.</span>
        <span class="c1"># details of the outputs from nn.LSTM can be found from: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html</span>
        <span class="c1"># c_n: containing the final cell state for each element in the sequence.</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">h_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># concat the last hidden state from two direction</span>
        <span class="n">hidden_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">h_n</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,:],</span><span class="n">h_n</span><span class="p">[</span><span class="mi">1</span><span class="p">,:,:]),</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">hidden_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span>
<span class="c1">#################### RNN ####################</span>
<span class="k">class</span> <span class="nc">Bi_RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_class</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">,</span> <span class="n">emb_table</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Bi_RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">)</span>
        <span class="c1"># [IMPORTANT] Initialize the Embedding layer with the lookup table we created </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">emb_table</span><span class="p">))</span>
        <span class="c1"># Optional: set requires_grad = False to make this lookup table untrainable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="n">emb_dim</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">batch_first</span> <span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_class</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Get the embeded tensor</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>        
        <span class="c1"># we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.</span>
        <span class="c1"># details of the outputs from nn.LSTM can be found from: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html</span>
        <span class="c1"># c_n: containing the final cell state for each element in the sequence.</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">h_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># concat the last hidden state from two direction</span>
        <span class="n">hidden_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">h_n</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,:],</span><span class="n">h_n</span><span class="p">[</span><span class="mi">1</span><span class="p">,:,:]),</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">hidden_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="class-preprocessing">
<h2>class - Preprocessing<a class="headerlink" href="#class-preprocessing" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Preprocessing</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lEnc</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
    <span class="c1">#################################### 1.1 URL Removal #############################</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    You are asked to remove the URL from the post. </span>
<span class="sd">    You are asked to compare, by experimental results (in Section 4.2), </span>
<span class="sd">    when you remove the URL from the post versus keeping the URL. </span>
<span class="sd">    Which will you use? (Section 4.2., Justify your decision)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">template4comment</span><span class="p">(</span><span class="n">var_</span><span class="p">:</span> <span class="nb">object</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;summary</span>
<span class="sd">        Args:</span>
<span class="sd">            var(type): comment</span>
<span class="sd">        Returns:</span>
<span class="sd">            var(type): comment</span>
<span class="sd">        &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">url_removal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">posts_</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;remove url from given post</span>
<span class="sd">        1. use ||| to split different posts by the same user</span>
<span class="sd">        2. use re to remove url with http pattern</span>
<span class="sd">        Args:</span>
<span class="sd">            post_(str): one instance of the training set</span>
<span class="sd">        Returns:</span>
<span class="sd">            str: with url removed</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">posts_</span><span class="p">:</span>
            <span class="c1"># x = x.replace(&#39;|||&#39;, &#39; &#39;)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;http\S+&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="c1">#TODO: explain</span>
            <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span>

    <span class="c1">#################################### 1.2 Preprocess data: for the trained model #############################</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    you are asked to pre-process the training set by integrating several </span>
<span class="sd">    tept pre-processing techniques [Lab5] (e.g. tokenisation, removing numbers,</span>
<span class="sd">    converting to lowercase, removing stop words, stemming, etc.). </span>
<span class="sd">    You should test and justify the reason why you apply the specific </span>
<span class="sd">    preprocessing techniques based on the test result </span>
<span class="sd">    in section 4.2 (Section 4.2., Justify your decision)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    #Note:</span>
<span class="sd">    Order of pre-processing matters, i.e. </span>
<span class="sd">    stop before stemming otherwise word chanegd and cannot be </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">pre_processing</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">posts_</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">is_urls</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">is_stopwords</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">is_number</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">is_lemm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">is_stem</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;pre-process the training set</span>
<span class="sd">        Args:</span>
<span class="sd">            posts_(List[str]): comment</span>
<span class="sd">        Returns:</span>
<span class="sd">            var( List[List[str]] ): a outer list contains a list of word pre-processed tokens, where each inner list represents a post</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">posts_</span><span class="p">:</span>
            <span class="c1"># separate different posts from the same user</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;|||&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_urls</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;http\S+&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> 

            <span class="c1">############################################################## before tokenisation</span>
            <span class="c1"># word tokenisation</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">max_len</span> <span class="o">=</span> <span class="mi">512</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="n">max_len</span><span class="p">]</span>
            <span class="c1"># remove puncation</span>
            <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[^\w\s]&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>

            <span class="c1"># remove stop words</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_stopwords</span><span class="p">:</span>
                
                <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sww</span><span class="p">]</span>

            <span class="c1"># remove number, note this will not remove string contain integer  </span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_number</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">w</span><span class="o">.</span><span class="n">isdigit</span><span class="p">()]</span>
            <span class="c1">############################################################## lab5</span>

            <span class="c1"># Lemmatisation </span>
            <span class="k">if</span> <span class="n">is_lemm</span><span class="p">:</span>
                <span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
                <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
            <span class="c1">############################################################## lab5</span>
            <span class="c1"># convert the tokens into lowercase</span>
            <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
            
            <span class="c1"># stemming</span>
            <span class="k">if</span> <span class="n">is_stem</span><span class="p">:</span>
                <span class="n">stemmer</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
                <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>

            <span class="c1"># remove empty word</span>
            <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span> <span class="k">if</span> <span class="n">w</span><span class="p">]</span>

            <span class="c1"># assignment</span>
            <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">res</span>
    <span class="c1">#################################### 2.1 Word Embedding Construction #############################</span>

    <span class="c1">############################### 2.1 (1): Preprocess data for word embeddings  #########################</span>
        
    <span class="k">def</span> <span class="nf">encode_label</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;encode the label from form of str to integer</span>
<span class="sd">        Args:</span>
<span class="sd">            labels(List[str]): a list of string: either F - Feeler or T - Thinker</span>
<span class="sd">        Returns:</span>
<span class="sd">            (List[str]): a list of integer labels instead of str labels</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="c1"># encode the labels </span>
        <span class="c1"># Hint: Try to understand the difference between fit_transform and transform</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lEnc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

    <span class="c1">#################### Prepare data ####################</span>
    <span class="k">def</span> <span class="nf">encode_and_add_padding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">word_index</span><span class="p">:</span><span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert the sentences to the word index that aligns with the lookup table</span>
<span class="sd">        Args:</span>
<span class="sd">            sentences(List[str]): pre-processed posts</span>
<span class="sd">            word_index(dict): mapping between unique word to integer</span>
<span class="sd">        Returns:</span>
<span class="sd">            sent_encoded(List[List[int]]): a list of list containing integer that correspond to each word in a sentece</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="n">len_list</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>
        <span class="n">max_seq_length</span> <span class="o">=</span> <span class="mi">128</span> <span class="c1">#max(len_list) #max length for all input sequence / post</span>
        
        <span class="n">sent_encoded</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
            <span class="c1"># if in then add idx, else integer for the unknown token</span>
            <span class="n">temp_encoded</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_index</span> <span class="k">else</span> <span class="n">word_index</span><span class="p">[</span><span class="s1">&#39;[UNKNOWN]&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">]</span> 
            <span class="c1"># if len is less than pre-defined, add pad token until same len</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">temp_encoded</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">max_seq_length</span><span class="p">:</span>
                <span class="n">temp_encoded</span> <span class="o">+=</span> <span class="p">[</span><span class="n">word_index</span><span class="p">[</span><span class="s1">&#39;[PAD]&#39;</span><span class="p">]]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_seq_length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">temp_encoded</span><span class="p">))</span> 
            <span class="c1"># exceed. then cutoff</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">temp_encoded</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_seq_length</span><span class="p">:</span>
                <span class="n">temp_encoded</span> <span class="o">=</span> <span class="n">temp_encoded</span><span class="p">[:</span><span class="n">max_seq_length</span><span class="p">]</span>
            <span class="n">sent_encoded</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp_encoded</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sent_encoded</span>

<span class="n">Prep</span> <span class="o">=</span> <span class="n">Preprocessing</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="class-word-embedding">
<h2>class - Word_embedding<a class="headerlink" href="#class-word-embedding" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Word_embedding</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    
    <span class="c1">############################### 2.1 (2): Build training model for word embeddings  #########################  </span>
  
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Build training model for word embeddings: You are to build a training model for word embeddings. </span>
<span class="sd">    You are required to articulate the hyperparameters [Lab2] you choose (dimension of embeddings and window size) </span>
<span class="sd">    in Section 4.1. Note that any word embeddings model [Lab2] (e.g. word2vec-CBOW, word2vec-Skip gram, fasttext, glove) </span>
<span class="sd">    can be applied. (Section 4.1. and Section 4.3., Justify your decision)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">make_self_trained_gensim_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">posts_</span><span class="p">,</span> <span class="n">dimension_</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">window_</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">which_</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Type</span><span class="p">[</span><span class="n">Word2Vec</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;contruct gensim word2vec model</span>
<span class="sd">        Args:</span>
<span class="sd">            posts_(List[str]): a list of string: either F - Feeler or T - Thinker</span>
<span class="sd">        Returns:</span>
<span class="sd">            (gensim.model.Word2Vec): gensim API to create word2vec model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">#TODO: params</span>
        <span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
            <span class="mi">1</span><span class="p">:</span> <span class="n">FastText</span><span class="p">,</span>
            <span class="mi">2</span><span class="p">:</span> <span class="n">Word2Vec</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">models</span><span class="p">[</span><span class="n">which_</span><span class="p">](</span><span class="n">sentences</span><span class="o">=</span><span class="n">posts_</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">dimension_</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="n">window_</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1">############################################### 2.2  Pretrained Word Embedding ##################################################</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    You are asked to extract and apply the pretrained word embedding. </span>
<span class="sd">    Gensim provides several pretrained word embeddings, you can find those in the gensim github. </span>
<span class="sd">    You can select the pretrained word embedding that would be useful for the assignment 1 task, </span>
<span class="sd">    personality type classification.(Section 4.3., Justify your decision)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1">######### load gensim pretrain model #########</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pre trained model based on link here: https://github.com/RaRe-Technologies/gensim-data#models</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">load_pre_train_gensim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">which_</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">object</span><span class="p">:</span>
        <span class="c1"># load the pretrained embedding</span>
        <span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
            <span class="mi">1</span><span class="p">:</span><span class="s1">&#39;glove-twitter-25&#39;</span><span class="p">,</span>
            <span class="mi">2</span><span class="p">:</span> <span class="s1">&#39;glove-wiki-gigaword-100&#39;</span><span class="p">,</span>
            <span class="c1"># 3: &#39;fasttext-wiki-news-subwords-300&#39;</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">api</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">models</span><span class="p">[</span><span class="n">which_</span><span class="p">])</span> <span class="c1"># NOTE: Download an embedding other than glove-twitter-255</span>

    <span class="c1">######### build_vocab #########</span>
    <span class="k">def</span> <span class="nf">build_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">docs_</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot; build vocabulary based on given dataset</span>
<span class="sd">        Args:</span>
<span class="sd">            docs_: list of documents that contains string of sentence post</span>
<span class="sd">        Returns:</span>
<span class="sd">            vocab with key as unique word, value as its mapping integer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">word_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span> 
        <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">docs_</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">:</span>
                <span class="n">word_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="n">word_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">&#39;[PAD]&#39;</span><span class="p">)</span>
        <span class="n">word_set</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="s1">&#39;[UNKNOWN]&#39;</span><span class="p">)</span>

        <span class="n">word_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">word_set</span><span class="p">)</span> 
        <span class="n">word_list</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span> <span class="c1"># just to make sure order persistent</span>

        <span class="n">word_index</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_list</span><span class="p">:</span>
            <span class="n">word_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">ind</span>
            <span class="n">ind</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">word_list</span><span class="p">,</span> <span class="n">word_index</span>

    <span class="c1">############################################### 2.3 Input Concatenation ##################################################</span>
    <span class="k">def</span> <span class="nf">build_concat_embed_table</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word_list_</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">word_emb_models</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="nb">object</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot; Build the embedding table from the pre-train model.</span>
<span class="sd">        Extract those word embedding that exists in the vocab of the current dataset</span>
<span class="sd">        Args:</span>
<span class="sd">            word_list_(list): a list of unique word in the corpus</span>
<span class="sd">        Returns:</span>
<span class="sd">            word_emb_models(List[gensim.model]): a list of gensim models</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">emb_dim</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">word_emb_models</span><span class="p">:</span>
            <span class="n">emb_dim</span> <span class="o">+=</span> <span class="n">m</span><span class="o">.</span><span class="n">vector_size</span>

        <span class="n">emb_table</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_list_</span><span class="p">):</span>
            <span class="n">is_existed</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># initially, assume word does not exist in any embedding </span>
            <span class="n">concat</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">word</span> <span class="ow">in</span> <span class="n">w_emb</span> <span class="k">for</span> <span class="n">w_emb</span> <span class="ow">in</span> <span class="n">word_emb_models</span><span class="p">):</span> <span class="c1"># must all in to ensure shape consistent </span>
                <span class="k">for</span> <span class="n">w_emb</span> <span class="ow">in</span> <span class="n">word_emb_models</span><span class="p">:</span> <span class="c1"># concat all</span>
                    <span class="k">if</span> <span class="n">concat</span> <span class="o">==</span> <span class="p">[]:</span> <span class="c1"># when empty initalise</span>
                        <span class="n">concat</span> <span class="o">=</span> <span class="n">w_emb</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">concat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">concat</span><span class="p">,</span> <span class="n">w_emb</span><span class="p">[</span><span class="n">word</span><span class="p">]),</span> <span class="mi">0</span><span class="p">)</span>
                <span class="n">emb_table</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">concat</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># The pretrained glove twitter does not contain the embeddings for the [PAD] and [UNKNOWN] tokens we defined</span>
                <span class="c1"># Here, we just use all 0 for both [PAD] and [UNKNOWN] tokens for simplicity</span>
                <span class="c1">#TODO</span>
                <span class="n">emb_table</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">emb_dim</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">emb_table</span><span class="p">),</span> <span class="n">emb_dim</span>
<span class="n">Emb</span> <span class="o">=</span> <span class="n">Word_embedding</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="class-train">
<h2>class - Train<a class="headerlink" href="#class-train" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">############################################### 3.2. Train Sequence Model (Bi-directional model) ###############################################</span>
<span class="k">class</span> <span class="nc">Train_model</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    <span class="k">def</span> <span class="nf">train_sequence_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train_</span><span class="p">,</span> <span class="n">y_train_</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span><span class="nb">dict</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">,</span> <span class="n">emb_table</span><span class="p">,</span> <span class="n">which_</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="p">[]):</span>
        <span class="sd">&quot;&quot;&quot;train the sequence model&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">which_</span><span class="p">)</span> <span class="o">==</span> <span class="nb">int</span><span class="p">,</span> <span class="s1">&#39;type(which_) == int&#39;</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span> <span class="o">==</span> <span class="nb">list</span><span class="p">,</span> <span class="s1">&#39;type(test_data) == list&#39;</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">emb_dim</span><span class="p">)</span> <span class="o">==</span> <span class="nb">int</span><span class="p">,</span> <span class="s1">&#39;type(emb_dim) == int&#39;</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">params</span>
        <span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_class</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">lr_</span><span class="p">,</span> <span class="n">n_epoch</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="s1">&#39;vocab_size&#39;</span><span class="p">],</span> <span class="n">p</span><span class="p">[</span><span class="s1">&#39;n_class&#39;</span><span class="p">],</span> <span class="n">p</span><span class="p">[</span><span class="s1">&#39;n_hidden&#39;</span><span class="p">],</span> <span class="n">p</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">],</span> <span class="n">p</span><span class="p">[</span><span class="s1">&#39;n_epoch&#39;</span><span class="p">]</span>

        <span class="n">model</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Move the model to GPU</span>
        <span class="k">if</span> <span class="n">which_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">Bi_LSTM_Emb</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_class</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">,</span> <span class="n">emb_table</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">which_</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">Bi_GRU</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_class</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">,</span> <span class="n">emb_table</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">which_</span> <span class="o">==</span><span class="mi">3</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">Bi_RNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_class</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">,</span> <span class="n">emb_table</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Loss function and optimizer</span>
        <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr_</span><span class="p">)</span>
        
        <span class="n">plot</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Parameters</span>
        <span class="n">params_dataset</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
                <span class="s1">&#39;shuffle&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                <span class="s1">&#39;num_workers&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>

        <span class="c1"># Generators</span>
        <span class="n">X_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X_train_</span><span class="p">))</span>
        <span class="n">y_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_train_</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># view(-1) because it is pass in as a list</span>
        
        <span class="n">training_set</span> <span class="o">=</span> <span class="n">CustomDataSet</span><span class="p">(</span><span class="n">X_torch</span><span class="p">,</span> <span class="n">y_torch</span><span class="p">)</span>
        <span class="n">training_generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">training_set</span><span class="p">,</span> <span class="o">**</span><span class="n">params_dataset</span><span class="p">)</span>
        
        <span class="c1"># Loop over epochs</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epoch</span><span class="p">):</span>
            <span class="c1"># Training</span>
            <span class="k">for</span> <span class="n">local_batch</span><span class="p">,</span> <span class="n">local_labels</span> <span class="ow">in</span> <span class="n">training_generator</span><span class="p">:</span>
                <span class="c1"># Transfer to GPU</span>
                <span class="n">local_batch</span><span class="p">,</span> <span class="n">local_labels</span> <span class="o">=</span> <span class="n">local_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">local_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># Model computations</span>
            
            <span class="c1"># Set the flag to training</span>
            <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
            
            <span class="c1"># forward + backward + optimize</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">local_batch</span><span class="p">)</span> 
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">local_labels</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            
            <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>            
            
            <span class="n">acc</span><span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">predicted</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">local_labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch: </span><span class="si">%d</span><span class="s1">, loss: </span><span class="si">%.5f</span><span class="s1">, train_acc: </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">acc</span><span class="p">))</span>
            
            <span class="k">if</span> <span class="n">test_data</span> <span class="o">!=</span> <span class="p">[]</span> <span class="ow">and</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">epoch</span> <span class="o">==</span> <span class="n">n_epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">eval_tmp</span> <span class="o">=</span> <span class="n">Instrinsic_eval</span><span class="p">()</span>
                <span class="n">f1</span> <span class="o">=</span> <span class="n">eval_tmp</span><span class="o">.</span><span class="n">eval_f1_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">test_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">plot</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">epoch</span><span class="p">,</span> <span class="n">f1</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">test_data</span> <span class="o">!=</span> <span class="p">[]:</span>
            <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">plot</span>
        <span class="k">return</span> <span class="n">model</span>
        
        <span class="c1">#####################################################################################</span>
<span class="n">Train</span> <span class="o">=</span> <span class="n">Train_model</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="class-instrinsic-eval">
<h2>class - instrinsic eval<a class="headerlink" href="#class-instrinsic-eval" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="k">class</span> <span class="nc">Instrinsic_eval</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">eval_f1_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_</span><span class="p">,</span> <span class="n">X_test_</span><span class="p">,</span> <span class="n">y_test_</span><span class="p">,</span> <span class="n">is_report</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">model_</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">input_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X_test_</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model_</span><span class="p">(</span><span class="n">input_torch</span><span class="p">)</span> 
        <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_report</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;F1 score: &#39;</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test_</span><span class="p">,</span> <span class="n">predicted</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test_</span><span class="p">,</span><span class="n">predicted</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
            <span class="k">return</span>
        <span class="k">return</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test_</span><span class="p">,</span> <span class="n">predicted</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>

    <span class="c1">############################################### 4.1. instrinsic evaluation ###############################################</span>

    <span class="k">def</span> <span class="nf">instrinsic_eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span><span class="nb">str</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This is for the model that is trained</span>
<span class="sd">        The example code is provided here - Word Embedding Intrinsic Evaluation</span>
<span class="sd">        ou also are to visualise the result (the example can be found in the Table 2 and Figure 2 from the Original GloVe Paper).</span>
<span class="sd">        code: https://colab.research.google.com/drive/1VdNkQpeI6iLPHeTsGe6sdHQFcGyV1Kmi?usp=sharing#scrollTo=n-LHSUrn7IKh</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1">################# </span>
        <span class="c1"># Download Google Analogy Test Set questions</span>
        

        <span class="c1">################# </span>
        <span class="c1"># The following code will save the trained embedding vectors in a text format.</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        9: 9 vectors, 4: dimension of 4</span>
<span class="sd">        Below is actual file content for the first few lines</span>

<span class="sd">        ```txt</span>
<span class="sd">        9 4</span>
<span class="sd">        word1 0.123 0.134 0.532 0.152</span>
<span class="sd">        word2 0.934 0.412 0.532 0.159</span>
<span class="sd">        ```</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1">#################</span>
        <span class="c1"># Below based on evaluation code in stanfordnlp glove github</span>
        <span class="c1"># Basic get embedding for later intrinsic evalution</span>
        <span class="n">vectors_file</span><span class="o">=</span><span class="n">path</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">vectors_file</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">vectors</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]:</span> <span class="c1"># we only need the embedding vectors starting from the second line </span>
                <span class="n">vals</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
                <span class="n">vectors</span><span class="p">[</span><span class="n">vals</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">vals</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>

        <span class="c1">#################</span>
        <span class="c1"># convert the format to desire mapping</span>
        <span class="n">vocab_words</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">vectors</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab_words</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocab size: &quot;</span><span class="p">,</span><span class="nb">str</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">))</span>

        <span class="c1"># create word-&gt;index and index-&gt;word converter</span>
        <span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab_words</span><span class="p">)}</span>
        <span class="n">ivocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">w</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab_words</span><span class="p">)}</span>
        <span class="c1">#################</span>
        <span class="c1"># create matrix &amp; normalisation</span>

        <span class="c1"># create the embedding matrix of shape (vocab_size, dim)</span>
        <span class="n">vector_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vectors</span><span class="p">[</span><span class="n">ivocab</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">vector_dim</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vectors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">word</span> <span class="o">==</span> <span class="s1">&#39;&lt;unk&gt;&#39;</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">W</span><span class="p">[</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">],</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">v</span>
        <span class="c1">#################</span>
        <span class="c1"># normalize each word vector to unit length</span>
        <span class="c1"># Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.</span>
        <span class="n">W_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
        <span class="n">W_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="c1">#################</span>

        <span class="n">correct_sem</span><span class="p">,</span> <span class="n">correct_syn</span><span class="p">,</span> <span class="n">correct_tot</span><span class="p">,</span> <span class="n">count_sem</span><span class="p">,</span> <span class="n">count_syn</span><span class="p">,</span> <span class="n">count_tot</span><span class="p">,</span> <span class="n">full_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_vectors</span><span class="p">(</span><span class="n">W_norm</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;GloVe/eval/question-data&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Questions seen/total: </span><span class="si">%.2f%%</span><span class="s1"> (</span><span class="si">%d</span><span class="s1">/</span><span class="si">%d</span><span class="s1">)&#39;</span> <span class="o">%</span>
            <span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">count_tot</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">full_count</span><span class="p">),</span> <span class="n">count_tot</span><span class="p">,</span> <span class="n">full_count</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Semantic accuracy: </span><span class="si">%.2f%%</span><span class="s1">  (</span><span class="si">%i</span><span class="s1">/</span><span class="si">%i</span><span class="s1">)&#39;</span> <span class="o">%</span>
            <span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">correct_sem</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">count_sem</span><span class="p">),</span> <span class="n">correct_sem</span><span class="p">,</span> <span class="n">count_sem</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Syntactic accuracy: </span><span class="si">%.2f%%</span><span class="s1">  (</span><span class="si">%i</span><span class="s1">/</span><span class="si">%i</span><span class="s1">)&#39;</span> <span class="o">%</span>
            <span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">correct_syn</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">count_syn</span><span class="p">),</span> <span class="n">correct_syn</span><span class="p">,</span> <span class="n">count_syn</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total accuracy: </span><span class="si">%.2f%%</span><span class="s1">  (</span><span class="si">%i</span><span class="s1">/</span><span class="si">%i</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">correct_tot</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">count_tot</span><span class="p">),</span> <span class="n">correct_tot</span><span class="p">,</span> <span class="n">count_tot</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">evaluate_vectors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;./eval/question-data/&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Evaluate the trained word vectors on a variety of tasks&quot;&quot;&quot;</span>

        <span class="n">filenames</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s1">&#39;capital-common-countries.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;capital-world.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;currency.txt&#39;</span><span class="p">,</span>
            <span class="s1">&#39;city-in-state.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;family.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;gram1-adjective-to-adverb.txt&#39;</span><span class="p">,</span>
            <span class="s1">&#39;gram2-opposite.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;gram3-comparative.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;gram4-superlative.txt&#39;</span><span class="p">,</span>
            <span class="s1">&#39;gram5-present-participle.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;gram6-nationality-adjective.txt&#39;</span><span class="p">,</span>
            <span class="s1">&#39;gram7-past-tense.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;gram8-plural.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;gram9-plural-verbs.txt&#39;</span><span class="p">,</span>
            <span class="p">]</span>

        <span class="c1"># to avoid memory overflow, could be increased/decreased</span>
        <span class="c1"># depending on system and vocab size</span>
        <span class="n">split_size</span> <span class="o">=</span> <span class="mi">100</span>

        <span class="n">correct_sem</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1"># count correct semantic questions</span>
        <span class="n">correct_syn</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1"># count correct syntactic questions</span>
        <span class="n">correct_tot</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># count correct questions</span>
        <span class="n">count_sem</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1"># count all semantic questions</span>
        <span class="n">count_syn</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1"># count all syntactic questions</span>
        <span class="n">count_tot</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># count all questions</span>
        <span class="n">full_count</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># count all questions, including those with unknown words</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">filenames</span><span class="p">)):</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">/</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">filenames</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">full_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>
                <span class="n">full_count</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">full_data</span><span class="p">)</span>
                <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">full_data</span> <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">word</span> <span class="ow">in</span> <span class="n">vocab</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">x</span><span class="p">)]</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ERROR: no lines of vocab kept for </span><span class="si">%s</span><span class="s2"> !&quot;</span> <span class="o">%</span> <span class="n">filenames</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Example missing line:&quot;</span><span class="p">,</span> <span class="n">full_data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="k">continue</span>

            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">row</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">data</span><span class="p">])</span>
            <span class="n">ind1</span><span class="p">,</span> <span class="n">ind2</span><span class="p">,</span> <span class="n">ind3</span><span class="p">,</span> <span class="n">ind4</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">T</span>

            <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">),))</span>
            <span class="n">num_iter</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">split_size</span><span class="p">)))</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iter</span><span class="p">):</span>
                <span class="n">subset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">j</span><span class="o">*</span><span class="n">split_size</span><span class="p">,</span> <span class="nb">min</span><span class="p">((</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">split_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ind1</span><span class="p">)))</span>

                <span class="n">pred_vec</span> <span class="o">=</span> <span class="p">(</span><span class="n">W</span><span class="p">[</span><span class="n">ind2</span><span class="p">[</span><span class="n">subset</span><span class="p">],</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">W</span><span class="p">[</span><span class="n">ind1</span><span class="p">[</span><span class="n">subset</span><span class="p">],</span> <span class="p">:]</span>
                    <span class="o">+</span>  <span class="n">W</span><span class="p">[</span><span class="n">ind3</span><span class="p">[</span><span class="n">subset</span><span class="p">],</span> <span class="p">:])</span>

                <span class="c1">#cosine similarity if input W has been normalized</span>
                <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">pred_vec</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>


                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">subset</span><span class="p">)):</span>
                    <span class="n">dist</span><span class="p">[</span><span class="n">ind1</span><span class="p">[</span><span class="n">subset</span><span class="p">[</span><span class="n">k</span><span class="p">]],</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">Inf</span>
                    <span class="n">dist</span><span class="p">[</span><span class="n">ind2</span><span class="p">[</span><span class="n">subset</span><span class="p">[</span><span class="n">k</span><span class="p">]],</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">Inf</span>
                    <span class="n">dist</span><span class="p">[</span><span class="n">ind3</span><span class="p">[</span><span class="n">subset</span><span class="p">[</span><span class="n">k</span><span class="p">]],</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">Inf</span>

                <span class="c1"># predicted word index</span>
                <span class="n">predictions</span><span class="p">[</span><span class="n">subset</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

            
            <span class="n">val</span> <span class="o">=</span> <span class="p">(</span><span class="n">ind4</span> <span class="o">==</span> <span class="n">predictions</span><span class="p">)</span> <span class="c1"># correct predictions</span>
            <span class="n">count_tot</span> <span class="o">=</span> <span class="n">count_tot</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">ind1</span><span class="p">)</span>
            <span class="n">correct_tot</span> <span class="o">=</span> <span class="n">correct_tot</span> <span class="o">+</span> <span class="nb">sum</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">:</span>
                <span class="n">count_sem</span> <span class="o">=</span> <span class="n">count_sem</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">ind1</span><span class="p">)</span>
                <span class="n">correct_sem</span> <span class="o">=</span> <span class="n">correct_sem</span> <span class="o">+</span> <span class="nb">sum</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">count_syn</span> <span class="o">=</span> <span class="n">count_syn</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">ind1</span><span class="p">)</span>
                <span class="n">correct_syn</span> <span class="o">=</span> <span class="n">correct_syn</span> <span class="o">+</span> <span class="nb">sum</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>

            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">:&quot;</span> <span class="o">%</span> <span class="n">filenames</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY TOP1: </span><span class="si">%.2f%%</span><span class="s1"> (</span><span class="si">%d</span><span class="s1">/</span><span class="si">%d</span><span class="s1">)&#39;</span> <span class="o">%</span>
                <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">val</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">val</span><span class="p">)))</span>
            
        <span class="k">return</span> <span class="n">correct_sem</span><span class="p">,</span> <span class="n">correct_syn</span><span class="p">,</span> <span class="n">correct_tot</span><span class="p">,</span> <span class="n">count_sem</span><span class="p">,</span> <span class="n">count_syn</span><span class="p">,</span> <span class="n">count_tot</span><span class="p">,</span> <span class="n">full_count</span>
<span class="n">Eval</span> <span class="o">=</span> <span class="n">Instrinsic_eval</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="preprocess-combo-compare">
<h2>4.1 preprocess combo compare<a class="headerlink" href="#preprocess-combo-compare" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="c1"># Dimension feature</span>
<span class="n">dimension_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">]</span>
<span class="n">d_semantic_list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">12.75</span><span class="p">,</span> <span class="mf">12.42</span><span class="p">,</span> <span class="mf">8.72</span><span class="p">]</span>
<span class="n">d_syntactic_list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">48.41</span><span class="p">,</span> <span class="mf">50.86</span><span class="p">,</span> <span class="mf">51.39</span><span class="p">]</span>
<span class="n">d_overal_list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">43.88</span><span class="p">,</span> <span class="mf">45.97</span><span class="p">,</span> <span class="mf">45.97</span><span class="p">]</span>

<span class="c1"># Window size </span>
<span class="n">window_size_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">w_semantic_list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">10.74</span><span class="p">,</span> <span class="mf">12.42</span><span class="p">,</span> <span class="mf">14.77</span><span class="p">,</span> <span class="mf">15.44</span><span class="p">]</span>
<span class="n">w_syntactic_list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">55.31</span><span class="p">,</span> <span class="mf">50.86</span><span class="p">,</span> <span class="mf">47.33</span><span class="p">,</span> <span class="mf">42.25</span><span class="p">]</span>
<span class="n">w_overal_list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">49.64</span><span class="p">,</span> <span class="mf">45.97</span><span class="p">,</span> <span class="mf">43.19</span><span class="p">,</span> <span class="mf">38.84</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">plot_multi_lines</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_label</span><span class="p">,</span> <span class="n">semantic_list_</span><span class="p">,</span> <span class="n">syntactic_list_</span><span class="p">,</span> <span class="n">overal_list_</span><span class="p">):</span>

    <span class="n">labels</span>  <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Semantic&#39;</span><span class="p">:</span> <span class="n">semantic_list_</span><span class="p">,</span> 
               <span class="s1">&#39;Syntactic&#39;</span><span class="p">:</span> <span class="n">syntactic_list_</span><span class="p">,</span> 
               <span class="s1">&#39;Overall&#39;</span><span class="p">:</span> <span class="n">overal_list_</span>
               <span class="p">}</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
    
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">labels</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="c1"># text(x, y, some_text), where x is last index for each line</span>
        <span class="c1"># add text at end of line (10 is the orginal x-axis label which is chanegd later on)</span>
        
    <span class="c1">#plt.legend(labels, loc=&#39;upper right&#39;)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy [%]&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">x_label</span><span class="p">)</span>
    <span class="c1"># positions = [i for i in range(11)]</span>
    <span class="c1"># x_labels = sizes</span>
    <span class="c1"># plt.xticks(positions, x_labels)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="hyperparams-tuning-data">
<h2>4.5 hyperparams tuning &amp; data<a class="headerlink" href="#hyperparams-tuning-data" title="Permalink to this headline">#</a></h2>
<p>Contain manually save data, and plot function</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
<span class="c1"># Learning rate = 1e-3</span>
<span class="n">lstm_ep_1e_3</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.39722835039723087</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.4423827587975953</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mf">0.42984313547950204</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mf">0.4926813580401134</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mf">0.4992443784462104</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mf">0.5245338268877596</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mf">0.5306104496008943</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">35</span><span class="p">,</span> <span class="mf">0.571148231826966</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mf">0.5858739622895017</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">45</span><span class="p">,</span> <span class="mf">0.5957303518771649</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mf">0.6020399043944912</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">55</span><span class="p">,</span> <span class="mf">0.614119366403292</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mf">0.6152170210349143</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">65</span><span class="p">,</span> <span class="mf">0.625110054864376</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">70</span><span class="p">,</span> <span class="mf">0.6298528726404834</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">75</span><span class="p">,</span> <span class="mf">0.6229937133550588</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">80</span><span class="p">,</span> <span class="mf">0.6287271067130272</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">85</span><span class="p">,</span> <span class="mf">0.627513288903641</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">90</span><span class="p">,</span> <span class="mf">0.63708728564391</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">95</span><span class="p">,</span> <span class="mf">0.6406995533900709</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mf">0.6394967646501325</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">105</span><span class="p">,</span> <span class="mf">0.6496326661647691</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">110</span><span class="p">,</span> <span class="mf">0.6429373137472711</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">115</span><span class="p">,</span> <span class="mf">0.6434909601240305</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">120</span><span class="p">,</span> <span class="mf">0.6334229428466362</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">125</span><span class="p">,</span> <span class="mf">0.6494375198520522</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">130</span><span class="p">,</span> <span class="mf">0.6393020458593476</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">135</span><span class="p">,</span> <span class="mf">0.6460219453556341</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">140</span><span class="p">,</span> <span class="mf">0.6441044978063931</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">145</span><span class="p">,</span> <span class="mf">0.647501483381419</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">149</span><span class="p">,</span> <span class="mf">0.6429745321971257</span><span class="p">]]</span>

<span class="c1"># learning rate = 1e-4 </span>

<span class="n">lstm_ep_1e_4</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.4523876403925913</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.41798941072273454</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mf">0.41216740632084864</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mf">0.40267291445184034</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mf">0.40024746705331604</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mf">0.4020764254516451</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mf">0.41154427747616645</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">35</span><span class="p">,</span> <span class="mf">0.4109207172903444</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mf">0.4162233872394977</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">45</span><span class="p">,</span> <span class="mf">0.41559035667904637</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mf">0.42022412401525167</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">55</span><span class="p">,</span> <span class="mf">0.42806439409999625</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mf">0.4341450946228735</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">65</span><span class="p">,</span> <span class="mf">0.44536590124890546</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">70</span><span class="p">,</span> <span class="mf">0.4432629830791194</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">75</span><span class="p">,</span> <span class="mf">0.44109912579157584</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">80</span><span class="p">,</span> <span class="mf">0.4444444470026792</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">85</span><span class="p">,</span> <span class="mf">0.44138306405150607</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">90</span><span class="p">,</span> <span class="mf">0.4429774411197592</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">95</span><span class="p">,</span> <span class="mf">0.45055341441534363</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mf">0.4559701893042565</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">105</span><span class="p">,</span> <span class="mf">0.4707978418741552</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">110</span><span class="p">,</span> <span class="mf">0.478475389974241</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">115</span><span class="p">,</span> <span class="mf">0.49025470500383994</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">120</span><span class="p">,</span> <span class="mf">0.49206442773521886</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">125</span><span class="p">,</span> <span class="mf">0.5004443663505393</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">130</span><span class="p">,</span> <span class="mf">0.500165534942555</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">135</span><span class="p">,</span> <span class="mf">0.507110374240826</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">140</span><span class="p">,</span> <span class="mf">0.5143752540607812</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">145</span><span class="p">,</span> <span class="mf">0.5175457304692035</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">149</span><span class="p">,</span> <span class="mf">0.5229774691199987</span><span class="p">]]</span>
<span class="c1"># hidden = 32</span>
<span class="n">lstm_1e_3_batch_64</span> <span class="o">=</span> \
<span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.47919457279822786</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.4999697266630849</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mf">0.4437178192414033</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mf">0.4060926393619775</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mf">0.42763861868516967</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mf">0.43123462199570695</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mf">0.4352723383568304</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">35</span><span class="p">,</span> <span class="mf">0.4275300852110676</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mf">0.40717003933262425</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">45</span><span class="p">,</span> <span class="mf">0.45241747266283433</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mf">0.5476665515163629</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">55</span><span class="p">,</span> <span class="mf">0.5432185015283442</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mf">0.5206169339788286</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">65</span><span class="p">,</span> <span class="mf">0.4913033173933436</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">70</span><span class="p">,</span> <span class="mf">0.4860421038899289</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">75</span><span class="p">,</span> <span class="mf">0.5014581815322253</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">80</span><span class="p">,</span> <span class="mf">0.5666760980828982</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">85</span><span class="p">,</span> <span class="mf">0.5708434242983643</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">90</span><span class="p">,</span> <span class="mf">0.5322840828415608</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">95</span><span class="p">,</span> <span class="mf">0.5022165915094046</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mf">0.4844128672411702</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">105</span><span class="p">,</span> <span class="mf">0.5573522709090807</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">110</span><span class="p">,</span> <span class="mf">0.5067051657120623</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">115</span><span class="p">,</span> <span class="mf">0.5501165691958634</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">120</span><span class="p">,</span> <span class="mf">0.5108916609826263</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">125</span><span class="p">,</span> <span class="mf">0.4970844546969114</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">130</span><span class="p">,</span> <span class="mf">0.55402462658386</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">135</span><span class="p">,</span> <span class="mf">0.5500895683645591</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">140</span><span class="p">,</span> <span class="mf">0.5622822792517499</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">145</span><span class="p">,</span> <span class="mf">0.5893549136573506</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">150</span><span class="p">,</span> <span class="mf">0.6031532831116099</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">155</span><span class="p">,</span> <span class="mf">0.5918066302378698</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">160</span><span class="p">,</span> <span class="mf">0.5981896844845174</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">165</span><span class="p">,</span> <span class="mf">0.5766750018430263</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">170</span><span class="p">,</span> <span class="mf">0.5762285367850134</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">175</span><span class="p">,</span> <span class="mf">0.5434906977811584</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">180</span><span class="p">,</span> <span class="mf">0.5994304010227302</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">185</span><span class="p">,</span> <span class="mf">0.4618307978991708</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">190</span><span class="p">,</span> <span class="mf">0.5375042743652012</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">195</span><span class="p">,</span> <span class="mf">0.5560940598700497</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mf">0.5397791734388953</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">205</span><span class="p">,</span> <span class="mf">0.5755286770959477</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">210</span><span class="p">,</span> <span class="mf">0.6077266541580119</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">215</span><span class="p">,</span> <span class="mf">0.5274067733430385</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">220</span><span class="p">,</span> <span class="mf">0.6164755059460085</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">225</span><span class="p">,</span> <span class="mf">0.6127152635016919</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">230</span><span class="p">,</span> <span class="mf">0.6101742665256591</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">235</span><span class="p">,</span> <span class="mf">0.5875667544653554</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">240</span><span class="p">,</span> <span class="mf">0.6093183047595814</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">245</span><span class="p">,</span> <span class="mf">0.6212028562138994</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">250</span><span class="p">,</span> <span class="mf">0.6300481356301687</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">255</span><span class="p">,</span> <span class="mf">0.6199111687725465</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">260</span><span class="p">,</span> <span class="mf">0.627022272083034</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">265</span><span class="p">,</span> <span class="mf">0.6296538958237122</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">270</span><span class="p">,</span> <span class="mf">0.628831600086741</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">275</span><span class="p">,</span> <span class="mf">0.6277966658762477</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">280</span><span class="p">,</span> <span class="mf">0.64487174574721</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">285</span><span class="p">,</span> <span class="mf">0.5899821610302591</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">290</span><span class="p">,</span> <span class="mf">0.5835005748221509</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">295</span><span class="p">,</span> <span class="mf">0.6360361057221893</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">300</span><span class="p">,</span> <span class="mf">0.6176269585921745</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">305</span><span class="p">,</span> <span class="mf">0.6230845432513964</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">310</span><span class="p">,</span> <span class="mf">0.6304356331227516</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">315</span><span class="p">,</span> <span class="mf">0.6509565613517861</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">320</span><span class="p">,</span> <span class="mf">0.6446956364968478</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">325</span><span class="p">,</span> <span class="mf">0.6463484580978437</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">330</span><span class="p">,</span> <span class="mf">0.6492676704067548</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">335</span><span class="p">,</span> <span class="mf">0.5474612718328334</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">340</span><span class="p">,</span> <span class="mf">0.633760598937994</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">345</span><span class="p">,</span> <span class="mf">0.6171299893677721</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">350</span><span class="p">,</span> <span class="mf">0.6082293663472247</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">355</span><span class="p">,</span> <span class="mf">0.6088397542731745</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">360</span><span class="p">,</span> <span class="mf">0.6297542348292059</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">365</span><span class="p">,</span> <span class="mf">0.639434010664856</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">370</span><span class="p">,</span> <span class="mf">0.6346010004083809</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">375</span><span class="p">,</span> <span class="mf">0.634195072219936</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">380</span><span class="p">,</span> <span class="mf">0.6244746056913296</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">385</span><span class="p">,</span> <span class="mf">0.5886745413815772</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">390</span><span class="p">,</span> <span class="mf">0.6204238568717076</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">395</span><span class="p">,</span> <span class="mf">0.5742709887048895</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">400</span><span class="p">,</span> <span class="mf">0.5919894189102074</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">405</span><span class="p">,</span> <span class="mf">0.6393414603523793</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">410</span><span class="p">,</span> <span class="mf">0.6429767389677897</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">415</span><span class="p">,</span> <span class="mf">0.6271375615342775</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">420</span><span class="p">,</span> <span class="mf">0.6257825486465812</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">425</span><span class="p">,</span> <span class="mf">0.6319611256598014</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">430</span><span class="p">,</span> <span class="mf">0.6417103775465354</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">435</span><span class="p">,</span> <span class="mf">0.6417406204183367</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">440</span><span class="p">,</span> <span class="mf">0.6436901484296242</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">445</span><span class="p">,</span> <span class="mf">0.647611226584176</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">450</span><span class="p">,</span> <span class="mf">0.6064711179195017</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">455</span><span class="p">,</span> <span class="mf">0.6375820113643694</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">460</span><span class="p">,</span> <span class="mf">0.6092058460177848</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">465</span><span class="p">,</span> <span class="mf">0.6046497883040685</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">470</span><span class="p">,</span> <span class="mf">0.6444287254355036</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">475</span><span class="p">,</span> <span class="mf">0.6352424770755322</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">480</span><span class="p">,</span> <span class="mf">0.6070902217095989</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">485</span><span class="p">,</span> <span class="mf">0.6488359442605428</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">490</span><span class="p">,</span> <span class="mf">0.6466099817117477</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">495</span><span class="p">,</span> <span class="mf">0.6430978962186006</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">499</span><span class="p">,</span> <span class="mf">0.6309111880046137</span><span class="p">]]</span>


<span class="n">lstm_1e_4_batch_64</span> <span class="o">=</span> \
 <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.4934724377436604</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.5180258889824689</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mf">0.5004939344906022</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mf">0.47749057190622457</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mf">0.4555492074192456</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mf">0.43609022816061427</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mf">0.4235088743427147</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">35</span><span class="p">,</span> <span class="mf">0.4219576124190175</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mf">0.42046912313755896</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">45</span><span class="p">,</span> <span class="mf">0.41212690673165714</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mf">0.41458071141031466</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">55</span><span class="p">,</span> <span class="mf">0.42984313547950204</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mf">0.45092799499524405</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">65</span><span class="p">,</span> <span class="mf">0.4543528441152135</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">70</span><span class="p">,</span> <span class="mf">0.4578802783267923</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">75</span><span class="p">,</span> <span class="mf">0.46683641425697037</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">80</span><span class="p">,</span> <span class="mf">0.46246552286739256</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">85</span><span class="p">,</span> <span class="mf">0.4594424408102081</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">90</span><span class="p">,</span> <span class="mf">0.4489174214103962</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">95</span><span class="p">,</span> <span class="mf">0.44430802510663275</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mf">0.44589701961991596</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">105</span><span class="p">,</span> <span class="mf">0.43966930695501494</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">110</span><span class="p">,</span> <span class="mf">0.43972365748830844</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">115</span><span class="p">,</span> <span class="mf">0.439000631217474</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">120</span><span class="p">,</span> <span class="mf">0.4446256933315248</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">125</span><span class="p">,</span> <span class="mf">0.44113553936052713</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">130</span><span class="p">,</span> <span class="mf">0.4234285242777012</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">135</span><span class="p">,</span> <span class="mf">0.42025290761107936</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">140</span><span class="p">,</span> <span class="mf">0.42025290761107936</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">145</span><span class="p">,</span> <span class="mf">0.4234285242777012</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">150</span><span class="p">,</span> <span class="mf">0.43685820610153053</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">155</span><span class="p">,</span> <span class="mf">0.4354617884910907</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">160</span><span class="p">,</span> <span class="mf">0.43107120798212467</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">165</span><span class="p">,</span> <span class="mf">0.4303852967561496</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">170</span><span class="p">,</span> <span class="mf">0.43116990515306125</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">175</span><span class="p">,</span> <span class="mf">0.43689397899077453</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">180</span><span class="p">,</span> <span class="mf">0.4424709604202399</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">185</span><span class="p">,</span> <span class="mf">0.44965887051000225</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">190</span><span class="p">,</span> <span class="mf">0.44817575436865387</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">195</span><span class="p">,</span> <span class="mf">0.45068621666180647</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mf">0.4662512425944868</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">205</span><span class="p">,</span> <span class="mf">0.4758546632906944</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">210</span><span class="p">,</span> <span class="mf">0.48109611665778756</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">215</span><span class="p">,</span> <span class="mf">0.48500343192363543</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">220</span><span class="p">,</span> <span class="mf">0.4929154063649951</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">225</span><span class="p">,</span> <span class="mf">0.4920579542082748</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">230</span><span class="p">,</span> <span class="mf">0.5044519741542047</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">235</span><span class="p">,</span> <span class="mf">0.5110127760203719</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">240</span><span class="p">,</span> <span class="mf">0.5215280367725184</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">245</span><span class="p">,</span> <span class="mf">0.5215280367725184</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">250</span><span class="p">,</span> <span class="mf">0.5173569791024616</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">255</span><span class="p">,</span> <span class="mf">0.5020251502069153</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">260</span><span class="p">,</span> <span class="mf">0.4945809077827374</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">265</span><span class="p">,</span> <span class="mf">0.47975355128172353</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">270</span><span class="p">,</span> <span class="mf">0.46098495281636764</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">275</span><span class="p">,</span> <span class="mf">0.456311841163198</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">280</span><span class="p">,</span> <span class="mf">0.4494376872300918</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">285</span><span class="p">,</span> <span class="mf">0.44869220569242957</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">290</span><span class="p">,</span> <span class="mf">0.44743386289639747</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">295</span><span class="p">,</span> <span class="mf">0.4546550485205109</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">300</span><span class="p">,</span> <span class="mf">0.4469002683571702</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">305</span><span class="p">,</span> <span class="mf">0.4521836339271986</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">310</span><span class="p">,</span> <span class="mf">0.4558711146634093</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">315</span><span class="p">,</span> <span class="mf">0.4597957085130668</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">320</span><span class="p">,</span> <span class="mf">0.4597957085130668</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">325</span><span class="p">,</span> <span class="mf">0.4716021073303817</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">330</span><span class="p">,</span> <span class="mf">0.48574566245316875</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">335</span><span class="p">,</span> <span class="mf">0.510779856740659</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">340</span><span class="p">,</span> <span class="mf">0.5158002605402944</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">345</span><span class="p">,</span> <span class="mf">0.49631489034784587</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">350</span><span class="p">,</span> <span class="mf">0.4928422445723484</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">355</span><span class="p">,</span> <span class="mf">0.4910337217142292</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">360</span><span class="p">,</span> <span class="mf">0.4901146617816677</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">365</span><span class="p">,</span> <span class="mf">0.4944740919710569</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">370</span><span class="p">,</span> <span class="mf">0.49631489034784587</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">375</span><span class="p">,</span> <span class="mf">0.5014827773825111</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">380</span><span class="p">,</span> <span class="mf">0.5023257524274365</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">385</span><span class="p">,</span> <span class="mf">0.4892758821198595</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">390</span><span class="p">,</span> <span class="mf">0.47792436494350476</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">395</span><span class="p">,</span> <span class="mf">0.47975355128172353</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">400</span><span class="p">,</span> <span class="mf">0.47666754306182707</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">405</span><span class="p">,</span> <span class="mf">0.47287996321173603</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">410</span><span class="p">,</span> <span class="mf">0.47367020678605437</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">415</span><span class="p">,</span> <span class="mf">0.47746272503544634</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">420</span><span class="p">,</span> <span class="mf">0.47176525376004025</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">425</span><span class="p">,</span> <span class="mf">0.47746272503544634</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">430</span><span class="p">,</span> <span class="mf">0.4825562075721428</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">435</span><span class="p">,</span> <span class="mf">0.48463707286291613</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">440</span><span class="p">,</span> <span class="mf">0.48619277433859254</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">445</span><span class="p">,</span> <span class="mf">0.4910337217142292</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">450</span><span class="p">,</span> <span class="mf">0.4909533013144025</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">455</span><span class="p">,</span> <span class="mf">0.4888209442908248</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">460</span><span class="p">,</span> <span class="mf">0.49019231422163395</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">465</span><span class="p">,</span> <span class="mf">0.5105100081887007</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">470</span><span class="p">,</span> <span class="mf">0.5322840828415608</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">475</span><span class="p">,</span> <span class="mf">0.5491301797553828</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">480</span><span class="p">,</span> <span class="mf">0.5583154054797947</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">485</span><span class="p">,</span> <span class="mf">0.5638753953697754</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">490</span><span class="p">,</span> <span class="mf">0.5622822792517499</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">495</span><span class="p">,</span> <span class="mf">0.5714936257481199</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">499</span><span class="p">,</span> <span class="mf">0.569475332669558</span><span class="p">]]</span>


<span class="n">plot_res_final</span> <span class="o">=</span> \
<span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.29130924387954726</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.43144516781030867</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mf">0.3921541860475242</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mf">0.39134791526405616</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mf">0.3863003876004812</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mf">0.3989829001039234</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mf">0.5005761324650939</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">35</span><span class="p">,</span> <span class="mf">0.4889974918152105</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mf">0.5105283234417715</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">45</span><span class="p">,</span> <span class="mf">0.5578370336704936</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mf">0.4373743852116897</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">55</span><span class="p">,</span> <span class="mf">0.43875034818122455</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mf">0.5118128937217583</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">65</span><span class="p">,</span> <span class="mf">0.5631041909874451</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">70</span><span class="p">,</span> <span class="mf">0.5617070357554786</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">75</span><span class="p">,</span> <span class="mf">0.5727985998274531</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">80</span><span class="p">,</span> <span class="mf">0.5621413020991778</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">85</span><span class="p">,</span> <span class="mf">0.5169467520823869</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">90</span><span class="p">,</span> <span class="mf">0.4585416615737298</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">95</span><span class="p">,</span> <span class="mf">0.501715153376459</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mf">0.5515223998550054</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">105</span><span class="p">,</span> <span class="mf">0.5575187875661773</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">110</span><span class="p">,</span> <span class="mf">0.5660485658827364</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">115</span><span class="p">,</span> <span class="mf">0.5637135701676076</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">120</span><span class="p">,</span> <span class="mf">0.5704240042951958</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">125</span><span class="p">,</span> <span class="mf">0.5737185885927196</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">130</span><span class="p">,</span> <span class="mf">0.5931486457428043</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">135</span><span class="p">,</span> <span class="mf">0.5850662295530101</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">140</span><span class="p">,</span> <span class="mf">0.6081993242942484</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">145</span><span class="p">,</span> <span class="mf">0.5884633919070942</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">150</span><span class="p">,</span> <span class="mf">0.6044978902711747</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">155</span><span class="p">,</span> <span class="mf">0.5754123715967085</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">160</span><span class="p">,</span> <span class="mf">0.5782906316929962</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">165</span><span class="p">,</span> <span class="mf">0.5864141060558729</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">170</span><span class="p">,</span> <span class="mf">0.5956774458309013</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">175</span><span class="p">,</span> <span class="mf">0.6115744832817899</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">180</span><span class="p">,</span> <span class="mf">0.5996130154837997</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">185</span><span class="p">,</span> <span class="mf">0.6023020337889038</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">190</span><span class="p">,</span> <span class="mf">0.6036617389490718</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">195</span><span class="p">,</span> <span class="mf">0.4860529441233696</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mf">0.5919708939395723</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">205</span><span class="p">,</span> <span class="mf">0.5386011918033841</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">210</span><span class="p">,</span> <span class="mf">0.5934544278037063</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">215</span><span class="p">,</span> <span class="mf">0.5825590764783072</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">220</span><span class="p">,</span> <span class="mf">0.6032295271049596</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">225</span><span class="p">,</span> <span class="mf">0.5855819991734224</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">230</span><span class="p">,</span> <span class="mf">0.5666160783718335</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">235</span><span class="p">,</span> <span class="mf">0.6070644311100708</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">240</span><span class="p">,</span> <span class="mf">0.6028422360442516</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">245</span><span class="p">,</span> <span class="mf">0.5979440107329906</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">249</span><span class="p">,</span> <span class="mf">0.5559555298167024</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">plot_val_history</span><span class="p">(</span><span class="n">plot_data</span><span class="p">,</span> <span class="n">title_</span><span class="p">):</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">plot_data</span><span class="p">:</span>
        <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
    <span class="c1"># create some x data and some integers for the y axis</span>
    <span class="c1"># x = np.array([3,5,2,4])</span>
    <span class="c1"># y = np.arange(4)</span>

    <span class="c1"># plot the data</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,)</span>

    <span class="c1"># # tell matplotlib which yticks to plot </span>
    <span class="c1"># ax.set_yticks([0,1,2,3])</span>

    <span class="c1"># # labelling the yticks according to your list</span>
    <span class="c1"># ax.set_yticklabels([&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;])</span>
    
   
    
    <span class="c1"># for i in range(len(labels)):</span>
    <span class="c1">#     plt.plot(raw_time_list[i])</span>
    <span class="c1">#     plt.text( 10, raw_time_list[i][-1], f&quot;{labels[i]}&quot;) # text(x, y, some_text), where x is last index for each line</span>
    <span class="c1">#     # add text at end of line (10 is the orginal x-axis label which is chanegd later on)</span>
        
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;F1 score&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of epoch&#39;</span><span class="p">)</span>
    <span class="c1"># positions = [i for i in range(11)]</span>
    <span class="c1"># x_labels = sizes</span>
    <span class="c1"># plt.xticks(positions, x_labels)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title_</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="customdataset">
<h2>CustomDataSet<a class="headerlink" href="#customdataset" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># create custom dataset class</span>
<span class="k">class</span> <span class="nc">CustomDataSet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    References:</span>
<span class="sd">    standford: https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span><span class="nb">list</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span><span class="nb">list</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialization&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">text</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">labels</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Denotes the total number of samples&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Generates one sample of data&quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="helper">
<h2>Helper<a class="headerlink" href="#helper" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>from google.colab import drive
drive.mount(&#39;/content/gdrive&#39;)

class Helper:
    
    def __init__(self):
        self.model_path = &#39;/content/gdrive/MyDrive/comp5046/a1/models/&#39;
        self.data_path = &#39;/content/gdrive/MyDrive/comp5046/a1/preprocessed_data/&#39;

    def save_numpy_txt_grive(self, data_, fname:str):
        path = f&#39;{self.data_path}/{fname}.txt&#39;
        np.savetxt(path, data_, fmt=&#39;%.5f&#39;) # float64, emb_table_concat_tmp[emb_table_concat_tmp!=0] gives most result as 5 float number

    def load_local_numpy_txt(self, fname:str):
        path = f&#39;{fname}.txt&#39;
        return np.loadtxt(path, dtype=np.float64)

    def save_json_grive(self, data_, fname:str) -&gt; None:
        data_dict = {&#39;data&#39;: data_}
        # Save data to json file
        path = f&#39;{self.data_path}/{fname}.json&#39;
        with open(path,&#39;w&#39;) as f:
            json.dump(data_dict,f)

    def load_json_grive(self, fname:str) -&gt; dict:
        # Load data from json file
        path = f&#39;{self.data_path}/{fname}.json&#39;
        with open(path,&#39;r&#39;) as f:
            return json.load(f)[&#39;data&#39;]
    
    &quot;&quot;&quot; load and train model: https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html#save-and-load-the-model
    &quot;&quot;&quot;
    def save_model_weight_gdrive(self, model_, model_name:str):
        
        path = f&#39;{self.model_path}/{model_name}.pth&#39;
        torch.save(model_.state_dict(), path)

    # PicklingError: Can&#39;t pickle &lt;class &#39;__main__.Bi_LSTM_Emb&#39;&gt;: it&#39;s not the same object as __main__.Bi_LSTM_Emb
    # def save_entire_model_gdrive(self, model_, model_name:str):
        
    #     path = f&#39;{self.model_path}/{model_name}.pth&#39;
    #     torch.save(model_, path)
       
    # def load_torch_model_gdrive(self, model_, model_name:str):
        
    #     path = f&#39;{self.model_path}/{model_name}.pth&#39;
    #     model_.load_state_dict(torch.load(path))
    #     return model_

    def load_torch_model(self, model_, model_name:str):
        
        path = f&#39;{self.model_path}/{model_name}.pth&#39;
        model_.load_state_dict(torch.load(path))
        return model_

    def download_best_model_gdrive(self):
        # Code to download file into Colaboratory:
        !pip install -U -q PyDrive
        from pydrive.auth import GoogleAuth
        from pydrive.drive import GoogleDrive
        from google.colab import auth
        from oauth2client.client import GoogleCredentials
        # Authenticate and create the PyDrive client.
        auth.authenticate_user()
        gauth = GoogleAuth()
        gauth.credentials = GoogleCredentials.get_application_default()
        drive = GoogleDrive(gauth) 

        id = &#39;18O4CEbINqkdGtXv8ouafHo7jn_9RJsVY&#39;
        downloaded = drive.CreateFile({&#39;id&#39;:id}) 
        downloaded.GetContentFile(best_model_path) 

        id = &#39;1-9QUH0PSCQgacT9vAkXbHD-CP-yjmGBI&#39;
        downloaded = drive.CreateFile({&#39;id&#39;:id}) 
        downloaded.GetContentFile(best_embed_path)         

    def load_best_model(self):

        model_file = Path(best_model_path)
        embed_file = Path(best_model_path)
        PARAMS_ = {
            &#39;vocab_size&#39;: 72668, # 69071, # len(word_list)
            &#39;n_class&#39;: 2,
            &#39;n_hidden&#39;: 32,
            &#39;lr&#39;: 1e-3, # learning rate 
            &#39;n_epoch&#39;: 150
        }
        if not (model_file.is_file() and embed_file.is_file()): # check if file exists
            
            # download the saved word embedding for the model and the model weight in order to reuse the best model
            self.download_best_model_gdrive()

        emb_dim_concat_ = 125 # 25 (twitter) + 100 (wiki)
        emb_table_concat_ = self.load_local_numpy_txt(best_embed_path)
        model_ = Bi_LSTM_Emb(PARAMS_[&#39;vocab_size&#39;], PARAMS_[&#39;n_hidden&#39;], PARAMS_[&#39;n_class&#39;], emb_dim_concat_, emb_table_concat_)
        model_ = self.load_local_torch_model(model_, best_model_path)
        return model_
    def load_local_numpy_txt(self, path_:str):
        
        return np.loadtxt(path_, dtype=np.float64)
    def load_local_torch_model(self, model_, path_:str):
    
        model_.load_state_dict(torch.load(path_))
        return model_
best_model_path = &#39;model_best_eval.pth&#39;
best_embed_path = &#39;model_best_embed.txt&#39;

h = Helper()
# data = [[&#39;this&#39;,&#39;is&#39;,&#39;a&#39;,&#39;cat&#39;],[&#39;today&#39;,&#39;is&#39;,&#39;a&#39;,&#39;sunny&#39;,&#39;day&#39;]]
# h.save_json(data, fname=&#39;test&#39;)
# h.load_json(data, fname=&#39;test&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(&quot;/content/gdrive&quot;, force_remount=True).
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># h.download_best_model_gdrive()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># m = Bi_LSTM_Emb(p_tmp[&#39;vocab_size&#39;], p_tmp[&#39;n_hidden&#39;], p_tmp[&#39;n_class&#39;], emb_dim_concat_tmp, emb_table_concat_tmp)</span>
<span class="c1"># res_model = h.load_torbch_model(m, &#39;model_best_eval&#39;)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># h.save_entire_model_gdrive(model_best_eval, &#39;best_entire_model&#39;)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># X_tmp = h.load_json_grive(fname=&#39;X_train_number&#39;)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># h.save_model_weight_gdrive(model_eval, &#39;model_best_eval&#39;)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># h.save_numpy_txt_grive(emb_table_concat, &#39;model_best_embed&#39;)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># a = h.load_local_numpy_txt(&#39;best_embed&#39;)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="side-note">
<h2>Side note<a class="headerlink" href="#side-note" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># from transformers import BertTokenizer, BertModel</span>

<span class="c1"># bert = BertModel.from_pretrained(&#39;bert-base-uncased&#39;)</span>

<span class="c1"># tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)</span>
<span class="c1"># encoded_inputs = tokenizer(training_posts, padding=True, </span>
<span class="c1">#                             truncation=True, return_tensors=&quot;pt&quot;)</span>
<span class="c1"># ids = encoded_inputs[&#39;input_ids&#39;]</span>
<span class="c1"># mask = encoded_inputs[&#39;attention_mask&#39;]</span>
<span class="c1"># output = bert(ids, mask)</span>
</pre></div>
</div>
</div>
</div>
<p>I use all as the variable name and realise I also use the all function, 25mins wasted…</p>
<p>Did not use &#64;staticmethod because the class name is a bit long assign a short variable name and call it is better.</p>
<p>should use the same preprocessing as the pretrain word embedding, in this case, below is a translation of ruby to python by someone for standford glove twitter:
<a class="reference external" href="https://www.kaggle.com/code/amackcrane/python-version-of-glove-twitter-preprocess-script/script">https://www.kaggle.com/code/amackcrane/python-version-of-glove-twitter-preprocess-script/script</a></p>
<p>Timing</p>
<section id="eda">
<h3>EDA<a class="headerlink" href="#eda" title="Permalink to this headline">#</a></h3>
<p>training min file len = 57
testing set min len &gt; 1000</p>
</section>
<section id="profiling">
<h3>profiling<a class="headerlink" href="#profiling" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">def tmp(posts_:List[str]) -&gt; List[List[str]]:</span>
<span class="sd">        &quot;&quot;&quot;pre-process the training set</span>
<span class="sd">        Args:</span>
<span class="sd">            posts_(List[str]): comment</span>
<span class="sd">        Returns:</span>
<span class="sd">            var( List[List[str]] ): a outer list contains a list of word pre-processed tokens, where each inner list represents a post</span>
<span class="sd">        &quot;&quot;&quot;</span>
<span class="sd">        res = []</span>
<span class="sd">        dict_ = {</span>
<span class="sd">            &#39;|||&#39;:0,</span>
<span class="sd">            &#39;http&#39;:0,</span>
<span class="sd">            &#39;token&#39;:0,</span>
<span class="sd">            &#39;puncation&#39;:0,</span>
<span class="sd">            &#39;stop&#39;:0,</span>
<span class="sd">            &#39;number&#39;:0,</span>
<span class="sd">            &#39;Lemmatisation&#39;:0,</span>
<span class="sd">            &#39;lowercase&#39;:0,</span>
<span class="sd">            &#39;stemming&#39;:0</span>
<span class="sd">        }</span>
<span class="sd">        for x in posts_:</span>
<span class="sd">            # separate different posts from the same user</span>
<span class="sd">            start = time.time()</span>
<span class="sd">            x = x.replace(&#39;|||&#39;, &#39; &#39;)</span>
<span class="sd">            end = time.time()</span>
<span class="sd">            dict_[&#39;|||&#39;] += end - start</span>

<span class="sd">            start = time.time()</span>

<span class="sd">            x = re.sub(r&#39;http\S+&#39;, &#39;&#39;, x) </span>
<span class="sd">            end = time.time()</span>
<span class="sd">            dict_[&#39;http&#39;] += end - start</span>
<span class="sd">    </span>
<span class="sd">            </span>
<span class="sd">            ############################################################## before tokenisation</span>
<span class="sd">            # word tokenisation</span>
<span class="sd">            </span>
<span class="sd">            start = time.time()</span>
<span class="sd">            # token</span>
<span class="sd">            x = word_tokenize(x)</span>
<span class="sd">            end = time.time()</span>
<span class="sd">            dict_[&#39;token&#39;] += end - start</span>
<span class="sd">            </span>
<span class="sd">            start = time.time()</span>
<span class="sd">            # remove puncation</span>
<span class="sd">            x = [re.sub(r&#39;[^\w\s]&#39;,&#39;&#39;, w) for w in x]</span>
<span class="sd">            end = time.time()</span>
<span class="sd">            dict_[&#39;puncation&#39;] += end - start</span>

<span class="sd">            start = time.time()</span>
<span class="sd">            # remove stop words</span>
<span class="sd">            x = [w for w in x if not w in sww]</span>
<span class="sd">            end = time.time()</span>
<span class="sd">            dict_[&#39;stop&#39;] += end - start</span>

<span class="sd">            # remove number, note this will not remove string contain integer  </span>
<span class="sd">            start = time.time()</span>
<span class="sd">            x = [w for w in x if not w.isdigit()]</span>
<span class="sd">            end = time.time()</span>
<span class="sd">            dict_[&#39;number&#39;] += end - start</span>
<span class="sd">            </span>

<span class="sd">            start = time.time()</span>
<span class="sd">            # Lemmatisation </span>
<span class="sd">            lemmatizer = WordNetLemmatizer()</span>
<span class="sd">            x = [lemmatizer.lemmatize(w) for w in x]</span>
<span class="sd">            end = time.time()</span>
<span class="sd">            dict_[&#39;Lemmatisation&#39;] += end - start</span>
<span class="sd">            </span>
<span class="sd">            </span>
<span class="sd">            start = time.time()</span>
<span class="sd">            # convert the tokens into lowercase</span>
<span class="sd">            x = [t.lower() for t in x]</span>
<span class="sd">            end = time.time()</span>
<span class="sd">            dict_[&#39;lowercase&#39;] += end - start</span>

<span class="sd">            start = time.time()</span>
<span class="sd">            # stemming</span>
<span class="sd">            stemmer = PorterStemmer()</span>
<span class="sd">            x = [stemmer.stem(w) for w in x]</span>
<span class="sd">            end = time.time()</span>
<span class="sd">            dict_[&#39;stemming&#39;] += end - start</span>
<span class="sd">            </span>

<span class="sd">            # remove empty word</span>
<span class="sd">            x = [w for w in x if w]</span>

<span class="sd">            # assignment</span>
<span class="sd">            res.append(x)</span>

<span class="sd">        return dict_</span>
<span class="sd">&#39;&#39;&#39;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;\ndef tmp(posts_:List[str]) -&gt; List[List[str]]:\n        &quot;&quot;&quot;pre-process the training set\n        Args:\n            posts_(List[str]): comment\n        Returns:\n            var( List[List[str]] ): a outer list contains a list of word pre-processed tokens, where each inner list represents a post\n        &quot;&quot;&quot;\n        res = []\n        dict_ = {\n            \&#39;|||\&#39;:0,\n            \&#39;http\&#39;:0,\n            \&#39;token\&#39;:0,\n            \&#39;puncation\&#39;:0,\n            \&#39;stop\&#39;:0,\n            \&#39;number\&#39;:0,\n            \&#39;Lemmatisation\&#39;:0,\n            \&#39;lowercase\&#39;:0,\n            \&#39;stemming\&#39;:0\n        }\n        for x in posts_:\n            # separate different posts from the same user\n            start = time.time()\n            x = x.replace(\&#39;|||\&#39;, \&#39; \&#39;)\n            end = time.time()\n            dict_[\&#39;|||\&#39;] += end - start\n\n            start = time.time()\n\n            x = re.sub(r\&#39;http\\S+\&#39;, \&#39;\&#39;, x) \n            end = time.time()\n            dict_[\&#39;http\&#39;] += end - start\n    \n            \n            ############################################################## before tokenisation\n            # word tokenisation\n            \n            start = time.time()\n            # token\n            x = word_tokenize(x)\n            end = time.time()\n            dict_[\&#39;token\&#39;] += end - start\n            \n            start = time.time()\n            # remove puncation\n            x = [re.sub(r\&#39;[^\\w\\s]\&#39;,\&#39;\&#39;, w) for w in x]\n            end = time.time()\n            dict_[\&#39;puncation\&#39;] += end - start\n\n            start = time.time()\n            # remove stop words\n            x = [w for w in x if not w in sww]\n            end = time.time()\n            dict_[\&#39;stop\&#39;] += end - start\n\n            # remove number, note this will not remove string contain integer  \n            start = time.time()\n            x = [w for w in x if not w.isdigit()]\n            end = time.time()\n            dict_[\&#39;number\&#39;] += end - start\n            \n\n            start = time.time()\n            # Lemmatisation \n            lemmatizer = WordNetLemmatizer()\n            x = [lemmatizer.lemmatize(w) for w in x]\n            end = time.time()\n            dict_[\&#39;Lemmatisation\&#39;] += end - start\n            \n            \n            start = time.time()\n            # convert the tokens into lowercase\n            x = [t.lower() for t in x]\n            end = time.time()\n            dict_[\&#39;lowercase\&#39;] += end - start\n\n            start = time.time()\n            # stemming\n            stemmer = PorterStemmer()\n            x = [stemmer.stem(w) for w in x]\n            end = time.time()\n            dict_[\&#39;stemming\&#39;] += end - start\n            \n\n            # remove empty word\n            x = [w for w in x if w]\n\n            # assignment\n            res.append(x)\n\n        return dict_\n&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># x = tmp(training_posts[:50])</span>
<span class="c1"># sorted(x.items(), key=lambda x: x[1], reverse=True)</span>
<span class="c1"># &#39;&#39;&#39;</span>
<span class="c1"># [(&#39;stemming&#39;, 1.3638272285461426),</span>
<span class="c1">#  (&#39;token&#39;, 0.9907956123352051),</span>
<span class="c1">#  (&#39;Lemmatisation&#39;, 0.3365650177001953),</span>
<span class="c1">#  (&#39;puncation&#39;, 0.14055538177490234),</span>
<span class="c1">#  (&#39;number&#39;, 0.015831470489501953),</span>
<span class="c1">#  (&#39;stop&#39;, 0.011178255081176758),</span>
<span class="c1">#  (&#39;lowercase&#39;, 0.006746768951416016),</span>
<span class="c1">#  (&#39;http&#39;, 0.0018148422241210938),</span>
<span class="c1">#  (&#39;|||&#39;, 0.0006151199340820312)]</span>
<span class="c1"># &#39;&#39;&#39;</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ds-courses/usyd/5046"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="a2.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">In Game Toxicity seq2seq classification</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../5349/5349-intro.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Cloud Computing (USYD)</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Jerry<br/>
  
      &copy; Copyright 2021.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>