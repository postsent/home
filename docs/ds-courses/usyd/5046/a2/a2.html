
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Report - In-game Toxicity Detection &#8212; My AI Notebook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css?v=c1a6f12f" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=949a1ff5" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=ff8fa330"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../_static/togglebutton.js?v=97881d71"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ds-courses/usyd/5046/a2/a2';</script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="In Game Toxicity seq2seq classification" href="../a2.html" />
    <link rel="prev" title="NLP (USYD)" href="../5046-intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">My AI Notebook</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://postsent.github.io/home/">Home</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../reference.html">Acknowledgement</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Research</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../research/polyu/dp/report.html">Data Pruning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../research/polyu/dd/report.html">Dataset Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../research/polyu/story/page.html">DD Motivation</a></li>







<li class="toctree-l1"><a class="reference internal" href="../../../../research/nas/doc.html">NAS</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Courses</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../unsw/9444/9444-intro.html">Deep Learning (UNSW)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../unsw/9444/9444-project.html">Image classification</a></li>








<li class="toctree-l2"><a class="reference internal" href="../../../unsw/9444/assignment1/assignment1.html">Characters, Spirals and Hidden Unit Dynamics</a></li>




















</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../unsw/3900/3900-intro.html">Capstone (UNSW)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../unsw/3900/project/3900-project.html"><strong>Chatbot</strong></a></li>






</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../unsw/3431/3431-intro.html">ROS &amp; CV (UNSW)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../unsw/3431/project/project.html">Mini self-driving</a></li>









</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../5046-intro.html">NLP (USYD)</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Report - In-game Toxicity Detection</a></li>







<li class="toctree-l2"><a class="reference internal" href="../a2.html">In Game Toxicity seq2seq classification</a></li>





<li class="toctree-l2"><a class="reference internal" href="../a1.html">Binary text classification</a></li>






</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../unsw/9417/9417-intro.html">Machine Learning (UNSW)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../unsw/9417/9417-basic.html">Basic with examples</a></li>







<li class="toctree-l2"><a class="reference internal" href="../../../unsw/9417/9417-project.html">Project - Classification</a></li>








<li class="toctree-l2"><a class="reference internal" href="../../../unsw/9417/9417-report/report.html">Project - report</a></li>







</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../unsw/9517/9517-intro.html">Computer Vision (UNSW)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../unsw/9517/9517-a1-code.html">Basic image processing, thresholding, count cells</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../../unsw/9517/9517-a1/report.html">Report on Basic image processing, etc</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../unsw/9517/9517-lane_detection.html">Lane detection</a></li>





<li class="toctree-l2"><a class="reference internal" href="../../../unsw/9517/9517-vehicle-detection.html">Vehicle detection (by teammate)</a></li>




</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../5349/5349-intro.html">Cloud Computing (USYD)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../5349/a2/a2.html">Data Preprocessing and Performance Tuning with Spark</a></li>









<li class="toctree-l2"><a class="reference internal" href="../../5349/a2.html">Data Preprocessing and Performance Tuning with Spark</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../5349/a1.html">Text Analysis with Spark RDD API</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../5349/a1-report.html">Report – Text Analysis with Spark RDD API</a></li>






</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../5338/5338-intro.html">NoSQL &amp; Neo4j (USYD)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../5338/a1.html">NoSQL basic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../5338/a2.html">NoSQL Aggregation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../5338/a2-report/report.html"><strong>Performance Observation Task</strong></a></li>





<li class="toctree-l2"><a class="reference internal" href="../../5338/a3.html">Neo4j Basic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../5338/a4.html">Neo4j Query</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../5048/5048-intro.html">Visual Analytics (USYD)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../5048/individual.html">Individual Report</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../5048/group.html">Group Report (My part)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../5328/5328-intro.html">Advance ML (USYD)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../5328/a1.html">NMF</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../5328/a2.html">Label Noise Learning</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../unsw/9418/9418-intro.html">PGM (UNSW)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../unsw/9418/9418-EDA.html">EDA on Times Series Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../unsw/9418/9418-project.html">Time series project code</a></li>






<li class="toctree-l2"><a class="reference internal" href="../../../unsw/9418/9418-project/report.html">Time series report</a></li>





</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ml/regression/regression.html">Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ml/regression/p1-crypto-prediction.html">Crypto Prediction</a></li>



</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ml/classification/classification.html">Classification (placeholder)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ml/time-series/time-series.html">Time Series (placeholder)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ml/ml.html">General</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../dl/dl.html">General</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../nlp/nlp.html">placeholder</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Vision</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../cv/cv.html">Placeholder</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../misc/math.html">Math</a></li>


<li class="toctree-l1"><a class="reference internal" href="../../../../misc/misc.html">Misc</a></li>




<li class="toctree-l1"><a class="reference internal" href="../../../../misc/term.html">Terminology</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../../../misc/todo.html">TODO</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Coding Basic</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../python/basic-intro.html">Numpy, Pandas, Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../python/numpy.html">Numpy</a></li>





<li class="toctree-l2"><a class="reference internal" href="../../../../python/pandas.html">Pandas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../python/leetcode.html">Leetcode</a></li>








</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Side Projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../side-project/web-scrapter.html">Course Enrolment Scrapter</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unfinished</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../unfinished/pytorch-nlp-bk/nlp-book.html">NLP Book</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../unfinished/pytorch-nlp-bk/intro.html">Intro</a></li>













<li class="toctree-l2"><a class="reference internal" href="../../../../unfinished/pytorch-nlp-bk/nn.html">Feed-Forward Networks for NLP</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../../unfinished/pytorch-nlp-bk/prac-ch3.html">Chapter 3</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../unfinished/jigsaw-intro.html">Jigsaw</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../unfinished/jiagsaw-toxic-comment-serverity-rate/README.html">Folder Structure</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../../unfinished/jiagsaw-toxic-comment-serverity-rate/notebooks/simple-rnn.html">Simple</a></li>













<li class="toctree-l2"><a class="reference internal" href="../../../../unfinished/jiagsaw-toxic-comment-serverity-rate/notebooks/lstm.html">Upgrade RNN</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../../../unfinished/jiagsaw-toxic-comment-serverity-rate/notebooks/helper.html">Common</a></li>

</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../unfinished/nlp-reading.html">Book Reading</a></li>


</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/postsent/home" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../../_sources/ds-courses/usyd/5046/a2/a2.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Report - In-game Toxicity Detection</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Report - In-game Toxicity Detection</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preprocessing">Data preprocessing</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#input-embedding">Input Embedding</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#syntactic">Syntactic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#semantic">Semantic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#domain">Domain</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#slot-filling-tagging-model">Slot Filling/Tagging model</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#stacked-seq2seq-model">Stacked Seq2Seq model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-score-functions">Attention score functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-and-positioning">Self-Attention and positioning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#crf-attachment">CRF Attachment</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">Evaluation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general">General</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-comparison">Performance Comparison</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ablation-studies">Ablation Studies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-result">Evaluation result</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment">Experiment</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Performance Comparison</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#ablation-study">Ablation Study</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#different-input-embedding-model">different input embedding model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#different-attention-strategy">different attention strategy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#different-stacked-layer">different Stacked layer*</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#with-without-crf">with/without CRF</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="report-in-game-toxicity-detection">
<h1>Report - In-game Toxicity Detection<a class="headerlink" href="#report-in-game-toxicity-detection" title="Link to this heading">#</a></h1>
<p>COMP5046 Assignment 2</p>
<p>Link: <a class="reference external" href="https://www.kaggle.com/c/2022-comp5046-a2">https://www.kaggle.com/c/2022-comp5046-a2</a></p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="data-preprocessing">
<h1>Data preprocessing<a class="headerlink" href="#data-preprocessing" title="Link to this heading">#</a></h1>
<p>No content.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="input-embedding">
<h1>Input Embedding<a class="headerlink" href="#input-embedding" title="Link to this heading">#</a></h1>
<p>Input embeddings are created to represent words in a manner readable by machine learning models. We focus on three aspects, syntactic, semantic and domain-specific feature embeddings. These embeddings are concatenated to form different combinations of combined embeddings. If an embedding does not have weights for a given word, then such embedding is treated as zero while other embeddings in the combination will still be used.</p>
<section id="syntactic">
<h2>Syntactic<a class="headerlink" href="#syntactic" title="Link to this heading">#</a></h2>
<p>We consider four syntactic embedding methods.</p>
<p>POS tagging categories words in a sentence into parts of speeches, such as nouns, verbs and adjectives. This may help identify target tags, for example, assume Dota-specific words such as hero or item names are always nouns, thus being able to identify a word as a noun may be a good indicator that it is also a Dota-specific word. Though in reality, there would also be non-nouns such as ‘feed’ and ‘gank’, so it is just an example. The tag embedding can be constructed by first passing the sentences from the CONDA dataset into the POS tagging model pretrained by the Spacy library, then recording the tag for each unique word. For each unique word, the most frequent tag will be its word embedding.</p>
<p>Dependency parsing assigns relationships between words within a sentence, such as in the sentence “She gave me a raise”, raise is the child of gave, with a dependency relationship of the direct object (dobj). Like POS tagging, these relations may be hints to the actual tag. The dependency embedding can be constructed by passing the sentences from the CONDA dataset into the dependency parsing model pretrained by the Spacy library, then recording the dependency for each unique word. For each unique word, the most frequent dependency will be its dependency embedding.</p>
<p>For POS tagging and dependency parsing, we used the en_core_web_sm model from Spacy. We chose it because of its efficiency. It is at least 16 times faster than the en_core_web_trf model when making inference, while its accuracy is not compromised by a lot when compared with the largest trf model, i.e. from 98% to 97% on POS tagging tasks, and 94% to 90% on dependency parsing tasks [1].</p>
<p>Capital form feature is considered because users can leverage the existing expressed meaning with it, such as stressing extreme happiness or angriness. Words in the dataset are lower-cased during the tokenization process, as we intend to reduce vocabulary redundancy in the corpus; thus, we retain the information of whether the word is originally capitalised with the help of the capital form embedding. The value includes the probability distribution of that word being capitalised and not being capitalised across the corpus.</p>
<p>The Word length feature is plainly the word length itself. It is chosen since some words are repeatedly long which the feature may give insight into the model.</p>
</section>
<section id="semantic">
<h2>Semantic<a class="headerlink" href="#semantic" title="Link to this heading">#</a></h2>
<p>We train prediction-based word models to generate features that represent the semantic relationship of words. Prediction-based models include Word2Vec, FastText, GloVe etc. These models in general can represent words in vectors that show semantic relationships such as in the form of “France - Paris equals to German - Berlin” [2].</p>
<p>Here we construct a FastText model trained on the given CONDA dataset. We select FastText because of its capability to deal with out-of-vocabulary issues. Intuitively, word spelling in-game chats are very flexible in the sense that players use a lot of nicknames/abbreviations, and also, they do not care much about typos, such as “pudg” as in the hero “pudge”. FastText constructs the word vector of each word with character n-grams, so the vector of “pudg”, if it is not in the dictionary, can be constructed by using part of the n-gram vectors of “pudge”.</p>
</section>
<section id="domain">
<h2>Domain<a class="headerlink" href="#domain" title="Link to this heading">#</a></h2>
<p>We construct two domain-specific input embeddings to provide more task-related information to the machine learning model i.e. for the toxicity and Dota label tags. We train two FastText-based embeddings with different domain-specific datasets.</p>
<p>The Dota Embedding is trained on texts that we extracted from the subtitles of Dota 2 documentaries and game guides in YouTube, using the online scraping tool “<a class="reference external" href="https://downsub.com/%E2%80%9D">https://downsub.com/”</a>. We chose these materials in order to increase input’s relatedness to Dota specific words, such as hero, ability, and item names, and also words used frequently in-game such as “jungler”, “gank”, “carry”. Due to the different structures of the texts between the Dota dataset and the original CONDA dataset, we have different preprocessing methods. The texts are to be pre-processed with lower-casing, retaining only alphabetical words, removing single-charactered tokens (such as left out ‘s’ when removing the apostrophe), and removing over short sentences as we observed that they are usually not useful such as “(crowd cheers)”. After pre-processing, the data is trained by the FastText model.</p>
<p><code class="docutils literal notranslate">&#160;  </code>The Jigsaw Embedding is trained on toxic comments from the Jigsaw Comment Classification Challenge from Kaggle (<a class="reference external" href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge">https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge</a>). In this case, we try to increase input’s relatedness to toxic language. To acquire the embedding, the texts are to be first pre-processed with lower-casing, and retaining only alphabetical words. Also, only non-toxic comments are filtered out because we only want to know more about toxic language. After pre-processing, the data is trained by FastText model.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="slot-filling-tagging-model">
<h1>Slot Filling/Tagging model<a class="headerlink" href="#slot-filling-tagging-model" title="Link to this heading">#</a></h1>
<p>The model we propose consists of three main components - the stacked seq2seq model, attention layer and CRF attachment.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="stacked-seq2seq-model">
<h1>Stacked Seq2Seq model<a class="headerlink" href="#stacked-seq2seq-model" title="Link to this heading">#</a></h1>
<p>We are using an RNN-based model. Specifically, we are using a multi-stack bi-directional LSTM model. Figure 1 shows the architecture of a sample of a 2-stack bi-direction LSTM model. At the bottom, we will have inputs of word embeddings of BOS/EOS tokens, SEPA tokens and words of the sentence. Then we have multiple layers of bi-LSTM, each represented by a forward LSTM and backward LSTM component. At the end of each layer, the output of the forward and backward components is concatenated (two directions are not treated separately) and passed as the input for the next layer. For our proposed model we have multiple stacks of layers. At the final layer, each output of the final layer will be passed to the attention layer.</p>
<p><img alt="" src="../../../../_images/Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.001.png" /></p>
<p>Figure 1: Stacked LSTM architecture, adapted from [3]</p>
<p>RNN-based model is selected as the foundation of the model because of its capability of handling sequential data (sequences to sequences). They are also a few of the most common deep learning algorithms for tackling POS tagging tasks [4], which bear some similarities to our tagging task. Compared with RNN, LSTM models have less tendency to gradient explosion or vanishing through gating; meanwhile having bi-directional model trains the sequence data from both directions in order to achieve better results [5]. Finally, stacking multiple layers is also found to be the method to improve the performance of an LSTM model [6].</p>
<p>The optimal number of stacked layers is 1 based on the experiment section.</p>
<section id="attention">
<h2>Attention<a class="headerlink" href="#attention" title="Link to this heading">#</a></h2>
<section id="attention-score-functions">
<h3>Attention score functions<a class="headerlink" href="#attention-score-functions" title="Link to this heading">#</a></h3>
<p>Four attention scoring functions are investigated in the experiment section – scaled dot product, dot product, location-based and additive. The general format of the attention calculation is shown in Figure 2 where the function contains three main vectors – query (Q), key (K) and value (V). The idea (similar to how query works in the database system) is to map the query with a set of key-value pairs to an output, whose value is calculated with a suitable function such as softmax in Figure2. Note that the attention used is at sentence level instead of document level i.e. the sentences are fed into the model since the given dataset is in the form of sentences.</p>
<p><img alt="Text Description automatically generated" src="../../../../_images/Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.002.png" /></p>
<p>Figure 2: attention calculation [7, Sec. 3.2.1]</p>
<p>Figure 3 shows the four attention functions that will be discussed in the report.</p>
<p>The dot product attention calculation, namely, is the dot product of the query with all keys and similarly, the scaled dot product divides each result by a scaling factor. The output of these is then applied with a normalisation function e.g. softmax to calculate the weight which would be then multiplied by the values [7, Sec. 3.2.1]. The additive attention can be achieved via concatenation or addition between the st and hi matrixes with a tanh function applied while the location-based involves a trainable weight matrix and softmax function.</p>
<p>![A picture containing text, watch</p>
<p>Description automatically generated](Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.003.png)    <img alt="Text Description automatically generated with medium confidence" src="../../../../_images/Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.004.png" /></p>
<p><code class="docutils literal notranslate"> </code><img alt="Text Description automatically generated with low confidence" src="../../../../_images/Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.005.png" />     <img alt="" src="../../../../_images/Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.006.png" /></p>
<p>Figure 3: four attention score functions</p>
</section>
<section id="self-attention-and-positioning">
<h3>Self-Attention and positioning<a class="headerlink" href="#self-attention-and-positioning" title="Link to this heading">#</a></h3>
<p>There are three types of attention mechanisms - global, local and self-attention. In this report, self-attention is considered along with the Bi-LSTM model. This is because the other two mechanisms rely more on encoder and decoder architecture where the connection between the decoder hidden states and encoder states are explored. For example, they are often used in the machine translation field where the knowledge of the contextual understanding of the long sequences is important.</p>
<p>The self-attention mechanism, on the other hand, considers the relationship between different positions of a single sequence [7, Sec. 2]. In the given dataset, the longest sequence length is 72 indicating that a comprehensive contextual understanding may not be required and thus self-attention is chosen. The short sequence length also means that the issue of self-attention operation scaling quadratically with the sequence length is not impactful [7, Sec. 3.5]<strong>.</strong> The flexibility of the self-attention implementation where any attention score functions can be used after replacing the target sequence with the input sequence is also a key factor why such a mechanism is chosen [8].</p>
<p>Three attention positions – after the first, second and third LSTM stack layers are examined as a result of the chosen self-attention. Note that the implementation involves multi-head attention, which is thought to give more flexibility when scaling the model complexity to improve performance. However, by the experiment below, it is shown that the dataset is relatively simple for the model to learn and thus only one head is chosen for the</p>
</section>
</section>
<section id="crf-attachment">
<h2>CRF Attachment<a class="headerlink" href="#crf-attachment" title="Link to this heading">#</a></h2>
<p>The input of the concatenated last hidden states from the forward and backward direction is passed into conditional random field (CRF) layer as its emission probabilities.</p>
<p>The CRF is a probabilistic model for labelling sequence data. Similar probabilistic models include hidden Markov Models (HMM) and maximum entropy Markov models (MEMM). HMM and MEMM predicts a distribution of tags at each time step, and chains them to the next; on the other hand, CRF focuses on a global level as shown in Figure 4. It was shown that CRF performs better than the other two models in POS tagging tasks [9, Sec. 5.3].</p>
<p><img alt="" src="../../../../_images/Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.007.png" />Figure 4 - chain structure between time-steps of HMM, MEMM and CRF [9, Fig. 2]</p>
<p>Combining the CRF model with the bi-LSTM layer helps us to solve the problem of bi-LSTM models being unable to handle strongly dependent tags well [10]. In a plain RNN-type model, the decision for each timestep is independent of the decision output tag of the previous timestamps, i.e. the output at time t will not care if the output at time t-1 is a C-tag or a D-tag. However, in practice there may be tags that are highly dependent on previous tags, e.g. in the case of NER tasks, I-PER is always followed after B-PER. CRF through its ability to learn in a sentence-wide manner can help to apply the implicit logic of the tags.</p>
<p><img alt="" src="../../../../_images/Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.008.png" /></p>
<p>Figure 5: CRF layer on top of bi-LSTM [11]</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="evaluation">
<h1>Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading">#</a></h1>
<section id="general">
<h2>General<a class="headerlink" href="#general" title="Link to this heading">#</a></h2>
<p>Below is the default configuration applying to the experiment result for the ablation studies.  All the experiments are run on Tesla P100-PCIE-16GB GPU and CUDA 11.1. To maximize the chance of reproducibility, a seed is set wherever is possible (e.g. on the torch random generated tensor). T-F1 evaluation ignores “O”, “SEPA” tags and [PAD] tags if applicable.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p><strong>Configuration</strong></p></th>
<th class="head text-left"><p><strong>Value</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Number of epochs</p></td>
<td class="text-left"><p>2</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Evaluation metric</p></td>
<td class="text-left"><p>F1 score with micro weight.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Model</p></td>
<td class="text-left"><p>Bi-LSTM</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Learning rate</p></td>
<td class="text-left"><p>0.01</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Batch size</p></td>
<td class="text-left"><p>20</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Model hidden dimension</p></td>
<td class="text-left"><p>50</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Optimiser</p></td>
<td class="text-left"><p>Adam</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Weight decay</p></td>
<td class="text-left"><p>1e-4</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Input embedding dimension</p></td>
<td class="text-left"><p>50</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Input embedding window</p></td>
<td class="text-left"><p>3</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Gradient clipping</p></td>
<td class="text-left"><p>Yes</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Table 1: Configurations for ablation studies</p></td>
<td class="text-left"><p></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="performance-comparison">
<h2>Performance Comparison<a class="headerlink" href="#performance-comparison" title="Link to this heading">#</a></h2>
<p>Our best model is a Bi-LSTM model. The configurations are described in Table 2. Note that gradient clipping, ReLU, freezing all embedding layers and adding training data from the validation set are also the keys to making such best model.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p><strong>Configuration</strong></p></th>
<th class="head text-left"><p><strong>Value</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Learning rate</p></td>
<td class="text-left"><p>5e-3</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Batch size</p></td>
<td class="text-left"><p>20</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Model hidden dimension</p></td>
<td class="text-left"><p>6</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Optimiser</p></td>
<td class="text-left"><p>Adam</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Weight decay</p></td>
<td class="text-left"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Embedding type</p></td>
<td class="text-left"><p>FastText on the given dataset, Dota, Jigsaw Toxicity, word being capital probability, POS tag, Dependency path,</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Input embedding dimension</p></td>
<td class="text-left"><p>20</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Input embedding window</p></td>
<td class="text-left"><p>4</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Number of stack layer</p></td>
<td class="text-left"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>CRF attachment</p></td>
<td class="text-left"><p>No</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Attention</p></td>
<td class="text-left"><p>No</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Table 2: Configurations for the best model</p></td>
<td class="text-left"><p></p></td>
</tr>
</tbody>
</table>
</div>
<p>There are several settings different from the baseline model configuration that we would specifically like to elaborate on.</p>
<p>We set the embedding window size to 4 in order to capture a more accurate context. From the Dota Embedding, we can see that there are many heroes or items with compound worded names, such as “Black King Bar” and “Eye of Skadi”. Having a window size that is too small may turn out capturing the proper noun but not the proper noun and its surrounding contexts. Secondly, the hidden dimension for the LSTM layer and input embedding sizes have been tuned down to reduce complexity. Thirdly, we combined the input embeddings proposed in Section 2 except word length, as the input of our best model. The word length embedding is left out because we are dubious of its effects.</p>
</section>
<section id="ablation-studies">
<h2>Ablation Studies<a class="headerlink" href="#ablation-studies" title="Link to this heading">#</a></h2>
<p>In the ablation studies, we will investigate how each part of the component of the system affects the overall performance. We focus on four major components, the embedding model, the attention strategy, the number of stacked layers and the usage of CRF.</p>
<p>For the embedding model, we will first test one additional embedding together with the baseline embedding (FastText trained on given CONDA dataset). The top three additional embedding methods are then chosen as a combination of 2 additional embeddings together with the baseline embedding. Note that the pretrained word embedding is frozen during the training.</p>
<p>For the attention strategy, we will test different attention scoring functions and also different attention positions separately.</p>
<p>We will also study the effect of stacking different numbers of bi-LSTM layers and using with/without the CRF layer.</p>
<p>The general configurations already described in 4.1.1 and specific configurations will be different in sections that examine the impact of such configurations.</p>
</section>
<section id="evaluation-result">
<h2>Evaluation result<a class="headerlink" href="#evaluation-result" title="Link to this heading">#</a></h2>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="experiment">
<h1>Experiment<a class="headerlink" href="#experiment" title="Link to this heading">#</a></h1>
<section id="id1">
<h2>Performance Comparison<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>Table 3 shows that the Bi-LSTM model performed significantly well compared to the baseline model (Bi-LSTM CRF) which scored 0.85726 on the T-F1 according to the Kaggle benchmark. To view the details of the baseline score, we implemented the baseline model offline, which has better performance than the Kaggle benchmark but our best model still outperformed it.</p>
<p>For the baseline model, the trend in Table 3 shows that the model generally predicted better on the tags O, P, and S while the scores on T, C, and D tags are relatively lower, especially for tag D which is around 20% less accurate. This trend, in general, follows the pattern of the size of each tag distribution which is O &gt; P &gt; S &gt; C &gt; T &gt; D. It is because machine learning models would tend to choose majority classes in order to seek lower losses [12].</p>
<p>Our best model bettered the baseline model in all sub-scores. The improvement in detecting toxic tags and Dota-related tags is especially significant. It could be due to our inclusion of the Dota Embedding and the Jigsaw Embedding where they are specifically used to improve performance on the Toxic and Dota tags. The class with lowest score for the best model is the Character tag and is also the least performant class in the baseline model result. This may indicate that the characters may have a lot of nicknames or antonomasia (Gordon ➜ the great chef), which could not be captured by the Dota Embedding as the dataset was extracted from Dota game commentaries and guides, in which their choice of words tends to be more formal than actual in-game.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p><strong>Metric</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><strong>Model / F1 score</strong></p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><p>Bi-LSTM + CRF </p><p>(Baseline)</p></p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><p>Bi-LSTM</p><p>(Best model)</p></p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Table 3: Performance Evaluation on the Base model. T –F1 evaluates all tags; T (Toxicity), S (game Slang), C (Character), D (Dota-specific), P (Pronoun), and O (Other).</p></td>
</tr>
</tbody>
</table>
</div>
<p><img alt="Text Description automatically generated" src="../../../../_images/Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.009.png" />
<img alt="Table Description automatically generated" src="../../../../_images/Aspose.Words.30b9eb51-e7a1-4b08-a7d4-e7b6b4ac9260.010.png" /></p>
<p>Figure 6: Running logs of best model (left) and baseline model (right) with same setting.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="ablation-study">
<h1>Ablation Study<a class="headerlink" href="#ablation-study" title="Link to this heading">#</a></h1>
<section id="different-input-embedding-model">
<h2>different input embedding model<a class="headerlink" href="#different-input-embedding-model" title="Link to this heading">#</a></h2>
<p>Table 4 shows that using the combination of the FastText embedding trained on the given CONDA dataset and the capital letter probability embedding gives us the highest score. Initially, the frequency of the word being full capital was thought to be a helpful feature, but it turns out to be a poor decision which makes the model less accurate. Then, the probability of such a word being full capital is used instead. The probabilistic information of it may have given it the edge in providing insightful information to the machine learning model, as compared to embeddings values that are constructed by majority counts like in POS tag feature and dependency path.</p>
<p>Some other combinations also give us comparable scores (less than 0.2% difference from the highest score), notably C+P, C+CP+DW and C+DW, which are combinations including the capital letter probability, the POS embedding and/or the Dota Embedding. We believe that the reason for the good performance with POS embedding is exactly the reason why we selected it as the embedding, that POS tags may have relations with our target tags. However, as we have explained that the POS embedding for each word is only the majority vote of it across the corpus, the word might not be having a correct POS tag, which may limit its performance. The Dota embedding, also as we have explained, aimed to provide Dota-specific contexts to the word. The embedding did not produce the best combination results, possibly because of its lack of informal Dota knowledge as mentioned in Section 4.2.1.</p>
<p>The remaining embedding methods seem to fall off in performance. Part of them are embeddings that were trained with the task-unrelated data, such as the pretrained Gensim model pretrained on tweets, which the performance was somehow expected as tweets have a fairly different nature to in-game chats. Nonetheless, we are pleased to see that all our proposed embeddings do improve the performance except the length embedding, which was also not surprising as words belonging to the same tag would have different lengths, such as a Dota character can be short as ‘Axe’ and long as ‘Bristleback’.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p><strong>Combination / dataset</strong></p></th>
<th class="head text-center"><p><strong>F1</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>C+CP</p></td>
<td class="text-center"><p>0.94872</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>C+P</p></td>
<td class="text-center"><p>0.94831</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>C+CP+DW</p></td>
<td class="text-center"><p>0.94750</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>C+DW</p></td>
<td class="text-center"><p>0.94743</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>C+J</p></td>
<td class="text-center"><p>0.94206</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>C+CP+P</p></td>
<td class="text-center"><p>0.93926</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>C+G</p></td>
<td class="text-center"><p>0.93648</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>C+DP</p></td>
<td class="text-center"><p>0.93455</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><p>C+DW+P </p><p>(semantic + domain + syntactic)</p></p></td>
<td class="text-center"><p>0.93420</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>C</p></td>
<td class="text-center"><p>0.92217</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>C+L</p></td>
<td class="text-center"><p>0.92166</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Table 4: C: FastText on given CONDA dataset, CP: Capital letter probability, DP: dependency path, DW: FastText on Dota dataset, G: Pretrained Gensim Glove Twitter, J: FastText on Jigsaw Comment Classification Challenge, L: Length of each token, P: POS tag feature.</p></td>
<td class="text-center"><p></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="different-attention-strategy">
<h2>different attention strategy<a class="headerlink" href="#different-attention-strategy" title="Link to this heading">#</a></h2>
<p>Table 5 shows that the dot product yields the best score while the location-based is the least accurate. However, the performance range between the four attention strategies only differs by 0.05.</p>
<p>The reason why the dot product performance is better than the scaled one could be due to the existing configuration of the model on minimizing the overfitting issues e.g. gradient clipping. These configurations reduce the impact of the scaling in the attention calculation and meanwhile the unscaled dot product can speed up the learning speed of the model given the limitation of 2 epochs.</p>
<p>Therefore, the dot product can be seen as the optimal choice as it gives the best performance.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p><strong>Strategy</strong></p></th>
<th class="head text-center"><p><strong>F1 Score</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>Dot Product</p></td>
<td class="text-center"><p>0.92159</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Additive</p></td>
<td class="text-center"><p>0.89888</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Scaled Dot Product</p></td>
<td class="text-center"><p>0.89136</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Location-based</p></td>
<td class="text-center"><p>0.87120</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Table 5: different attention scoring functions</p></td>
<td class="text-center"><p></p></td>
</tr>
</tbody>
</table>
</div>
<p>Table 6 shows that attention at the position of first layer is the best in terms of performance. This may be due to the higher importance of the early layers which gather more detailed information compares to the later ones that focus on the abstraction (bigger picture) [13] i.e. the earlier the attention involves, the more information extracted from the later layers. However, such trend could be caused by the impact of the number of stack layers since section 4.2.4 demonstrates that the longer the stack layers, the worse the performance. Therefore, the optimal attention position is after the first layer.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p><strong>Attention Position (which layer)</strong></p></th>
<th class="head text-center"><p><strong>F1 Score</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>1</p></td>
<td class="text-center"><p>0.89727</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>2</p></td>
<td class="text-center"><p><code class="docutils literal notranslate"> </code>0.86915</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>3</p></td>
<td class="text-center"><p>0.76964</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Table 6: Model performance on three attention position</p></td>
<td class="text-center"><p></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="different-stacked-layer">
<h2>different Stacked layer*<a class="headerlink" href="#different-stacked-layer" title="Link to this heading">#</a></h2>
<p>Table 7 shows that, in general, the fewer stacked layers the LSTM model has, the better the performance. The reason could be that the dataset is simple relative to the complexity of the model. Although the higher stack layers give the model more capability to project data on higher dimension space to draw the decision boundary between different classes, the complexity of such hidden dynamic is unhealthy for the model to learn the simple dataset, causing issues such as overfitting [14]. Thus, optimal number is 1.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p><strong>Number of stack layers</strong></p></th>
<th class="head text-center"><p><strong>F1 Score</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>1</p></td>
<td class="text-center"><p>0.93388</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>2</p></td>
<td class="text-center"><p>0.94335</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>3</p></td>
<td class="text-center"><p>0.90298</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>4</p></td>
<td class="text-center"><p>0.64582 (failed to learn in 2 epochs)</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Table 7: Performance comparison on models with different numbers of stack layers.</p></td>
<td class="text-center"><p></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="with-without-crf">
<h2>with/without CRF<a class="headerlink" href="#with-without-crf" title="Link to this heading">#</a></h2>
<p>We have already mentioned that CRF can solve the dependent tag issue of an LSTM model and the study result seems to agree with the statement. Table 8 shows that the CRF attachment improved the performance of the Bi-LSTM model significantly. However, this comparison was not conducted under the optimal configuration from our best model. With hyperparameters fine-tuning, our best model achieved 99.085% f1 in validation score without CRF attachment, thus we decided to keep the model from further increasing complexity which may lead to overfitting issue.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p><strong>CRF Attachment</strong></p></th>
<th class="head text-center"><p><strong>F1 Score</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>Yes</p></td>
<td class="text-center"><p>0.98005</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>No</p></td>
<td class="text-center"><p>0.94098</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Table 8: Performance comparison between Bi-LSTM with and without CRF layer.</p></td>
<td class="text-center"><p></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>References</strong></p>
<ol class="arabic simple">
<li><p>“English · spaCy Models Documentation”, <a class="reference external" href="http://spacy.io">spacy.io</a>, <a class="reference external" href="https://spacy.io/models/en">https://spacy.io/models/en</a> (accessed: May 31, 2022).</p></li>
<li><p>T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, “Distributed Representations of Words and Phrases and their Compositionality” arXiv:1310.4546 [cs], Oct. 2013.</p></li>
<li><p>Z. Wang and J.-C. Liu, “Translating math formula images to latex sequences using deep neural networks with sequence-level training,” International Journal on Document Analysis and Recognition (IJDAR), 2020.</p></li>
<li><p>A. Chiche and B. Yitagesu, “Part of speech tagging: A systematic review of deep learning and machine learning approaches,” Journal of Big Data, vol. 9, no. 1, 2022.</p></li>
<li><p>S. Siami-Namini, N. Tavakoli, and A. S. Namin, “The performance of LSTM and BiLSTM in forecasting time series,” 2019 IEEE International Conference on Big Data (Big Data), 2019.</p></li>
<li><p>H. Liu, M. Liu, Y. Zhang, J. Xu, and Y. Chen, “Improved character-based Chinese dependency parsing by using stack-tree LSTM,” Natural Language Processing and Chinese Computing, pp. 203–212, 2018.</p></li>
<li><p>A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is All You Need” arXiv:1706.03762 [cs], Jun. 2017.</p></li>
<li><p>J. Cheng, L. Dong, and M. Lapata, “Long Short-Term Memory-Networks for Machine Reading” arXiv: 1601.06733 [cs], Jun. 2017.</p></li>
<li><p>J. D. Lafferty, A. McCallum, and F. C. N. Pereira, “Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data”, Proceedings of the Eighteenth International Conference on Machine Learning, pp. 282–289, Jul. 2001.</p></li>
<li><p>H. Wei, M. Gao, A. Zhou, F. Chen, W. Qu, C. Wang, and M. Lu, “Named entity recognition from biomedical texts using a fusion attention-based BILSTM-CRF,” IEEE Access, vol. 7, pp. 73627–73636, 2019.</p></li>
<li><p>Z. Huang, W. Xu, and K. Yu, “Bidirectional LSTM-CRF Models for Sequence Tagging” arXiv: 1508.01991 [cs], Sep. 2016.</p></li>
<li><p>S. Kotsiantis, D. Kanellopoulos, and P. E. Pintelas, “Handling imbalanced datasets: A review”. GESTS International Transactions on Computer Science and Engineering, vol. 30, pp. 25-36, 2006.</p></li>
<li><p>Santiago A. Cadena, Marissa A. Weis, Leon A. Gatys, Matthias Bethge, Alexander S. Ecker; Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 217-232</p></li>
<li><p>J. Lever, M. Krzywinski, and N. Altman, “Model selection and overfitting,” Nature Methods, vol. 13, no. 9, pp. 703–704, 2016.</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ds-courses\usyd\5046\a2"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../5046-intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">NLP (USYD)</p>
      </div>
    </a>
    <a class="right-next"
       href="../a2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">In Game Toxicity seq2seq classification</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Report - In-game Toxicity Detection</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preprocessing">Data preprocessing</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#input-embedding">Input Embedding</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#syntactic">Syntactic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#semantic">Semantic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#domain">Domain</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#slot-filling-tagging-model">Slot Filling/Tagging model</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#stacked-seq2seq-model">Stacked Seq2Seq model</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-score-functions">Attention score functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-and-positioning">Self-Attention and positioning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#crf-attachment">CRF Attachment</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">Evaluation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general">General</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-comparison">Performance Comparison</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ablation-studies">Ablation Studies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-result">Evaluation result</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment">Experiment</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Performance Comparison</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#ablation-study">Ablation Study</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#different-input-embedding-model">different input embedding model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#different-attention-strategy">different attention strategy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#different-stacked-layer">different Stacked layer*</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#with-without-crf">with/without CRF</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Jerry
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2021.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>